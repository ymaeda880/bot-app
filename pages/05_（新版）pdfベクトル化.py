# pages/05_pdfãƒ™ã‚¯ãƒˆãƒ«åŒ–.py
# ------------------------------------------------------------
# ğŸ“¥ <PDF_ROOT>/<shard>/<pno> ã‚’å–ã‚Šè¾¼ã¿ã€
#    <VS_ROOT>/<backend>/<shard>/ ã« vectors.npy / meta.jsonl ã‚’è¿½è¨˜ã€‚
#    meta ã«ã¯ year / pno / page / embed_model / shard_id / chunk_len_tokens / ocr ç­‰ã‚’ä»˜ä¸ã€‚
#    é‡è¦: meta.jsonl ã¸ã®è¿½è¨˜ã¯ NumpyVectorDB.add() ãŒè¡Œã†ãŸã‚äºŒé‡è¿½è¨˜ã—ãªã„ã€‚
#    â€» OpenAI ã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã¯ text-embedding-3-large ã«å›ºå®šï¼ˆ3072 æ¬¡å…ƒï¼‰
# ------------------------------------------------------------

"""PDFãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆpage å˜ä½ with year/pno ãƒ¡ã‚¿, _ocr å„ªå…ˆï¼‰

æœ¬ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€æŒ‡å®šã•ã‚ŒãŸ PDF ãƒ«ãƒ¼ãƒˆï¼ˆPDF_ROOTï¼‰é…ä¸‹ã®ã€Œã‚·ãƒ£ãƒ¼ãƒ‰ï¼ˆshard=å¹´åº¦ãƒ•ã‚©ãƒ«ãƒ€ï¼‰ã€ã‚’å˜æ•°é¸æŠã—ã€
ãã®ç›´ä¸‹ã®ã€Œãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç•ªå·ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆpnoï¼‰ã€ã‚’é¸æŠã—ã¦ã€å½“è©²ãƒ•ã‚©ãƒ«ãƒ€å†…ã® PDF ã‚’å–ã‚Šè¾¼ã¿ã¾ã™ã€‚

ãƒ»_ocr.pdf ã‚’å„ªå…ˆæ¡ç”¨ï¼ˆä¾‹: A.pdf ã¨ A_ocr.pdf ãŒåŒå±… â†’ A_ocr.pdf ã®ã¿å–ã‚Šè¾¼ã¿ã€A.pdf ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
ãƒ»meta ã¸ ocr çŠ¶æ…‹ï¼ˆ"done"/"no"ï¼‰ã€yearã€pno ã‚’ä»˜ä¸
ãƒ»NumpyVectorDBï¼ˆ<VS_ROOT>/<backend>/<shard>/ï¼‰ã« vectors.npy / meta.jsonl ã‚’è¿½è¨˜
"""

from __future__ import annotations
from pathlib import Path
from datetime import datetime
from typing import List, Tuple, Dict
import json

import streamlit as st
import pdfplumber
import numpy as np
import tiktoken

from config.path_config import PATHS
from config import pricing
from lib.rag_utils import split_text, EmbeddingStore, NumpyVectorDB, ProcessedFilesSimple
from lib.vectorstore_utils import load_processed_files, save_processed_files
from lib.text_normalize import normalize_ja_text

# ============================================================
# å®šæ•°ï¼ˆConstantsï¼‰
# ============================================================
OPENAI_EMBED_MODEL = "text-embedding-3-large"  # å›ºå®šï¼ˆ3072 æ¬¡å…ƒï¼‰

# ============================================================
# tokenizer æº–å‚™ï¼ˆTokenizer setupï¼‰
# ============================================================
try:
    enc = tiktoken.encoding_for_model(OPENAI_EMBED_MODEL)
except Exception:
    enc = tiktoken.get_encoding("cl100k_base")

def count_tokens(text: str) -> int:
    """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆtoken countï¼‰ã‚’è¿”ã™ã€‚"""
    return len(enc.encode(text))

# ============================================================
# UIï¼ˆStreamlit UIï¼‰
# ============================================================
st.set_page_config(page_title="05 ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆpageãƒ»year/pnoå¯¾å¿œãƒ»_ocrå„ªå…ˆï¼‰", page_icon="ğŸ§±", layout="wide")
st.title("ğŸ§± å¹´åº¦ï¼ˆ=ã‚·ãƒ£ãƒ¼ãƒ‰ï¼‰â†’ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç•ªå·ï¼ˆ=ãƒ•ã‚©ãƒ«ãƒ€ï¼‰ã”ã¨ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–")

with st.sidebar:
    st.subheader("ğŸ““ ç¾åœ¨ã®ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆè¡¨ç¤ºã®ã¿ï¼‰")
    st.markdown(f"**Location:** `{PATHS.preset}`")
    st.markdown("#### ğŸ“‚ è§£æ±ºãƒ‘ã‚¹ï¼ˆã‚³ãƒ”ãƒ¼å¯ï¼‰")
    st.text_input("PDF_ROOT", str(PATHS.pdf_root), key="p_pdf", disabled=True)
    if hasattr(PATHS, "data_root"):
        st.text_input("DATA_ROOT", str(PATHS.data_root), key="p_data", disabled=True)
    st.text_input("VS_ROOT",  str(PATHS.vs_root),  key="p_vs",  disabled=True)

PDF_ROOT: Path = PATHS.pdf_root
VS_ROOT: Path  = PATHS.vs_root

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
col1, col2, col3 = st.columns([1, 1, 2])
with col1:
    backend = st.radio("åŸ‹ã‚è¾¼ã¿ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰", ["openai", "local"], index=0, horizontal=True)
    if backend == "openai":
        st.caption(f"ğŸ”§ Embedding ãƒ¢ãƒ‡ãƒ«ã¯ **{OPENAI_EMBED_MODEL}ï¼ˆ3072æ¬¡å…ƒï¼‰å›ºå®š**")
with col2:
    chunk_size = st.number_input("ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆæ–‡å­—ï¼‰", 200, 3000, 900, 50)
    overlap    = st.number_input("ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼ˆæ–‡å­—ï¼‰", 0, 600, 150, 10)
with col3:
    batch_size = st.number_input("åŸ‹ã‚è¾¼ã¿ãƒãƒƒãƒæ•°", 8, 512, 64, 8)
    st.caption("â€» OCR ãŒå¿…è¦ãª PDF ã¯äº‹å‰ã«æ¤œç´¢å¯èƒ½ PDF åŒ–ï¼ˆocrmypdf ç­‰ï¼‰æ¨å¥¨ã€‚")

# å…¥å‡ºåŠ›ã®å®Ÿãƒ‘ã‚¹
st.info(
    f"**å…¥åŠ›**: `{PDF_ROOT}/<shard>/<pno>`\n\n"
    f"**å‡ºåŠ›**: `{VS_ROOT}/{backend}/<shard>`"
)

# ============================================================
# ãƒ‘ã‚¹ãƒ»åˆ—æŒ™ãƒ˜ãƒ«ãƒ‘ï¼ˆPath helpersï¼‰
# ============================================================
def list_shards() -> List[str]:
    """PDF_ROOT ç›´ä¸‹ã®å¹´åº¦ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆã‚·ãƒ£ãƒ¼ãƒ‰ï¼‰ä¸€è¦§ã€‚"""
    if not PDF_ROOT.exists():
        return []
    return sorted([p.name for p in PDF_ROOT.iterdir() if p.is_dir()])

def list_pnos(shard_id: str) -> List[str]:
    """æŒ‡å®š shard é…ä¸‹ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç•ªå·ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆpnoï¼‰ä¸€è¦§ã€‚"""
    d = PDF_ROOT / shard_id
    if not d.exists():
        return []
    return sorted([p.name for p in d.iterdir() if p.is_dir()])

def list_pdfs_in_pno(shard_id: str, pno: str) -> List[Path]:
    """æŒ‡å®š shard/pno é…ä¸‹ã® *.pdf ä¸€è¦§ã€‚"""
    d = PDF_ROOT / shard_id / pno
    if not d.exists():
        return []
    return sorted(d.glob("*.pdf"))

def ensure_vs_dir(backend: str, shard_id: str) -> Path:
    """<VS_ROOT>/<backend>/<shard> ã‚’ä½œæˆï¼ˆãªã‘ã‚Œã°ï¼‰ã€‚"""
    d = VS_ROOT / backend / shard_id
    d.mkdir(parents=True, exist_ok=True)
    return d

def get_vector_count(base_dir: Path) -> int:
    """vectors.npy ã®è¡Œæ•°ï¼ˆç·ãƒ™ã‚¯ãƒˆãƒ«æ•°ï¼‰ã‚’è¿”ã™ã€‚"""
    p = base_dir / "vectors.npy"
    if not p.exists():
        return 0
    try:
        arr = np.load(p, mmap_mode="r")
        return int(arr.shape[0]) if arr.ndim == 2 else 0
    except Exception:
        return 0

# ============================================================
# processed_files.json ã® canon åŒ–ï¼ˆshard/pno/filename å½¢å¼ï¼‰
# ============================================================
def migrate_processed_files_to_canonical(pf_json: Path, shard_id: str, pno: str) -> None:
    """æ—§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆ'filename' ã¾ãŸã¯ 'shard/filename'ï¼‰ã‚’ 'shard/pno/filename' ã¸æ­£è¦åŒ–ã€‚"""
    pf_list = load_processed_files(pf_json)
    if not pf_list:
        return
    changed = False
    canonical: List[str] = []
    for entry in pf_list:
        if isinstance(entry, str):
            val = entry
        elif isinstance(entry, dict):
            val = entry.get("file") or entry.get("path") or entry.get("name")
        else:
            continue
        if not val:
            continue
        parts = val.split("/")
        if len(parts) == 1:
            # 'filename' â†’ 'shard/pno/filename'
            val = f"{shard_id}/{pno}/{val}"
            changed = True
        elif len(parts) == 2:
            # 'shard/filename' â†’ 'shard/pno/filename'
            if parts[0] == shard_id:
                val = f"{shard_id}/{pno}/{parts[1]}"
                changed = True
        # len(parts) >= 3 ã¯ãã®ã¾ã¾æ¡ç”¨
        canonical.append(val)

    # é‡è¤‡é™¤å»ï¼ˆé †åºç¶­æŒï¼‰
    seen, dedup = set(), []
    for v in canonical:
        if v in seen:
            continue
        seen.add(v)
        dedup.append(v)

    if changed:
        save_processed_files(pf_json, dedup)

# ============================================================
# _ocr å„ªå…ˆã®å–ã‚Šè¾¼ã¿å€™è£œæ±ºå®š
# ============================================================
def decide_ocr_candidates(pdf_paths: List[Path]) -> Tuple[List[Tuple[Path, str]], List[str]]:
    """åŒä¸€ãƒ™ãƒ¼ã‚¹åã® A.pdf / A_ocr.pdf ãŒã‚ã‚‹å ´åˆã¯ _ocr ã‚’æ¡ç”¨ã—ã€A.pdf ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã€‚

    Returns:
        candidates: [(æ¡ç”¨ã™ã‚‹ Path, ocr_flag("done" or "no")), ...]
        logs:      [ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡¨ç¤ºç”¨ã®åˆ¤æ–­ãƒ­ã‚°æ–‡å­—åˆ—]
    """
    by_base: Dict[str, Dict[str, Path]] = {}
    for p in pdf_paths:
        stem = p.stem
        if stem.endswith("_ocr"):
            base = stem[:-4]  # remove '_ocr'
            by_base.setdefault(base, {})["ocr"] = p
        else:
            base = stem
            by_base.setdefault(base, {})["base"] = p

    candidates: List[Tuple[Path, str]] = []
    logs: List[str] = []

    for base, d in by_base.items():
        ocr = d.get("ocr")
        basep = d.get("base")
        if ocr is not None:
            candidates.append((ocr, "done"))
            if basep is not None:
                logs.append(f"ğŸŸ¢ æ¡ç”¨: {ocr.name}ï¼ˆ_ocrå„ªå…ˆï¼‰ / â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {basep.name}")
            else:
                logs.append(f"ğŸŸ¢ æ¡ç”¨: {ocr.name}ï¼ˆ_ocr å˜ç‹¬ï¼‰")
        elif basep is not None:
            candidates.append((basep, "no"))
            logs.append(f"ğŸŸ¢ æ¡ç”¨: {basep.name}ï¼ˆ_ocr ãªã—ï¼‰")
        # ã©ã¡ã‚‰ã‚‚ç„¡ã„ã‚±ãƒ¼ã‚¹ã¯å®Ÿè³ªèµ·ããªã„

    # ãƒ•ã‚¡ã‚¤ãƒ«åæ˜‡é †ã«å®‰å®šåŒ–
    candidates.sort(key=lambda t: t[0].name)
    return candidates, logs

# ============================================================
# ã‚·ãƒ£ãƒ¼ãƒ‰â†’pno é¸æŠ UI
# ============================================================
shards = list_shards()
if not shards:
    st.warning(f"{PDF_ROOT} é…ä¸‹ã«å¹´åº¦ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ï¼ˆ=ã‚·ãƒ£ãƒ¼ãƒ‰ï¼‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ä¾‹: {PDF_ROOT}/2025/<pno>/*.pdf")
    st.stop()

selected_shard = st.selectbox("å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ï¼ˆå¹´åº¦ï¼‰ã‚’é¸æŠ", shards, index=0)
pnos = list_pnos(selected_shard)
if not pnos:
    st.warning(f"{PDF_ROOT}/{selected_shard} ã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç•ªå·ãƒ•ã‚©ãƒ«ãƒ€ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    st.stop()

selected_pnos = st.multiselect("å¯¾è±¡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç•ªå·ï¼ˆpnoï¼‰ã‚’é¸æŠï¼ˆè¤‡æ•°å¯ï¼‰", pnos, default=pnos[:1])

st.info(
    "é¸æŠã—ãŸ **å¹´åº¦/ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ** é…ä¸‹ã® PDF ã‚’å–ã‚Šè¾¼ã¿ã¾ã™ã€‚"
    " `_ocr.pdf` ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’æ¡ç”¨ã—ã€åŒåã®å…ƒ PDF ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
)

run = st.button("é¸æŠã—ãŸ pno ãƒ•ã‚©ãƒ«ãƒ€å†…ã® PDF ã‚’å–ã‚Šè¾¼ã¿", type="primary")

# ============================================================
# å®Ÿè¡Œ
# ============================================================
if run:
    estore = EmbeddingStore(backend=backend, openai_model=OPENAI_EMBED_MODEL)
    total_files = 0
    total_chunks = 0

    overall_progress = st.progress(0.0, text="æº–å‚™ä¸­â€¦")
    file_progress = st.progress(0.0, text="ãƒ•ã‚¡ã‚¤ãƒ«é€²æ—ï¼šå¾…æ©Ÿä¸­â€¦")
    status_current = st.empty()

    num_pnos = len(selected_pnos)
    # å¹´åº¦æƒ…å ±
    try:
        year_val = int(selected_shard)
    except ValueError:
        year_val = None

    # ãƒ™ã‚¯ãƒˆãƒ«å‡ºåŠ›ï¼ˆã‚·ãƒ£ãƒ¼ãƒ‰å˜ä½ã§ä¿å­˜ï¼‰
    vs_dir = ensure_vs_dir(backend, selected_shard)
    tracker = ProcessedFilesSimple(vs_dir / "processed_files.json")
    vdb = NumpyVectorDB(vs_dir)

    # ---- pno ã”ã¨ã®ãƒ«ãƒ¼ãƒ— ----
    for i_pno, pno in enumerate(selected_pnos, start=1):
        st.markdown(f"### ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: `{selected_shard}/{pno}`")

        # æ—§ processed_files ã‚’ 'shard/pno/filename' ã«æ­£è¦åŒ–
        migrate_processed_files_to_canonical(vs_dir / "processed_files.json", selected_shard, pno)

        # å€™è£œä¸€è¦§ï¼ˆ_ocr å„ªå…ˆã®æ¡ç”¨æ±ºå®šï¼‹ãƒ­ã‚°ï¼‰
        raw_pdfs = list_pdfs_in_pno(selected_shard, pno)
        candidates, ocr_logs = decide_ocr_candidates(raw_pdfs)

        with st.expander("ğŸ§¾ å–ã‚Šè¾¼ã¿å‰ã®åˆ¤å®šãƒ­ã‚°ï¼ˆ_ocr å„ªå…ˆã®æ¡å¦ï¼‰", expanded=True):
            for line in ocr_logs:
                st.write(line)
            if not candidates:
                st.info("ã“ã® pno ã«ã¯å–ã‚Šè¾¼ã¿å¯¾è±¡ã® PDF ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

        if not candidates:
            overall_progress.progress(i_pno / num_pnos, text=f"{i_pno}/{num_pnos} pno å®Œäº†ï¼ˆç©ºï¼‰")
            continue

        pno_new_files = 0
        pno_new_chunks = 0

        # ---- æ¡ç”¨å€™è£œã”ã¨ã®å‡¦ç† ----
        for i_file, (pdf_path, ocr_flag) in enumerate(candidates, start=1):
            name = pdf_path.name
            key_full = f"{selected_shard}/{pno}/{name}"  # æ­£æº–ã‚­ãƒ¼

            # æ—§ã‚­ãƒ¼ï¼ˆshard/name, nameï¼‰ã‚‚ã‚¹ã‚­ãƒƒãƒ—å¯¾è±¡ã«å«ã‚ã‚‹ï¼ˆå¾Œæ–¹äº’æ›ï¼‰
            if tracker.is_done(key_full) or tracker.is_done(f"{selected_shard}/{name}") or tracker.is_done(name):
                status_current.info(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¢å–è¾¼ï¼‰: `{selected_shard}/{pno}` / **{name}**")
                file_progress.progress(1.0, text=f"ãƒ•ã‚¡ã‚¤ãƒ« {i_file}/{len(candidates)} å®Œäº†: {name}")
                continue

            try:
                with pdfplumber.open(str(pdf_path)) as pdf:
                    total_pages = max(len(pdf.pages), 1)
                    status_current.info(
                        f"ğŸ“¥ å–ã‚Šè¾¼ã¿é–‹å§‹: `{selected_shard}/{pno}` / **{name}**ï¼ˆ{i_file}/{len(candidates)}ï¼‰ å…¨{total_pages}ãƒšãƒ¼ã‚¸"
                    )
                    file_progress.progress(0.0, text=f"ãƒ•ã‚¡ã‚¤ãƒ« {i_file}/{len(candidates)}: {name} - 0/{total_pages} ãƒšãƒ¼ã‚¸")

                    # ---- ãƒšãƒ¼ã‚¸ã”ã¨ã«æŠ½å‡ºâ†’åˆ†å‰²â†’åŸ‹ã‚è¾¼ã¿ ----
                    for page_no, page in enumerate(pdf.pages, start=1):
                        raw = page.extract_text(x_tolerance=1.5, y_tolerance=1.5) or ""
                        raw = raw.replace("\t", " ").replace("\xa0", " ")
                        text = " ".join(raw.split())

                        if not text:
                            file_progress.progress(page_no / total_pages,
                                text=f"ãƒ•ã‚¡ã‚¤ãƒ« {i_file}/{len(candidates)}: {name} - {page_no}/{total_pages} ãƒšãƒ¼ã‚¸")
                            continue

                        text = normalize_ja_text(text)
                        spans: List[Tuple[str, int, int]] = split_text(
                            text, chunk_size=int(chunk_size), overlap=int(overlap)
                        )
                        if not spans:
                            file_progress.progress(page_no / total_pages,
                                text=f"ãƒ•ã‚¡ã‚¤ãƒ« {i_file}/{len(candidates)}: {name} - {page_no}/{total_pages} ãƒšãƒ¼ã‚¸")
                            continue

                        chunks = [s[0] for s in spans]
                        vectors: List[np.ndarray] = []
                        metas: List[dict] = []

                        for i in range(0, len(chunks), int(batch_size)):
                            batch = chunks[i:i + int(batch_size)]
                            vecs = estore.embed(batch, batch_size=int(batch_size)).astype("float32")
                            vectors.append(vecs)

                            for j, (ch, s, e) in enumerate(spans[i:i + int(batch_size)]):
                                metas.append({
                                    "file": key_full,                           # 'shard/pno/filename'
                                    "year": year_val,                           # ä¾‹: 2025
                                    "pno": pno,                                 # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç•ªå·
                                    "page": page_no,
                                    "chunk_id": f"{name}#p{page_no}-{i+j}",
                                    "chunk_index": i + j,
                                    "text": ch,
                                    "span_start": s,
                                    "span_end": e,
                                    "chunk_len_tokens": count_tokens(ch),
                                    "ingested_at": datetime.utcnow().isoformat(timespec="seconds") + "Z",
                                    "shard_id": selected_shard,
                                    "embed_model": OPENAI_EMBED_MODEL if backend == "openai" else "local-model",
                                    "ocr": "done" if ocr_flag == "done" else "no",
                                })

                        vec_mat = np.vstack(vectors) if len(vectors) > 1 else vectors[0]
                        vdb.add(vec_mat, metas)
                        pno_new_chunks += len(metas)

                        file_progress.progress(page_no / total_pages,
                            text=f"ãƒ•ã‚¡ã‚¤ãƒ« {i_file}/{len(candidates)}: {name} - {page_no}/{total_pages} ãƒšãƒ¼ã‚¸")

                tracker.mark_done(key_full)
                pno_new_files += 1
                status_current.success(f"âœ… å®Œäº†: `{selected_shard}/{pno}` / **{name}**ï¼ˆ{i_file}/{len(candidates)}ï¼‰")

            except Exception as e:
                st.error(f"âŒ å–ã‚Šè¾¼ã¿å¤±æ•—: {name} : {e}")
                status_current.error(f"âŒ å¤±æ•—: `{selected_shard}/{pno}` / **{name}** - {e}")

            overall_progress.progress(
                (i_pno - 1 + i_file / max(len(candidates), 1)) / num_pnos,
                text=f"å…¨ä½“ {i_pno}/{num_pnos} pno å‡¦ç†ä¸­â€¦ï¼ˆ{pno}: {i_file}/{len(candidates)} ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰"
            )

        st.success(f"ğŸ“ `{selected_shard}/{pno}`: æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ« {pno_new_files} ä»¶ / è¿½åŠ ãƒãƒ£ãƒ³ã‚¯ {pno_new_chunks} ä»¶")
        st.caption(f"ğŸ” ã‚·ãƒ£ãƒ¼ãƒ‰å†…ãƒ™ã‚¯ãƒˆãƒ«ç·æ•°ï¼ˆDBè¨ˆæ¸¬ï¼‰: {get_vector_count(vs_dir):,d}")

        overall_progress.progress(i_pno / num_pnos, text=f"{i_pno}/{num_pnos} pno å®Œäº†")

        total_files  += pno_new_files
        total_chunks += pno_new_chunks

    st.toast(f"âœ… å®Œäº†: æ–°è¦ {total_files} ãƒ•ã‚¡ã‚¤ãƒ« / {total_chunks} ãƒãƒ£ãƒ³ã‚¯ï¼ˆ_ocrå„ªå…ˆãƒ»year/pnoä»˜ãï¼‰", icon="âœ…")

    # ---------- æ–™é‡‘è¨ˆç®—ï¼ˆopenai backend ã®ã¿ï¼‰ ----------
    if total_chunks > 0:
        st.markdown("### ğŸ’° åŸ‹ã‚è¾¼ã¿ã‚³ã‚¹ãƒˆã®æ¦‚ç®—")
        if backend == "openai":
            total_tokens = 0
            meta_path = (VS_ROOT / backend / selected_shard / "meta.jsonl")
            if meta_path.exists():
                with meta_path.open("r", encoding="utf-8") as f:
                    for line in f:
                        try:
                            obj = json.loads(line)
                        except Exception:
                            continue
                        total_tokens += int(obj.get("chunk_len_tokens", 0))
            model = OPENAI_EMBED_MODEL
            usd = pricing.estimate_embedding_cost_usd(total_tokens, model)
            jpy = pricing.estimate_embedding_cost_jpy(total_tokens, model)
            st.write(f"- ãƒ¢ãƒ‡ãƒ«: **{model}**")
            st.write(f"- ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {total_tokens:,}")
            st.write(f"- æ¦‚ç®—ã‚³ã‚¹ãƒˆ: `${usd:.4f}` â‰ˆ Â¥{jpy:,.0f}")
        else:
            st.info("local backend ã®ãŸã‚ã‚³ã‚¹ãƒˆã¯ç™ºç”Ÿã—ã¾ã›ã‚“ã€‚")
