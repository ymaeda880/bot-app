# pages/29_ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢.py
# ------------------------------------------------------------
# ğŸ” meta.jsonl æ¨ªæ–­æ¤œç´¢ + ï¼ˆä»»æ„ï¼‰OpenAI ç”Ÿæˆè¦ç´„ï¼ˆãƒœã‚¿ãƒ³ã§å®Ÿè¡Œï¼‰
# - ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¯ OpenAI å›ºå®šï¼ˆvectorstore/openaiï¼‰
# - æ¤œç´¢çµæœï¼ˆè¡¨ãƒ»ã‚¹ãƒ‹ãƒšãƒƒãƒˆï¼‰ã¯å¸¸ã«è¡¨ç¤ºã•ã‚ŒãŸã¾ã¾
# - è¦ç´„ã¯ã€ŒğŸ§  è¦ç´„ã‚’ç”Ÿæˆã€ãƒœã‚¿ãƒ³æŠ¼ä¸‹æ™‚ã®ã¿å®Ÿè¡Œ
#   * OpenAIæœªè¨­å®šã‚„å¤±æ•—æ™‚ã¯ãƒ­ãƒ¼ã‚«ãƒ«æŠ½å‡ºã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
#   * è¡¨ç¤ºã¯ã€Œã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚° / ä¸€æ‹¬è¡¨ç¤ºã€ã‚’ãƒ©ã‚¸ã‚ªã§åˆ‡æ›¿ï¼ˆlib/gpt_responder çµŒç”±ï¼‰
# ------------------------------------------------------------

"""
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¨ªæ–­æ¤œç´¢ï¼ˆmeta.jsonlï¼‰ï¼‹è¦ç´„ãƒšãƒ¼ã‚¸

æœ¬ãƒšãƒ¼ã‚¸ã¯ã€å–ã‚Šè¾¼ã¿æ¸ˆã¿ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ï¼ˆOpenAI ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ï¼‰é…ä¸‹ã®
å„ã‚·ãƒ£ãƒ¼ãƒ‰ã«å«ã¾ã‚Œã‚‹ `meta.jsonl` ã‚’æ¨ªæ–­ã—ã€å˜ç´”ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰/æ­£è¦è¡¨ç¾æ¤œç´¢ã§
ãƒ’ãƒƒãƒˆè¡Œã‚’æŠ½å‡ºãƒ»è¡¨ç¤ºã—ã¾ã™ã€‚ã•ã‚‰ã«ã€ãƒ’ãƒƒãƒˆã‚¹ãƒ‹ãƒšãƒƒãƒˆä¸Šä½Nä»¶ã‚’ã¾ã¨ã‚ã¦
ç”Ÿæˆç³»ãƒ¢ãƒ‡ãƒ«ã§è¦ç´„ï¼ˆResponses API äº’æ› / Chat Completions äº’æ›ï¼‰ã™ã‚‹æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚

ä¸»ãªæ§‹æˆ:
- ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šæ¤œç´¢ç¯„å›²ãƒ»è¡¨ç¤ºè¨­å®šãƒ»ç”Ÿæˆè¦ç´„ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³
- æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ ï¼šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€AND/ORã€æ­£è¦è¡¨ç¾ ç­‰
- æ¤œç´¢å®Ÿè¡Œï¼šjsonl ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒ èª­ã¿è¾¼ã¿ã—ã€æ¡ä»¶ä¸€è‡´ã‚’è¡¨å½¢å¼ + ã‚¹ãƒ‹ãƒšãƒƒãƒˆã§è¡¨ç¤º
- ç”Ÿæˆè¦ç´„ï¼šãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯æ™‚ã« `lib.gpt_responder.GPTResponder` ã‚’ä»‹ã—ã¦
             ã€Œé€æ¬¡è¡¨ç¤ºï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰ã€/ã€Œä¸€æ‹¬è¡¨ç¤ºã€ã‚’åˆ‡ã‚Šæ›¿ãˆå¯èƒ½
- ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šAPIæœªè¨­å®š/ã‚¨ãƒ©ãƒ¼/ãƒˆãƒ¼ã‚¯ãƒ³äºˆç®—è¶…éç­‰ã¯ãƒ­ãƒ¼ã‚«ãƒ«æŠ½å‡ºã‚µãƒãƒªã«åˆ‡æ›¿

æ³¨æ„:
- BASE_DIR ã¯ `PATHS.vs_root / "openai"` ã‚’å‰æ
- `lib/gpt_responder.py` ã®é…ç½®ãŒå¿…è¦
"""

from __future__ import annotations
from pathlib import Path
from typing import List, Dict, Iterable, Any
import re, json, os, traceback
import pandas as pd
import streamlit as st

from lib.text_normalize import normalize_ja_text
from config.path_config import PATHS  # PATHSã«ä¸€æœ¬åŒ–
from lib.gpt_responder import GPTResponder, CompletionResult  # ç”Ÿæˆè¦ç´„ã®çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

# ============== ãƒ‘ã‚¹ï¼ˆPATHSã«çµ±ä¸€ï¼‰ ==============
# OpenAI ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å›ºå®šã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãƒ»ãƒ«ãƒ¼ãƒˆ
BASE_DIR: Path = PATHS.vs_root / "openai"

# ============== åŸºæœ¬UI ==============
st.set_page_config(page_title="20 ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆmetaæ¨ªæ–­ / OpenAIï¼‰", page_icon="ğŸ”", layout="wide")
st.title("ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆmeta.jsonl æ¨ªæ–­ / OpenAIï¼‰")

# ============== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ==============
def strip_html(s: str) -> str:
    """HTMLã‚¿ã‚°ã‚’å˜ç´”é™¤å»ã—ã¦ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™ã€‚"""
    return re.sub(r"<[^>]+>", "", s or "")


def make_snippet(text: str, pats: List[re.Pattern], total_len: int = 240) -> str:
    """
    ãƒ’ãƒƒãƒˆå‘¨è¾ºã®æ–‡è„ˆã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’ç”Ÿæˆã—ã€è©²å½“èªã‚’ <mark> ã§ãƒã‚¤ãƒ©ã‚¤ãƒˆã™ã‚‹ã€‚

    Args:
        text: å…ƒãƒ†ã‚­ã‚¹ãƒˆï¼ˆHTMLå¯ï¼‰
        pats: æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒªã‚¹ãƒˆ
        total_len: ã‚¹ãƒ‹ãƒšãƒƒãƒˆã®ç›®å®‰é•·ï¼ˆãƒ’ãƒƒãƒˆä½ç½®ã‚’ä¸­å¿ƒã«å‰å¾Œåˆ‡ã‚Šå‡ºã—ï¼‰

    Returns:
        ãƒã‚¤ãƒ©ã‚¤ãƒˆæ¸ˆã¿ã®ã‚¹ãƒ‹ãƒšãƒƒãƒˆæ–‡å­—åˆ—ï¼ˆä¸¡ç«¯çœç•¥è¨˜å·ä»˜ãï¼‰
    """
    # æœ€åˆã«ãƒ’ãƒƒãƒˆã—ãŸä½ç½®ã®ç¯„å›²ã‚’æ¢ã™ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°å…ˆé ­ã€œtotal_lenï¼‰
    pos = next((m.span() for p in pats if (m := p.search(text))), (0, min(len(text), total_len)))
    left, right = max(0, pos[0] - total_len // 2), min(len(text), pos[1] + total_len // 2)
    snip = text[left:right]
    # ãã‚Œãã‚Œã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ <mark> ã§å¼·èª¿
    for p in pats:
        try:
            snip = p.sub(lambda m: f"<mark>{m.group(0)}</mark>", snip)
        except re.error:
            pass
    return ("â€¦" if left else "") + snip + ("â€¦" if right < len(text) else "")


def _encoding_for(model_hint: str):
    """
    tiktoken ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å–å¾—ï¼ˆå¤±æ•—æ™‚ã¯ Noneï¼‰.

    Returns:
        tiktoken.Encoding | None
    """
    try:
        import tiktoken
        try:
            return tiktoken.encoding_for_model(model_hint)
        except Exception:
            return tiktoken.get_encoding("cl100k_base")
    except Exception:
        return None


def count_tokens(text: str, model_hint: str = "gpt-5-mini") -> int:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã®æ¦‚ç®—ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ç®—å‡ºï¼ˆtiktoken ãŒãªã‘ã‚Œã° 1tokenâ‰’4æ–‡å­—è¿‘ä¼¼ï¼‰."""
    enc = _encoding_for(model_hint)
    if not enc:
        return max(1, len(text or "") // 4)
    try:
        return len(enc.encode(text or ""))
    except Exception:
        return max(1, len(text or "") // 4)


def truncate_by_tokens(text: str, max_tokens: int, model_hint: str = "gpt-5-mini") -> str:
    """
    ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ‡ã‚Šè©°ã‚ã‚‹ï¼ˆå¯èƒ½ãªã‚‰ tiktokenã€ãªã‘ã‚Œã°æ–‡å­—æ•°è¿‘ä¼¼ï¼‰."""
    enc = _encoding_for(model_hint)
    if not enc:
        return (text or "")[: max(100, max_tokens * 4)]
    try:
        toks = enc.encode(text or "")
        return text if len(toks) <= max_tokens else enc.decode(toks[:max_tokens])
    except Exception:
        return (text or "")[: max(100, max_tokens * 4)]


def is_gpt5(model_name: str) -> bool:
    """ãƒ¢ãƒ‡ãƒ«åãŒ gpt-5 ç³»ã‹ã©ã†ã‹ã‚’åˆ¤å®šã€‚"""
    return (model_name or "").lower().startswith("gpt-5")


def iter_jsonl(path: Path) -> Iterable[Dict]:
    """
    JSON Lines ã‚’1è¡Œãšã¤è¾æ›¸ã¨ã—ã¦èª­ã¿å‡ºã™ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã€‚
    ç•°å¸¸è¡Œã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã€å­˜åœ¨ã—ãªã„å ´åˆã¯ä½•ã‚‚è¿”ã•ãªã„ã€‚
    """
    if not path.exists():
        return
    with path.open("r", encoding="utf-8") as f:
        for s in f:
            s = s.strip()
            if s:
                try:
                    yield json.loads(s)
                except Exception:
                    # å£Šã‚ŒãŸè¡Œã¯ç„¡è¦–
                    pass


def compile_terms(q: str, *, use_regex: bool, case_sensitive: bool, normalize_query: bool) -> List[re.Pattern]:
    """
    ã‚¯ã‚¨ãƒªæ–‡å­—åˆ—ã‚’ç©ºç™½ã§åˆ†å‰²ã—ã€å„èªã‚’ re.Pattern ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã™ã‚‹ã€‚

    Args:
        q: å…¥åŠ›ã‚¯ã‚¨ãƒªï¼ˆç©ºç™½åŒºåˆ‡ã‚Šï¼‰
        use_regex: True ãªã‚‰ç”Ÿã®æ­£è¦è¡¨ç¾ã€False ãªã‚‰ãƒªãƒ†ãƒ©ãƒ«ã¨ã—ã¦ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
        case_sensitive: å¤§æ–‡å­—å°æ–‡å­—ã®åŒºåˆ¥
        normalize_query: æ—¥æœ¬èªã®å…¨è§’/åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ç­‰ã‚’æ­£è¦åŒ–

    Returns:
        æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒªã‚¹ãƒˆ
    """
    if normalize_query:
        q = normalize_ja_text(q)
    terms = [t for t in q.split() if t]
    flags = 0 if case_sensitive else re.IGNORECASE
    pats: List[re.Pattern] = []
    for t in terms:
        try:
            pats.append(re.compile(t if use_regex else re.escape(t), flags))
        except re.error:
            # ä¸æ­£ãªæ­£è¦è¡¨ç¾ã¯ãƒªãƒ†ãƒ©ãƒ«ã¨ã—ã¦æ‰±ã†
            pats.append(re.compile(re.escape(t), flags))
    return pats


def local_summary(labelled_snips: List[str], max_sent: int = 10) -> str:
    """
    ãƒ­ãƒ¼ã‚«ãƒ«æŠ½å‡ºã«ã‚ˆã‚‹ç°¡æ˜“ã‚µãƒãƒªã‚’ç”Ÿæˆã€‚
    - HTMLã‚¿ã‚°é™¤å» â†’ æ–‡åˆ†å‰² â†’ å…ˆé ­ã‹ã‚‰é‡è¤‡é™¤å»ã—ã¤ã¤ç®‡æ¡æ›¸ãåŒ– â†’ çŸ­ã„ã¾ã¨ã‚ã‚’è¿½åŠ 
    """
    text = re.sub(r"<[^>]+>", "", "\n\n".join(labelled_snips))
    text = re.sub(r"(?m)^---\s*$|(?m)^#\s*Source:.*$", "", text)
    parts = [p.strip() for p in re.split(r"[ã€‚ï¼.!?ï¼ï¼Ÿ]\s*|\n+", text) if len((p or "").strip()) >= 6]
    out, seen = [], set()
    for p in parts:
        if p in seen:
            continue
        seen.add(p)
        out.append(f"ãƒ»{p}")
        if len(out) >= max_sent:
            break
    if not out:
        return "ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«æŠ½å‡ºã‚µãƒãƒªï¼šè¦ç´„ã§ãã‚‹æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼‰"
    short = (parts[0][:120] + "â€¦") if parts else ""
    return "### ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«æŠ½å‡ºã‚µãƒãƒªï¼‰\n" + "\n".join(out) + f"\n\nâ€” çŸ­ã„ã¾ã¨ã‚: {short}"


def fit_to_budget(
    snips: List[str],
    *,
    model: str,
    sys_prompt: str,
    user_prefix: str,
    want_output: int,
    context_limit: int,
    safety_margin: int,
) -> List[str]:
    """
    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¨ä½“ï¼ˆsystem + user_prefix + ã‚¹ãƒ‹ãƒšãƒƒãƒˆ + æœŸå¾…å‡ºåŠ›ï¼‰ãŒ
    ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¸Šé™ã‚’è¶…ãˆãªã„ã‚ˆã†ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’é–“å¼•ããƒ»åˆ†å‰²ã™ã‚‹ã€‚
    """
    while True:
        toks = (
            count_tokens(sys_prompt, model)
            + count_tokens(user_prefix, model)
            + sum(count_tokens(s, model) for s in snips)
        )
        # æœŸå¾…å‡ºåŠ› + å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³è¾¼ã¿ã§åã¾ã‚‹ã¾ã§æœ«å°¾ã‚’å‰Šã‚‹
        if toks + want_output + safety_margin <= context_limit or not snips:
            break
        snips.pop()
    if snips:
        # å€‹ã€…ã®ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’å¿…è¦ã«å¿œã˜ã¦ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ã§åˆ‡ã‚Šè©°ã‚ã‚‹
        budget = max(
            500,
            context_limit
            - (count_tokens(sys_prompt, model) + count_tokens(user_prefix, model) + want_output + safety_margin),
        )
        snips = [
            s if count_tokens(s, model) <= budget else truncate_by_tokens(s, budget, model) for s in snips
        ]
    return snips


# ============== ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆOpenAI å›ºå®šï¼‰ ==============
with st.sidebar:
    st.header("æ¤œç´¢å¯¾è±¡ï¼ˆOpenAIï¼‰")

    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãƒ»ãƒ«ãƒ¼ãƒˆã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼ˆæœªæ§‹ç¯‰æ™‚ã®æ—©æœŸä¸­æ–­ï¼‰
    if not BASE_DIR.exists():
        st.error(f"{BASE_DIR} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã« 03 ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’ OpenAI ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # åˆ©ç”¨å¯èƒ½ãªã‚·ãƒ£ãƒ¼ãƒ‰ä¸€è¦§ã‚’æŠ½å‡ºã—ã€æ¤œç´¢å¯¾è±¡ã‚’é¸ã°ã›ã‚‹
    shard_ids = [p.name for p in sorted([p for p in BASE_DIR.iterdir() if p.is_dir()])]
    sel_shards = st.multiselect("å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰", shard_ids, default=shard_ids)

    st.divider()
    st.subheader("çµã‚Šè¾¼ã¿ï¼ˆä»»æ„ï¼‰")
    # å¹´ã§ã®ç°¡æ˜“ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆãƒ¡ã‚¿ã« year ãŒã‚ã‚‹æƒ³å®šï¼‰
    year_min = st.number_input("å¹´ï¼ˆä¸‹é™ï¼‰", value=0, step=1, help="0 ã§ç„¡åŠ¹")
    year_max = st.number_input("å¹´ï¼ˆä¸Šé™ï¼‰", value=9999, step=1, help="9999 ã§ç„¡åŠ¹")
    # ãƒ•ã‚¡ã‚¤ãƒ«åã®éƒ¨åˆ†ä¸€è‡´ãƒ•ã‚£ãƒ«ã‚¿
    file_filter = st.text_input("ãƒ•ã‚¡ã‚¤ãƒ«åãƒ•ã‚£ãƒ«ã‚¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", value="").strip()

    st.divider()
    st.subheader("è¡¨ç¤ºè¨­å®š")
    # è¡¨ç¤ºä»¶æ•°ã®ä¸Šé™ï¼ˆæ¤œç´¢ã¯æ—©ã‚ã«æ‰“ã¡åˆ‡ã‚‹ï¼‰
    max_rows = st.number_input("æœ€å¤§è¡¨ç¤ºä»¶æ•°", min_value=50, max_value=5000, value=500, step=50)
    # ã‚¹ãƒ‹ãƒšãƒƒãƒˆé•·ï¼ˆãƒ’ãƒƒãƒˆå‘¨è¾ºã®å‰å¾Œåˆè¨ˆï¼‰
    snippet_len = st.slider("ã‚¹ãƒ‹ãƒšãƒƒãƒˆé•·ï¼ˆå‰å¾Œåˆè¨ˆï¼‰", min_value=80, max_value=800, value=240, step=20)
    # è¡¨ç¤ºã‚«ãƒ©ãƒ é¸æŠï¼ˆtext ã¯åˆ¥ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã‚‚å‡ºã™ãŸã‚çœç•¥å¯èƒ½ï¼‰
    show_cols = st.multiselect(
        "è¡¨ç¤ºã‚«ãƒ©ãƒ ",
        ["file", "year", "page", "shard_id", "chunk_id", "chunk_index", "score", "text"],
        default=["file", "year", "page", "shard_id", "score", "text"],
    )

    st.divider()
    st.subheader("ğŸ§  ç”Ÿæˆã‚ªãƒ—ã‚·ãƒ§ãƒ³")

    def get_openai_key() -> str | None:
        """secrets â†’ env ã®é †ã§ OPENAI_API_KEY ã‚’å–å¾—ã€‚"""
        try:
            return st.secrets.get("OPENAI_API_KEY") or (st.secrets.get("openai") or {}).get("api_key") or os.getenv(
                "OPENAI_API_KEY"
            )
        except Exception:
            return os.getenv("OPENAI_API_KEY")

    OPENAI_API_KEY = get_openai_key()

    # ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆgpt-5 / gpt-4 ç³»ï¼‰
    model = st.selectbox("ãƒ¢ãƒ‡ãƒ«", ["gpt-5-mini", "gpt-5", "gpt-4o-mini", "gpt-4o", "gpt-4.1-mini"], index=0)
    # gpt-5 ç³»ã¯ Responses API æº–æ‹ ã§ temperature æŒ‡å®šãªã—ã®ãŸã‚ 1.0 ãƒ€ãƒŸãƒ¼å›ºå®š
    temperature = 1.0 if is_gpt5(model) else st.slider("temperature", 0.0, 1.0, 0.2, 0.05)
    # å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™ï¼ˆæ¦‚ç®—ç›®å®‰ï¼‰
    max_tokens = st.slider("æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆç›®å®‰ï¼‰", 1000, 40000, 12000, 500)

    # è¦ç´„ã«ä½¿ã†ã‚¹ãƒ‹ãƒšãƒƒãƒˆæ•°ãƒ»ãƒ†ãƒ³ãƒ—ãƒ¬
    topn_snippets = st.slider("è¦ç´„ã«ä½¿ã†ä¸Šä½ã‚¹ãƒ‹ãƒšãƒƒãƒˆæ•°", 5, 200, 30, 5)
    sys_prompt = st.text_area(
        "System Prompt",
        "ã‚ãªãŸã¯äº‹å®Ÿã«å¿ å®Ÿãªãƒªã‚µãƒ¼ãƒã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ ¹æ‹ ã®ã‚ã‚‹è¨˜è¿°ã®ã¿ã‚’æ—¥æœ¬èªã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚",
        height=80,
    )
    user_prompt_tpl = st.text_area(
        "User Prompt ãƒ†ãƒ³ãƒ—ãƒ¬ï¼ˆ{query}, {snippets} ã‚’åŸ‹ã‚è¾¼ã¿ï¼‰",
        "ä»¥ä¸‹ã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã§å¾—ã‚‰ã‚ŒãŸãƒ’ãƒƒãƒˆã‚¹ãƒ‹ãƒšãƒƒãƒˆã§ã™ã€‚ã“ã®æƒ…å ±ã€ã®ã¿ã€‘ã‚’æ ¹æ‹ ã«ã€"
        "ã‚¯ã‚¨ãƒªã€{query}ã€ã«ã¤ã„ã¦è¦ç‚¹ã‚’ç®‡æ¡æ›¸ãâ†’çŸ­ã„ã¾ã¨ã‚ã®é †ã§æ•´ç†ã—ã¦ãã ã•ã„ã€‚\n\n# ãƒ’ãƒƒãƒˆã‚¹ãƒ‹ãƒšãƒƒãƒˆ\n{snippets}",
        height=120,
    )

    # ã€Œã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚° / ä¸€æ‹¬è¡¨ç¤ºã€ã®åˆ‡æ›¿ï¼ˆgpt_responder ãŒå†…éƒ¨ã§è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚å®Ÿè£…ï¼‰
    summary_display_mode = st.radio(
        "è¦ç´„ã®è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰",
        ["é€æ¬¡è¡¨ç¤ºï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰", "ä¸€æ‹¬è¡¨ç¤º"],
        index=0,
        help="gpt-5ç³»ã¯Responsesã®streamã€‚ä¸€éƒ¨ç’°å¢ƒã§ã¯è‡ªå‹•ã§ä¸€æ‹¬è¡¨ç¤ºã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚",
    )

    debug_mode = st.checkbox("ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º", value=False)

# ============== æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ  ==============
st.markdown("### ã‚¯ã‚¨ãƒª")

# ã‚¯ã‚¨ãƒªæœ¬ä½“ + AND/OR ã®ãƒ–ãƒ¼ãƒ«ãƒ¢ãƒ¼ãƒ‰
c1, c2 = st.columns([3, 2])
with c1:
    query = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆç©ºç™½åŒºåˆ‡ã‚Šã§ AND / OR æŒ‡å®šï¼‰", value="")
with c2:
    bool_mode = st.radio("ãƒ¢ãƒ¼ãƒ‰", ["AND", "OR"], index=0, horizontal=True)

# æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆæ­£è¦è¡¨ç¾ / å¤§æ–‡å­—å°æ–‡å­— / æ—¥æœ¬èªæ­£è¦åŒ– / æœ¬æ–‡æ­£è¦åŒ–ï¼‰
c3, c4, c5, c6 = st.columns(4)
with c3:
    use_regex = st.checkbox("æ­£è¦è¡¨ç¾", value=False)
with c4:
    case_sensitive = st.checkbox("å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥", value=False)
with c5:
    normalize_query = st.checkbox("æ—¥æœ¬èªã‚¹ãƒšãƒ¼ã‚¹æ­£è¦åŒ–ï¼ˆæ¨å¥¨ï¼‰", value=True)
with c6:
    norm_body = st.checkbox("æœ¬æ–‡ã‚‚æ­£è¦åŒ–ã—ã¦æ¤œç´¢", value=True, help="å–ã‚Šè¾¼ã¿æ™‚ã«æ­£è¦åŒ–ã—ã¦ã„ãªã„ã‚³ãƒ¼ãƒ‘ã‚¹å‘ã‘")

# æ¤œç´¢å®Ÿè¡Œãƒœã‚¿ãƒ³
go = st.button("æ¤œç´¢ã‚’å®Ÿè¡Œ", type="primary")

# ============== æ¤œç´¢ã®å®Ÿè¡Œï¼ˆçµæœã¯ session_state ã«ä¿å­˜ï¼‰ ==============
if go:
    try:
        # æ¤œç´¢å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰æœªé¸æŠã®æ—©æœŸä¸­æ–­
        if not sel_shards:
            st.warning("å°‘ãªãã¨ã‚‚1ã¤ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        # ã‚¯ã‚¨ãƒª â†’ ãƒ‘ã‚¿ãƒ¼ãƒ³ç¾¤ã¸
        pats = compile_terms(
            query, use_regex=use_regex, case_sensitive=case_sensitive, normalize_query=normalize_query
        )
        if not pats:
            st.warning("æ¤œç´¢èªãŒç©ºã§ã™ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        rows: List[Dict[str, Any]] = []
        total_scanned = 0  # èµ°æŸ»ä»¶æ•°ï¼ˆãƒ‡ãƒãƒƒã‚°/ç›®å®‰ï¼‰

        # é¸æŠã•ã‚ŒãŸã‚·ãƒ£ãƒ¼ãƒ‰ã‚’é †ã«èµ°æŸ»
        for sid in sel_shards:
            meta_path = BASE_DIR / sid / "meta.jsonl"
            if not meta_path.exists():
                st.warning(f"{meta_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                continue

            # jsonl ã‚’1è¡Œãšã¤èª­ã¿ãªãŒã‚‰æ—©æœŸæ‰“ã¡åˆ‡ã‚Šï¼ˆæœ€å¤§è¡¨ç¤ºä»¶æ•°ï¼‰
            for obj in iter_jsonl(meta_path):
                total_scanned += 1

                # å¹´ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆyear ãŒ int ã®ã¨ãã®ã¿é©ç”¨ï¼‰
                yr = obj.get("year")
                if isinstance(yr, int):
                    if year_min and yr < year_min:
                        continue
                    if year_max < 9999 and yr > year_max:
                        continue

                # ãƒ•ã‚¡ã‚¤ãƒ«åãƒ•ã‚£ãƒ«ã‚¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰
                if (f := str(obj.get("file", ""))) and file_filter and file_filter.lower() not in f.lower():
                    continue

                # æœ¬æ–‡æ­£è¦åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆå–ã‚Šè¾¼ã¿æ™‚ã®æ­£è¦åŒ–å·®ç•°ã‚’å¸åã™ã‚‹ãŸã‚ã®å¦¥å”ç­–ï¼‰
                text = str(obj.get("text", ""))
                tgt = normalize_ja_text(text) if norm_body else text

                # AND/OR ãƒ¢ãƒ¼ãƒ‰ã§ãƒãƒƒãƒåˆ¤å®š
                ok = all(p.search(tgt) for p in pats) if bool_mode == "AND" else any(p.search(tgt) for p in pats)
                if not ok:
                    continue

                # ç°¡æ˜“ã‚¹ã‚³ã‚¢ï¼šãƒãƒƒãƒå›æ•°ã®ç·å’Œï¼ˆã‚½ãƒ¼ãƒˆç”¨ï¼‰
                score = sum(1 for p in pats for _ in p.finditer(tgt))

                # è¡¨ç¤ºè¡Œã‚’è¿½åŠ 
                rows.append(
                    {
                        "file": obj.get("file"),
                        "year": obj.get("year"),
                        "page": obj.get("page"),
                        "shard_id": obj.get("shard_id", sid),
                        "chunk_id": obj.get("chunk_id"),
                        "chunk_index": obj.get("chunk_index"),
                        "score": int(score),
                        "text": make_snippet(text, pats, total_len=int(snippet_len)),
                    }
                )

                # è¦å®šæ•°ã«é”ã—ãŸã‚‰ã‚·ãƒ£ãƒ¼ãƒ‰å•ã‚ãšæ‰“ã¡åˆ‡ã‚Š
                if len(rows) >= int(max_rows):
                    break
            if len(rows) >= int(max_rows):
                break

        if not rows:
            st.warning("ãƒ’ãƒƒãƒˆãªã—ã€‚æ¤œç´¢èªã‚„ãƒ•ã‚£ãƒ«ã‚¿ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        # æ¤œç´¢çµæœãƒ»è¨­å®šã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜ï¼ˆãƒœã‚¿ãƒ³å¾Œã®å†æç”»ã§ã‚‚å†åˆ©ç”¨ï¼‰
        st.session_state["kw_rows"] = rows
        st.session_state["kw_scanned"] = total_scanned
        st.session_state["kw_show_order"] = [c for c in show_cols if c in rows[0].keys()] or [
            "file",
            "year",
            "page",
            "shard_id",
            "score",
            "text",
        ]
        st.session_state["kw_sort_cols"] = ["score", "year", "file", "page"]
        st.session_state["kw_query"] = query

        # è¦ç´„ç”¨è¨­å®šã®ä¿å­˜ï¼ˆãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ»è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰ç­‰ï¼‰
        st.session_state["kw_gen_cfg"] = dict(
            OPENAI_API_KEY=OPENAI_API_KEY,
            model=model,
            temperature=float(temperature),
            max_tokens=int(max_tokens),
            topn=int(topn_snippets),
            sys_prompt=sys_prompt,
            user_prompt_tpl=user_prompt_tpl,
            summary_display_mode=summary_display_mode,
        )

        st.success(f"ãƒ’ãƒƒãƒˆ {len(rows):,d} ä»¶ / èµ°æŸ» {total_scanned:,d} ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆä¸Šä½ã®ã¿è¡¨ç¤ºï¼‰")

    except Exception:
        st.error("æ¤œç´¢å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", icon="ğŸ›‘")
        if debug_mode:
            st.code("".join(traceback.format_exc()))

# ============== å…±é€šæç”»ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆæ¤œç´¢ç›´å¾Œ / ãƒœã‚¿ãƒ³æŠ¼ä¸‹å¾Œã®ä¸¡æ–¹ã§å®Ÿè¡Œï¼‰ ==============
if st.session_state.get("kw_rows"):
    rows_saved: List[Dict[str, Any]] = st.session_state["kw_rows"]
    sort_cols = st.session_state.get("kw_sort_cols", ["score", "year", "file", "page"])
    show_order = st.session_state.get("kw_show_order", ["file", "year", "page", "shard_id", "score", "text"])

    # è¡¨ç¤ºå‰ã« DataFrame åŒ– & ä¸¦ã³æ›¿ãˆ
    df = pd.DataFrame(rows_saved).sort_values(sort_cols, ascending=[False, True, True, True])

    # 1) ãƒ’ãƒƒãƒˆä¸€è¦§ï¼ˆè¡¨ï¼‰â€” æ¨ªå¹…ã„ã£ã±ã„ä½¿ã†
    st.dataframe(df[[c for c in show_order if c != "text"]], width="stretch", height=420)

    # 2) CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆUTF-8 BOMä»˜ãï¼‰
    csv_bytes = df[show_order].to_csv(index=False).encode("utf-8-sig")
    st.download_button("ğŸ“¥ CSV ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv_bytes, file_name="keyword_hits.csv", mime="text/csv")

    # 3) ãƒ’ãƒƒãƒˆã‚¹ãƒ‹ãƒšãƒƒãƒˆï¼ˆå¸¸ã«è¡¨ç¤ºã•ã‚ŒãŸã¾ã¾ã«ã—ã¦ã€çµã‚Šè¾¼ã¿ã¨ç‹¬ç«‹ï¼‰
    if "text" in show_order:
        st.divider()
        with st.expander("ãƒ’ãƒƒãƒˆã‚¹ãƒ‹ãƒšãƒƒãƒˆï¼ˆã‚¯ãƒªãƒƒã‚¯ã§å±•é–‹ï¼‰", expanded=False):
            # è¡¨ã‚’å¤§é‡ã«å‡ºã™ã¨é‡ããªã‚‹ãŸã‚ã€ã‚¹ãƒ‹ãƒšãƒƒãƒˆã¯ä¸Šä½200ä»¶ã«é™å®š
            for i, row in df.head(200).iterrows():
                colA, colB = st.columns([4, 1])
                with colA:
                    st.markdown(
                        f"**{row.get('file')}**  year={row.get('year')}  p.{row.get('page')}  score={row.get('score')}",
                        help=row.get("chunk_id"),
                    )
                    st.markdown(row.get("text", ""), unsafe_allow_html=True)
                with colB:
                    # ã‚¯ãƒªãƒƒã‚¯ã§ year/file ã‚’ã‚³ãƒ”ãƒ¼ã™ã‚‹è»½é‡ãª HTML ãƒœã‚¿ãƒ³
                    payload = json.dumps(str(row.get("file")), ensure_ascii=False)
                    st.components.v1.html(
                        f"""
                    <button id="cpy_{i}" style="padding:6px 10px;border-radius:8px;border:1px solid #dadce0;background:#fff;cursor:pointer;font-size:0.9rem;">ğŸ“‹ year/file ã‚’ã‚³ãƒ”ãƒ¼</button>
                    <script>
                      const b=document.getElementById("cpy_{i}");
                      b&&b.addEventListener("click",async()=>{{
                        try{{await navigator.clipboard.writeText({payload});
                          const o=b.innerText;b.innerText="âœ… ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ";setTimeout(()=>{{b.innerText=o}},1200);
                        }}catch(e){{alert("ã‚³ãƒ”ãƒ¼ã«å¤±æ•—: "+e)}}
                      }});
                    </script>
                    """,
                        height=38,
                    )

    # 4) ğŸ§  è¦ç´„ãƒœã‚¿ãƒ³ï¼ˆå¹…ã„ã£ã±ã„ï¼‰
    st.divider()
    gen_clicked = st.button("ğŸ§  è¦ç´„ã‚’ç”Ÿæˆ", type="primary", width="stretch")

    if gen_clicked:
        # ä¿å­˜æ¸ˆã¿ã®ç”Ÿæˆè¨­å®šã‚’å–å¾—
        cfg = st.session_state.get("kw_gen_cfg", {})
        OPENAI_API_KEY = cfg.get("OPENAI_API_KEY")
        model = cfg.get("model", "gpt-5-mini")
        temperature = cfg.get("temperature", 1.0 if is_gpt5(model) else 0.2)
        max_tokens = cfg.get("max_tokens", 2000)
        topn_snippets = cfg.get("topn", 30)
        sys_prompt = cfg.get("sys_prompt", "ã‚ãªãŸã¯äº‹å®Ÿã«å¿ å®Ÿãªãƒªã‚µãƒ¼ãƒã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ—¥æœ¬èªã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚")
        user_prompt_tpl = cfg.get("user_prompt_tpl", "ã‚¯ã‚¨ãƒªã€{query}ã€\n{snippets}")
        query = st.session_state.get("kw_query", "")
        summary_display_mode = cfg.get("summary_display_mode", "é€æ¬¡è¡¨ç¤ºï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰")

        # ---- è¦ç´„å¯¾è±¡ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’æº–å‚™ï¼ˆä¸Šä½Nä»¶ï¼‰----
        labelled = [
            f"---\n# Source: {r.get('file')} p.{r.get('page')} (score={r.get('score')})\n{strip_html(str(r.get('text', '')))}"
            for _, r in df.head(int(topn_snippets)).iterrows()
        ]

        # ---- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆäºˆç®—ã«åã¾ã‚‹ã‚ˆã†ã«èª¿æ•´ ----
        user_prefix = user_prompt_tpl.replace("{snippets}", "").format(query=query, snippets="")
        context_limit, safety_margin = (128_000, 2_000) if is_gpt5(model) else (128_000, 1_000)
        fitted = fit_to_budget(
            labelled,
            model=model,
            sys_prompt=sys_prompt,
            user_prefix=user_prefix,
            want_output=int(max_tokens),
            context_limit=context_limit,
            safety_margin=safety_margin,
        )

        # ---- ç”Ÿæˆã®å®Ÿè¡Œï¼ˆgpt_responder â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰----
        st.subheader("ğŸ§  ç”Ÿæˆè¦ç´„")

        # OpenAI æœªè¨­å®š or äºˆç®—çš„ã«ç©ºï¼ˆfitted ãŒç©ºï¼‰â†’ ãƒ­ãƒ¼ã‚«ãƒ«æŠ½å‡ºã‚µãƒãƒª
        if not fitted or not OPENAI_API_KEY:
            with st.spinner("ğŸ§© ãƒ­ãƒ¼ã‚«ãƒ«æŠ½å‡ºã‚µãƒãƒªã‚’ç”Ÿæˆä¸­â€¦"):
                st.markdown(local_summary(labelled if not fitted else fitted, max_sent=12))
        else:
            snippets_text = "\n\n".join(fitted)
            user_prompt = user_prompt_tpl.format(query=query, snippets=snippets_text)
            approx_in = count_tokens(user_prompt, model) + count_tokens(sys_prompt, model)
            st.caption(
                f"ï¼ˆæ¨å®šå…¥åŠ› ~{approx_in:,} tok / å‡ºåŠ›ä¸Šé™ {int(max_tokens):,} tok / ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ~{context_limit:,} tokï¼‰"
            )

            try:
                responder = GPTResponder(api_key=OPENAI_API_KEY)

                if summary_display_mode == "é€æ¬¡è¡¨ç¤ºï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰":
                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºï¼ˆå†…éƒ¨ã§å¿…è¦ã«å¿œã˜ã¦ä¸€æ‹¬ã¸è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                    with st.spinner("ğŸ§  è¦ç´„ã‚’ç”Ÿæˆä¸­â€¦"):
                        st.write_stream(
                            responder.stream(
                                model=model,
                                system_instruction=sys_prompt.strip(),
                                user_content=user_prompt,
                                max_output_tokens=int(max_tokens),
                                on_error_text="Responses stream error.",
                            )
                        )
                    # å¿…è¦ã§ã‚ã‚Œã° usage ã‚’è¡¨ç¤ºã™ã‚‹ã“ã¨ã‚‚å¯èƒ½
                    # st.caption(f"in={responder.usage.input_tokens} / out={responder.usage.output_tokens}")

                else:
                    # ä¸€æ‹¬è¡¨ç¤ºï¼ˆå®Œäº†å¾Œã«ä¸€æ°—ã«å‡ºã™ï¼‰
                    with st.spinner("ğŸ§  è¦ç´„ã‚’ç”Ÿæˆä¸­â€¦"):
                        result: CompletionResult = responder.complete(
                            model=model,
                            system_instruction=sys_prompt.strip(),
                            user_content=user_prompt,
                            max_output_tokens=int(max_tokens),
                        )
                    out = (result.text or "").strip()
                    if out:
                        st.markdown(out)
                    else:
                        # ã¾ã‚Œã«ç©ºæ–‡å­—ãŒè¿”ã‚‹ã‚±ãƒ¼ã‚¹ã¸ã®æ•‘æ¸ˆ
                        with st.spinner("ğŸ§© ãƒ­ãƒ¼ã‚«ãƒ«æŠ½å‡ºã‚µãƒãƒªã‚’ç”Ÿæˆä¸­â€¦"):
                            st.info("âš ï¸ ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ãŒç©ºã ã£ãŸãŸã‚ã€ãƒ­ãƒ¼ã‚«ãƒ«æŠ½å‡ºã‚µãƒãƒªã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
                            st.markdown(local_summary(fitted, max_sent=12))

            except Exception as e:
                # OpenAI/é€šä¿¡/SDKå·®åˆ†ãªã©ã€ç”Ÿæˆæ™‚ã®ä¾‹å¤–ã¯ãƒ­ãƒ¼ã‚«ãƒ«æŠ½å‡ºã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if debug_mode:
                    st.error(f"OpenAI / è¦ç´„ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}", icon="ğŸ›‘")
                    st.code("".join(traceback.format_exc()))
                with st.spinner("ğŸ§© ãƒ­ãƒ¼ã‚«ãƒ«æŠ½å‡ºã‚µãƒãƒªã‚’ç”Ÿæˆä¸­â€¦"):
                    st.markdown(local_summary(fitted, max_sent=12))

# ============== åˆæœŸã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ ==============
if not st.session_state.get("kw_rows"):
    # åˆå›è¡¨ç¤ºã‚„ã€æ¤œç´¢æœªå®Ÿè¡Œæ™‚ã®ç°¡å˜ãªæ¡ˆå†…
    st.info("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§æ¡ä»¶ã‚’è¨­å®šã—ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ã€æ¤œç´¢ã‚’å®Ÿè¡Œã€ã—ã¦ãã ã•ã„ã€‚")
