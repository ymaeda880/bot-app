# pages/28_ãƒœãƒƒãƒˆï¼ˆé–‹ç™ºç”¨ï¼‰.py
# =============================================================================
# ğŸ’¬ Internal Bot (RAG, Shards) â€” year/pno ãƒ•ã‚£ãƒ«ã‚¿ä»˜ãï¼ˆå€™è£œãƒ—ãƒªãƒ•ã‚£ãƒ«ã‚¿ï¼‹TopKãƒ–ãƒ¼ã‚¹ãƒˆï¼‰
# + â± å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°è¨ˆæ¸¬ï¼ˆVectorDB èµ°æŸ» / åŸ‹ã‚è¾¼ã¿ / æ¤œç´¢ / GPT API / ã‚¹ãƒˆãƒªãƒ¼ãƒ ï¼‰
# + ğŸ‘¤ ãƒ­ã‚°ã‚¤ãƒ³è¡¨ç¤ºãƒãƒƒã‚¸ï¼ˆå³ä¸Šï¼‰
# + ğŸ“ JSONL ãƒ­ã‚°ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®ã¿ / ãƒ—ãƒªã‚»ãƒƒãƒˆç›´é€ã¯é™¤å¤– / è©³ã—ã•ã¯æ—¥æœ¬èªã¨è‹±èªã‚³ãƒ¼ãƒ‰ï¼‰
# =============================================================================

from __future__ import annotations
from pathlib import Path
from typing import List, Tuple, Dict, Any, Iterable, Set
import heapq
from itertools import count
import json

import streamlit as st
import numpy as np

from config.path_config import PATHS
from config.sample_questions import SAMPLES, ALL_SAMPLES
from lib.text_normalize import normalize_ja_text
from lib.prompts.bot_prompt import build_prompt
from lib.gpt_responder import GPTResponder, CompletionResult
from lib.rag.rag_utils import EmbeddingStore, NumpyVectorDB
from lib.costs import (
    MODEL_PRICES_USD, EMBEDDING_PRICES_USD, DEFAULT_USDJPY,
    ChatUsage, estimate_chat_cost, estimate_embedding_cost, usd_to_jpy,
    render_usage_summary, _model_prices_per_1k
)
from lib.openai_utils import count_tokens
from lib.bot_utils import (
    list_shard_dirs_openai, norm_path, fmt_source,
    enrich_citations, citation_tag, get_openai_api_key,
    parse_inline_files, strip_inline_files,
    # æ–°è¦ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    to_halfwidth_digits, clean_pno_token, parse_years, parse_pnos, norm_pno_forms,
    year_ok, pno_ok, file_ok, scan_candidate_files, filters_caption,
)

from lib.metrics.timing_utils import Timings, stream_with_timing, render_metrics_ui
import datetime as dt
import time


from io import BytesIO
try:
    from docx import Document
    from docx.shared import Pt
    from docx.enum.text import WD_ALIGN_PARAGRAPH
except Exception:
    Document = None  # python-docx æœªå°å…¥æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨



# ============================================================
# sys.path ã« common_lib ã‚’ç¢ºå®Ÿã«è¿½åŠ ï¼ˆä¸Šæ–¹æ¢ç´¢ï¼‰
# ============================================================
import sys
from pathlib import Path

_THIS = Path(__file__).resolve()
# ä¾‹: .../projects/bot_project/bot_app/pages/10_ãƒœãƒƒãƒˆ.py
APP_DIR = _THIS.parents[1]        # .../bot_app
PROJ_DIR = _THIS.parents[2]       # .../bot_project
MONO_ROOT = _THIS.parents[3]      # .../projects  â† common_lib ãŒã“ã“ç›´ä¸‹ã«ã‚ã‚‹æƒ³å®š

for p in (MONO_ROOT, PROJ_DIR, APP_DIR):
    if str(p) not in sys.path:
        sys.path.insert(0, str(p))
# ============================================================

# === è¿½åŠ ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ­ã‚°ã‚¤ãƒ³è¡¨ç¤ºãƒ»JSONLãƒ­ã‚¬ãƒ¼ï¼‰ ============================
from common_lib.auth.auth_helpers import get_current_user_from_session_or_cookie
from common_lib.logs.jsonl_logger import JsonlLogger, sha256_short

# ã‚¢ãƒ—ãƒªï¼ãƒšãƒ¼ã‚¸æƒ…å ±ã¨ãƒ­ã‚¬ãƒ¼åˆæœŸåŒ–
_APP_DIR = Path(__file__).resolve().parents[1]
_PAGE_NAME = Path(__file__).stem
logger = JsonlLogger(app_dir=_APP_DIR, page_name=_PAGE_NAME)

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ¬æ–‡ã‚’ãƒ­ã‚°ä¿å­˜ã™ã‚‹ã‹
INCLUDE_FULL_PROMPT_IN_LOG = True


# ===== ä¾¡æ ¼ãƒ†ãƒ¼ãƒ–ãƒ«æ•´å½¢ï¼ˆUSD/1K tokï¼‰ =======================================
MODEL_PRICES_PER_1K = _model_prices_per_1k()

VS_ROOT: Path = PATHS.vs_root

# ===== wordã¸å‡ºåŠ›ã™ã‚‹ãŸã‚ã®é–¢æ•° =======================================
def _build_docx(prompt_text: str, answer_text: str, meta: Dict[str, Any], filters: Dict[str, Any] | None = None) -> bytes:
    if Document is None:
        raise RuntimeError("python-docx ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`pip install python-docx` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    doc = Document()

    # ã‚¿ã‚¤ãƒˆãƒ«
    title = doc.add_paragraph("Internal Bot å¿œç­”")
    title.runs[0].font.size = Pt(16)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER

    # ãƒ¡ã‚¿æƒ…å ±
    doc.add_paragraph("")
    m = doc.add_paragraph()
    m.add_run("Meta").bold = True
    doc.add_paragraph(f"User: {meta.get('user') or '(anonymous)'}")
    doc.add_paragraph(f"Model: {meta.get('chat_model')}")
    doc.add_paragraph(f"Detail: {meta.get('detail_label')} ({meta.get('detail')})")
    doc.add_paragraph(f"Max Tokens: {meta.get('max_tokens')}")
    doc.add_paragraph(f"Top-K: {meta.get('top_k')}")
    doc.add_paragraph(f"Generated At: {meta.get('ts_jst')}")
  
    # ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆæŒ‡å®šãŒã‚ã‚‹ã¨ãã ã‘ï¼‰
    if filters:
        years = filters.get("years") or []
        pnos = filters.get("pnos") or []
        files = filters.get("file_whitelist") or []
        shards = filters.get("shards") or []

        # ã„ãšã‚Œã‹ã«è¦ç´ ãŒã‚ã‚Œã°ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¿½åŠ 
        if any([years, pnos, files, shards]):
            doc.add_paragraph("")
            f_hdr = doc.add_paragraph("Filters")
            f_hdr.runs[0].bold = True

            if years:
                doc.add_paragraph(f"year: {', '.join(map(str, years))}")
            if pnos:
                doc.add_paragraph(f"pno: {', '.join(pnos)}")
            # if files:
            #     doc.add_paragraph("files:")
            #     for s in files[:200]:
            #         doc.add_paragraph(f" - {s}")
            #     if len(files) > 200:
            #         doc.add_paragraph(f" ... and {len(files)-200} more")
            if shards:
                doc.add_paragraph(f"shards: {', '.join(shards)}")






    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    doc.add_paragraph("")
    p_hdr = doc.add_paragraph("è³ªå•ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰")
    p_hdr.runs[0].bold = True
    for ln in (prompt_text or "").splitlines():
        doc.add_paragraph(ln)

    # å›ç­”
    doc.add_paragraph("")
    a_hdr = doc.add_paragraph("å›ç­”")
    a_hdr.runs[0].bold = True
    for ln in (answer_text or "").splitlines():
        doc.add_paragraph(ln)

    bio = BytesIO()
    doc.save(bio)
    return bio.getvalue()



# ===== Streamlit åŸºæœ¬è¨­å®šï¼ã‚¿ã‚¤ãƒˆãƒ« ==========================================
st.set_page_config(page_title="Chat Bot (Sharded)", page_icon="ğŸ’¬", layout="wide")

# â–¼â–¼ ãƒ­ã‚°ã‚¤ãƒ³è¡¨ç¤ºä»˜ãã‚¿ã‚¤ãƒˆãƒ«ï¼ˆå³ä¸Šã«ãƒãƒƒã‚¸ï¼‰ â–¼â–¼
col_title, col_user = st.columns([5, 2], vertical_alignment="center")
with col_title:
    st.title("ğŸ’¬ Internal Bot (RAG, Shards)")
with col_user:
    current_user, _payload = get_current_user_from_session_or_cookie(st)
    if current_user:
        st.success(f"ãƒ­ã‚°ã‚¤ãƒ³ä¸­: **{current_user}**")
    else:
        st.warning("æœªãƒ­ã‚°ã‚¤ãƒ³ï¼ˆCookie æœªæ¤œå‡ºï¼‰")

if "q" not in st.session_state:
    st.session_state.q = ""

def _set_q(x: str) -> None:
    st.session_state.q = x or ""


# â–¼ ä½¿ã„æ–¹ï¼ˆãƒ˜ãƒ«ãƒ—ï¼‰
with st.expander("â„¹ï¸ ã“ã®ãƒšãƒ¼ã‚¸ã®ä½¿ã„æ–¹ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®šã‚’å«ã‚€ï¼‰", expanded=False):
    st.markdown("""
### 1) ä½•ãŒã§ãã‚‹ï¼Ÿ
- ç¤¾å†…PDFã‚’ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã—ãŸãƒ™ã‚¯ãƒˆãƒ«DBã‹ã‚‰ **RAG æ¤œç´¢ï¼‹å›ç­”ç”Ÿæˆ** ã‚’è¡Œã„ã¾ã™ã€‚
- **year / pno ãƒ•ã‚£ãƒ«ã‚¿**ã‚„**å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«é™å®š**ã§å¯¾è±¡ã‚’çµã‚Šè¾¼ã¿å¯èƒ½ã€‚
- ç”Ÿæˆã—ãŸ **å›ç­”ï¼‹ã‚ãªãŸã®è³ªå•**ã‚’ **Wordï¼ˆ.docxï¼‰** ã§ä¿å­˜ã§ãã¾ã™ï¼ˆãƒ•ã‚£ãƒ«ã‚¿æƒ…å ±ã‚‚å‡ºåŠ›ï¼‰ã€‚

---

### 2) ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ä¸»ãªè¨­å®š
- **ãƒ¢ãƒ‡ãƒ«ï¼ˆResponsesï¼‰**: å›ç­”ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠï¼ˆä¾‹: `gpt-5-mini`ï¼‰ã€‚
- **æ¤œç´¢ä»¶æ•°ï¼ˆTop-Kï¼‰**: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ã®ä»¶æ•°ã€‚ï¼ˆé©å®œèª¿æ•´ã—ã¦ãã ã•ã„ï¼ï¼‰
- **è©³ã—ã•**: å‡ºåŠ›ã®ç²’åº¦ï¼ˆã€Œç°¡æ½”/æ¨™æº–/è©³ç´°/è¶…è©³ç´°ã€ï¼‰ã€‚  
  â€» ãƒ­ã‚°ã«ã¯ **æ—¥æœ¬èªãƒ©ãƒ™ãƒ«**ã¨**è‹±èªã‚³ãƒ¼ãƒ‰**ã®ä¸¡æ–¹ï¼ˆä¾‹: `è©³ç´°` / `detailed`ï¼‰ãŒä¿å­˜ã•ã‚Œã¾ã™ã€‚
- **å‡ºå…¸ã‚’ [S1] ã§ä¿ƒã™**: å›ç­”ã«å‡ºå…¸ã‚¿ã‚°ã‚’å«ã‚ã‚‹ã‹ã€‚
- **æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³**: å›ç­”ã®é•·ã•ä¸Šé™ã€‚ï¼ˆã“ã‚Œã¯å¤‰æ›´ã®å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ï¼‰
- **å›ç­”ç”Ÿæˆ**:  ï¼ˆã“ã‚Œã¯å¤‰æ›´ã®å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ï¼‰
  - `OpenAI`â€¦æ¤œç´¢ï¼‹è¦ç´„ï¼ˆé€šå¸¸ï¼‰  
  - `Retrieve-only`â€¦**è¦ç´„ãªã—**ã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿è¡¨ç¤º
- **System Instruction**: ãƒ¢ãƒ‡ãƒ«ã¸ã®å‰ææŒ‡ç¤ºï¼ˆä»»æ„ï¼‰ã€‚ï¼ˆã“ã‚Œã¯å¤‰æ›´ã®å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ï¼‰
- **è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰**: é€æ¬¡ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ ï¼‰or ä¸€æ‹¬ã€‚ï¼ˆã“ã‚Œã¯å¤‰æ›´ã®å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ï¼‰
- **æ¤œç´¢å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰**: ã©ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’æ¤œç´¢ã™ã‚‹ã‹ï¼ˆæœªé¸æŠ=ã™ã¹ã¦ï¼‰ã€‚
- **year / pno ãƒ•ã‚£ãƒ«ã‚¿**: å¹´ã‚„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç•ªå·ã§å¯¾è±¡ã‚’çµã‚Šè¾¼ã¿ã€‚æœªå…¥åŠ›ãªã‚‰å…¨ä»¶ã€‚
- **å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä»»æ„ï¼‰**: `2025/foo.pdf, 2024/bar.pdf` ã®ã‚ˆã†ã« **ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ã ã‘**ã«é™å®šã—ãŸã„ã¨ãã«æŒ‡å®šã€‚ï¼ˆã“ã‚Œã¯å¤‰æ›´ã®å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ï¼‰
- **ğŸ§ª ãƒ‡ãƒ¢ç”¨ã‚µãƒ³ãƒ—ãƒ«è³ªå•**:  
  - ã€Œâ¬‡ï¸ ã“ã®è³ªå•ã‚’å…¥åŠ›æ¬„ã¸ã‚»ãƒƒãƒˆã€ã§å…¥åŠ›æ¬„ã«ã‚³ãƒ”ãƒ¼  
  - ã€ŒğŸš€ ã‚µãƒ³ãƒ—ãƒ«ã§å³é€ä¿¡ã€ã§å³å®Ÿè¡Œ  

> å‚è€ƒ: ä¸‹æ®µã® **è§£æ±ºãƒ‘ã‚¹** ã¯ãƒ‘ã‚¹ã®ç¢ºèªç”¨ã§ã™ï¼ˆç·¨é›†ã¯ã§ãã¾ã›ã‚“ï¼‰ã€‚

---

### 3) ä½¿ã„æ–¹ã®æµã‚Œ
1. ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‚’è¨­å®šã™ã‚‹ï¼ˆå¿…è¦ã«å¿œã˜ã¦ year/pno/ãƒ•ã‚¡ã‚¤ãƒ«é™å®šã‚‚æŒ‡å®šï¼‰ã€‚
2. ç”»é¢ä¸­å¤®ã®ã€Œè³ªå•ã‚’å…¥åŠ›ã€ã«èããŸã„å†…å®¹ã‚’å…¥åŠ›ã€‚
3. **é€ä¿¡** ã‚’æŠ¼ã™ã€‚  
   - é€æ¬¡è¡¨ç¤ºã‚’é¸ã‚“ã§ã„ã‚Œã°ã€å›ç­”ãŒã‚¹ãƒˆãƒªãƒ¼ãƒ ã§å‡ºã¾ã™ã€‚  
   - å›ç­”ä¸‹éƒ¨ã®ã€ŒğŸ” å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€ã§ã€ä½¿ã‚ã‚ŒãŸä¸Šä½æ–‡è„ˆã‚’ç¢ºèªã§ãã¾ã™ã€‚
4. å›ç­”ãŒå‡ºãŸã‚‰ã€**ã€Œâ¬‡ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‹å›ç­”ã‚’ Word ã§ä¿å­˜ã€** ãƒœã‚¿ãƒ³ã§ .docx ã‚’ä¿å­˜ã€‚  
   - Word ã«ã¯ **è³ªå•ãƒ»å›ç­”ãƒ»ãƒ¡ã‚¿æƒ…å ±ï¼ˆãƒ¢ãƒ‡ãƒ«/è©³ã—ã•/Top-Kç­‰ï¼‰**ã«åŠ ãˆã€  
     æŒ‡å®šã—ãŸ **year/pno/ã‚·ãƒ£ãƒ¼ãƒ‰/ãƒ•ã‚¡ã‚¤ãƒ«é™å®š** ã®ãƒ•ã‚£ãƒ«ã‚¿ã‚‚è¨˜éŒ²ã•ã‚Œã¾ã™ã€‚  

---

### 4) ç”»é¢å„éƒ¨ã®è¦‹æ–¹
- **ğŸ§  å›ç­”**: ç”Ÿæˆå›ç­”ã€‚å‡ºå…¸ã‚¿ã‚° `[S1]` ç­‰ã¯å¾Œè¿°ã®æ‹¡å¼µã§å±•é–‹ã•ã‚Œã¾ã™ã€‚
- **ğŸ“ å‡ºå…¸æ‹¡å¼µæ¸ˆã¿æœ€çµ‚ãƒ†ã‚­ã‚¹ãƒˆ**: `[Sx]` ã‚’ã‚½ãƒ¼ã‚¹è¡¨è¨˜ã«å±•é–‹ã—ãŸä¸€è¦§ã€‚
- **ğŸ” å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ**: ä¸Šä½ãƒ’ãƒƒãƒˆã®ã‚¹ãƒ‹ãƒšãƒƒãƒˆã¨ãƒ¡ã‚¿ï¼ˆscore, year, pno ãªã©ï¼‰ã€‚
- **ğŸ“Š ä½¿ç”¨é‡ã®æ¦‚ç®—**: åŸ‹ã‚è¾¼ã¿/ãƒãƒ£ãƒƒãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒ»æ¦‚ç®—ã‚³ã‚¹ãƒˆã€‚
- **ğŸ§  ãƒ¡ãƒ¢ãƒªçŠ¶æ³ï¼ˆå›ç­”å‰/å¾Œï¼‰**: å®Ÿè¡Œå‰å¾Œã®ãƒ¡ãƒ¢ãƒªã®ç°¡æ˜“ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€‚

---

### 5) ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
- **ã€Œæ¤œç´¢å¯èƒ½ãªã‚·ãƒ£ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€**: å…ˆã«ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼ˆ`vectors.npy` ãŒå¿…è¦ï¼‰ã€‚
- **è©²å½“ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„**: Top-K ã‚’å¢—ã‚„ã™ / **ã‚¯ã‚¨ãƒªã‚’å…·ä½“åŒ–ï¼ˆå…·ä½“çš„ãªè³ªå•ã‚’è¡Œãªã£ã¦ãã ã•ã„ï¼‰** / ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’çµã‚‹ã€‚
    """)


####################
# ãƒ¡ãƒ¢ãƒªç›£è¦–ï¼ˆä»»æ„ï¼‰
#####################
from lib.monitors.ui_memory_monitor import render_memory_kpi_row

st.divider()
st.markdown("### ğŸ§  ãƒ¡ãƒ¢ãƒªçŠ¶æ³ï¼ˆå‚è€ƒï¼‰")
render_memory_kpi_row()

# ===== ã‚µã‚¤ãƒ‰ãƒãƒ¼ ==============================================================
with st.sidebar:
    st.header("è¨­å®š")

    model_candidates = [m for m in MODEL_PRICES_PER_1K if m.startswith(("gpt-5", "gpt-4.1"))]
    all_models_sorted = sorted(model_candidates, key=lambda x: (0 if x.startswith("gpt-5") else 1, x))
    if not all_models_sorted:
        st.error("åˆ©ç”¨å¯èƒ½ãª Responses ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚lib/costs.MODEL_PRICES_USD ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    default_idx = all_models_sorted.index("gpt-5-mini") if "gpt-5-mini" in all_models_sorted else 0
    chat_model = st.selectbox("ãƒ¢ãƒ‡ãƒ«ï¼ˆResponsesï¼‰", all_models_sorted, index=default_idx)

    top_k = st.slider("æ¤œç´¢ä»¶æ•°ï¼ˆTop-Kï¼‰", 1, 12, 6, 1)

    # â–¼â–¼ è©³ã—ã•ï¼šæ—¥æœ¬èªãƒ©ãƒ™ãƒ«ä¿æŒï¼‹è‹±èªã‚³ãƒ¼ãƒ‰å¤‰æ› â–¼â–¼
    _detail_label = st.selectbox("è©³ã—ã•", ["ç°¡æ½”", "æ¨™æº–", "è©³ç´°", "è¶…è©³ç´°"], index=2)
    _detail_map = {"ç°¡æ½”": "concise", "æ¨™æº–": "standard", "è©³ç´°": "detailed", "è¶…è©³ç´°": "very_detailed"}
    detail = _detail_map[_detail_label]     # æ—¢å­˜ build_prompt ç”¨
    detail_label = _detail_label            # ãƒ­ã‚°ç”¨ï¼ˆæ—¥æœ¬èªãƒ©ãƒ™ãƒ«ï¼‰

    cite = st.checkbox("å‡ºå…¸ã‚’ [S1] ã§ä¿ƒã™", True)
    max_tokens = st.slider("æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³", 1000, 40000, 12000, 500)
    answer_backend = st.radio("å›ç­”ç”Ÿæˆ", ["OpenAI", "Retrieve-only"], index=0)
    sys_inst = st.text_area("System Instruction", "ã‚ãªãŸã¯å„ªç§€ãªç¤¾å†…ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™.", height=80)
    display_mode = st.radio("è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰", ["é€æ¬¡è¡¨ç¤ºï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ ï¼‰", "ä¸€æ‹¬è¡¨ç¤º"], index=0)

    st.divider(); st.subheader("æ¤œç´¢å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ï¼ˆOpenAIï¼‰")
    shard_dirs_all = list_shard_dirs_openai(VS_ROOT)
    shard_ids_all = [p.name for p in shard_dirs_all]
    target_shards = st.multiselect("ï¼ˆæœªé¸æŠ=ã™ã¹ã¦ï¼‰", shard_ids_all, default=shard_ids_all)

    # âœ… year/pno å…¥åŠ›ï¼ˆãƒ‘ãƒ¼ã‚¹ã¯ bot_utils ã«å§”è­²ï¼‰
    st.divider(); st.subheader("year / pno ãƒ•ã‚£ãƒ«ã‚¿")
    years_input = st.text_input("yearï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šãƒ»ä»»æ„ï¼‰", value="", help="ä¾‹: 2019,2023")
    pnos_input  = st.text_input("pnoï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç•ªå·, ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šãƒ»ä»»æ„ï¼‰", value="", help="ä¾‹: 010,045,120")

    years_sel: Set[int] = parse_years(years_input)
    pnos_raw: Set[str]  = parse_pnos(pnos_input)

    # pno ã®è¡¨è¨˜ã‚†ã‚Œï¼ˆå…ƒ/ã‚¼ãƒ­é™¤å»/3æ¡ã‚¼ãƒ­åŸ‹ã‚ï¼‰ã‚’å¸å
    pnos_sel_norm: Set[str] = set()
    for p in pnos_raw:
        pnos_sel_norm |= norm_pno_forms(p)

    st.caption(
        f"year ãƒ•ã‚£ãƒ«ã‚¿: {sorted(list(years_sel)) or 'ï¼ˆæœªæŒ‡å®šï¼‰'} / "
        f"pno ãƒ•ã‚£ãƒ«ã‚¿: {sorted(list(pnos_sel_norm)) or 'ï¼ˆæœªæŒ‡å®šï¼‰'}"
    )
    st.caption("â€» æœªå…¥åŠ›ãªã‚‰å…¨ä»¶å¯¾è±¡ã€‚ã©ã¡ã‚‰ã‹/ä¸¡æ–¹ã‚’å…¥åŠ›ã—ãŸå ´åˆã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ã—ã¾ã™ã€‚")

    st.caption("å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ï¼ˆä¾‹: 2025/foo.pdf, 2024/bar.pdfï¼‰")
    file_whitelist_str = st.text_input("å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä»»æ„ï¼‰", value="")
    ui_file_whitelist = {s.strip() for s in file_whitelist_str.split(",") if s.strip()}

    api_key = get_openai_api_key()
    if not api_key and answer_backend == "OpenAI":
        st.error("OpenAI APIã‚­ãƒ¼ãŒ secrets.toml / ç’°å¢ƒå¤‰æ•°ã«ã‚ã‚Šã¾ã›ã‚“ã€‚")

    st.divider(); st.markdown("### ğŸ“‚ è§£æ±ºãƒ‘ã‚¹ï¼ˆå‚ç…§ç”¨ï¼‰")
    st.text_input("VS_ROOT", str(PATHS.vs_root), disabled=True)
    st.text_input("PDF_ROOT", str(PATHS.pdf_root), disabled=True)
    st.text_input("BACKUP_ROOT", str(PATHS.backup_root), disabled=True)
    if hasattr(PATHS, "data_root"):
        st.text_input("DATA_ROOT", str(PATHS.data_root), disabled=True)

    st.divider(); st.subheader("ğŸ§ª ãƒ‡ãƒ¢ç”¨ã‚µãƒ³ãƒ—ãƒ«è³ªå•")
    cat = st.selectbox("ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ", ["ï¼ˆæœªé¸æŠï¼‰"] + list(SAMPLES.keys()))
    sample_options = ["ï¼ˆæœªé¸æŠï¼‰"] if cat == "ï¼ˆæœªé¸æŠï¼‰" else ["ï¼ˆæœªé¸æŠï¼‰"] + SAMPLES.get(cat, [])
    sample = st.selectbox("ã‚µãƒ³ãƒ—ãƒ«è³ªå•ã‚’é¸æŠ", sample_options, index=0)
    cols_demo = st.columns(2)
    with cols_demo[0]:
        st.button("â¬‡ï¸ ã“ã®è³ªå•ã‚’å…¥åŠ›æ¬„ã¸ã‚»ãƒƒãƒˆ", width='stretch',
                  disabled=(sample in ("", "ï¼ˆæœªé¸æŠï¼‰")), on_click=lambda: _set_q(sample))
    with cols_demo[1]:
        st.button("ğŸ² ãƒ©ãƒ³ãƒ€ãƒ æŒ¿å…¥", width='stretch',
                  on_click=lambda: _set_q(str(np.random.choice(ALL_SAMPLES)) if ALL_SAMPLES else ""))
    send_now = st.button("ğŸš€ ã‚µãƒ³ãƒ—ãƒ«ã§å³é€ä¿¡", width='stretch',
                         disabled=(st.session_state.q.strip() == ""))

    # â–¼â–¼ ãƒ—ãƒªã‚»ãƒƒãƒˆç›´é€ï¼ˆğŸš€ï¼‰ã‚’åˆ¤å®šï¼ˆãƒ­ã‚°æŠ‘åˆ¶ã«ä½¿ç”¨ï¼‰ â–¼â–¼
    is_preset_direct_send = bool(send_now)

    #### ãƒ¡ãƒ¢ãƒªçŠ¶æ³ï¼ˆå›ç­”å‰ / å›ç­”å¾Œï¼‰
    st.divider()
    st.subheader("ğŸ§  ãƒ¡ãƒ¢ãƒªçŠ¶æ³ï¼ˆå›ç­”å‰ / å›ç­”å¾Œï¼‰")
    # å›ç­”å‰/å¾Œã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæç”»å…ˆï¼ˆã“ã®é †åºã§ä¸‹ã«ä¸¦ã¶ï¼‰
    mem_pre_box = st.container()
    mem_post_box = st.container()


# ===== æœ¬æ–‡ =========================================================
q = st.text_area("è³ªå•ã‚’å…¥åŠ›", value=st.session_state.q, height=100,
                 placeholder="ã“ã®ç¤¾å†…ãƒœãƒƒãƒˆã«è³ªå•ã—ã¦ãã ã•ã„â€¦")
if q != st.session_state.q:
    st.session_state.q = q

go = st.button("é€ä¿¡", type="primary")
go = go or bool(locals().get("send_now"))

# ===== å®Ÿè¡Œï¼šæ¤œç´¢ â†’ ç”Ÿæˆ â†’ ã‚³ã‚¹ãƒˆ â†’ å‚ç…§ ===========================
if go and st.session_state.q.strip():
    timings = Timings()
    timings.mark("pipeline_start")

    # â–¼â–¼ å›ç­”å‰ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ â–¼â–¼
    with mem_pre_box:
        st.caption("ï¼ˆå›ç­”å‰ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼‰")
        render_memory_kpi_row()

    # â–¼â–¼ ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ­ã‚°ï¼ˆğŸš€ç›´é€ã¯é™¤å¤–ï¼‰ â–¼â–¼
    try:
        if not is_preset_direct_send:
            _prompt_text = st.session_state.q.strip()
            logger.append({
                "user": current_user or "(anonymous)",
                "action": "ask",
                "chat_model": chat_model,
                "detail_label": detail_label,   # ä¾‹ï¼šã€Œè©³ç´°ã€ã€Œè¶…è©³ç´°ã€
                "detail": detail,               # ä¾‹ï¼š"detailed"
                "cite": bool(cite),
                "max_tokens": int(max_tokens),
                "top_k": int(top_k),
                "preset": False,                # ãƒ—ãƒªã‚»ãƒƒãƒˆç›´é€ã§ã¯ãªã„
                "prompt_hash": sha256_short(_prompt_text),
                **({"prompt": _prompt_text} if INCLUDE_FULL_PROMPT_IN_LOG else {}),
            })
        # else: ãƒ—ãƒªã‚»ãƒƒãƒˆç›´é€ã¯ä¿å­˜ã—ãªã„
    except Exception as _log_e:
        st.warning(f"ãƒ­ã‚°ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {_log_e}")

    try:
        vs_backend_dir = PATHS.vs_root / "openai"
        if not vs_backend_dir.exists():
            st.warning(f"ãƒ™ã‚¯ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆ{vs_backend_dir}ï¼‰ã€‚å…ˆã«ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        selected_ids = target_shards or [p.name for p in list_shard_dirs_openai(PATHS.vs_root)]
        shard_dirs = [vs_backend_dir / s for s in selected_ids]
        shard_dirs = [p for p in shard_dirs if p.is_dir() and (p / "vectors.npy").exists()]
        if not shard_dirs:
            st.warning("æ¤œç´¢å¯èƒ½ãªã‚·ãƒ£ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            st.stop()

        # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ [[files: ...]] ã¨ UI ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆã‚’åˆç®—ï¼ˆæ­£è¦åŒ–ã¯ file_ok å†…ã§å†åˆ©ç”¨ï¼‰
        inline_files = parse_inline_files(st.session_state.q)
        effective_whitelist_norm: Set[str] = {norm_path(x) for x in (ui_file_whitelist | inline_files)}

        # ---- meta.jsonl èµ°æŸ»ï¼ˆå€™è£œæŠ½å‡ºï¼‰ ----
        timings.mark("scan_start")
        cand_by_shard, cand_total = scan_candidate_files(shard_dirs, years_sel, pnos_sel_norm)
        timings.mark("scan_end")

        if (years_sel or pnos_sel_norm) and cand_total == 0:
            st.warning("æŒ‡å®šã® year/pno ã«ä¸€è‡´ã™ã‚‹**å€™è£œãƒ•ã‚¡ã‚¤ãƒ«ãŒ 0 ä»¶**ã®ãŸã‚ã€æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã›ã‚“ã€‚æ¡ä»¶ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        # year/pno ç”±æ¥ã®å€™è£œã‚’ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆã«åˆæˆ
        for _sd, files in cand_by_shard.items():
            effective_whitelist_norm |= files

        # ãƒ‡ãƒãƒƒã‚°è¡¨è¨˜
        st.caption(filters_caption(years_sel, pnos_sel_norm, cand_total if (years_sel or pnos_sel_norm) else None))

        # ---- ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ ----
        question = normalize_ja_text(strip_inline_files(st.session_state.q))
        estore = EmbeddingStore(backend="openai")
        question_tokens = count_tokens(st.session_state.q, "text-embedding-3-large")

        timings.mark("embed_start")
        qv = estore.embed([question]).astype("float32")
        timings.mark("embed_end")

        # ---- å„ã‚·ãƒ£ãƒ¼ãƒ‰ TopK ã‚’ãƒãƒ¼ã‚¸ ----
        timings.mark("search_start")
        K = int(top_k)
        heap_: List[Tuple[float, int, int, Dict[str, Any]]] = []  # (score, tiebreak, row_idx, meta)
        tie = count()
        for shp in shard_dirs:
            try:
                vdb = NumpyVectorDB(shp)
                local_k = K
                if years_sel or pnos_sel_norm or effective_whitelist_norm:
                    local_k = max(K * 10, 50)
                hits = vdb.search(qv, top_k=local_k, return_="similarity")
                for h in hits:
                    if isinstance(h, tuple) and len(h) == 3:
                        row_idx, score, meta = h
                    else:
                        score, meta = h
                        row_idx = -1

                    md = dict(meta or {}); md["shard_id"] = shp.name

                    # çµ±ä¸€åˆ¤å®šï¼ˆyear/pno/fileï¼‰
                    if not year_ok(md, years_sel):                 # å¹´
                        continue
                    if not pno_ok(md, pnos_sel_norm):              # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç•ªå·
                        continue
                    if not file_ok(md, effective_whitelist_norm):  # ãƒ•ã‚¡ã‚¤ãƒ«é™å®š
                        continue

                    sc = float(score)
                    if len(heap_) < K:
                        heapq.heappush(heap_, (sc, next(tie), row_idx, md))
                    elif sc > heap_[0][0]:
                        heapq.heapreplace(heap_, (sc, next(tie), row_idx, md))
            except Exception as e:
                st.warning(f"ã‚·ãƒ£ãƒ¼ãƒ‰ {shp.name} ã®æ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼: {e}")
        timings.mark("search_end")

        raw_hits = [(rid, sc, md) for (sc, _tb, rid, md) in sorted(heap_, key=lambda x: x[0], reverse=True)]
        if not raw_hits:
            if years_sel or pnos_sel_norm:
                st.warning("æ¡ä»¶ã«ã¯åˆã†å€™è£œãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã—ãŸãŒã€ä¸Šä½ã‚¹ã‚³ã‚¢ã«å…¥ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n"
                           "ğŸ‘‰ Top-K ã‚’å¢—ã‚„ã™ï¼ã‚¯ã‚¨ãƒªã‚’å…·ä½“åŒ–ã™ã‚‹ï¼ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’çµã‚‹ã€ã®ã„ãšã‚Œã‹ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚")
            else:
                st.warning("è©²å½“ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            st.stop()

        # ---- å›ç­”ç”Ÿæˆ ----
        chat_prompt_tokens = chat_completion_tokens = 0
        use_backend = "OpenAI" if (answer_backend == "OpenAI" and api_key) else "Retrieve-only"

        if use_backend == "OpenAI":
            labeled = [
                f"[S{i}] {m.get('text', '')}\n[meta: {fmt_source(m)} / score={float(s):.3f}]"
                for i, (_rid, s, m) in enumerate(raw_hits, 1)
            ]
            prompt = build_prompt(
                question, labeled, sys_inst=sys_inst, style_hint=detail, cite=cite, strict=False
            )
            responder = GPTResponder(api_key=api_key)
            use_stream = (display_mode == "é€æ¬¡è¡¨ç¤ºï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ ï¼‰")

            # ğŸ”¹ é€æ¬¡å‡ºåŠ›ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ ï¼‰
            if use_stream:
                st.subheader("ğŸ§  å›ç­”ï¼ˆé€æ¬¡è¡¨ç¤ºï¼‰")
                timings.mark("gpt_req_start")
                with st.chat_message("assistant"):
                    st.write_stream(
                        stream_with_timing(
                            responder.stream(
                                model=chat_model,
                                system_instruction=sys_inst,
                                user_content=prompt,
                                max_output_tokens=int(max_tokens),
                                on_error_text="Responses stream error."
                            ),
                            timings,
                            first_key="gpt_first_token",
                            done_key="gpt_done",
                        )
                    )
                answer = responder.final_text or ""
                chat_prompt_tokens = responder.usage.input_tokens
                chat_completion_tokens = responder.usage.output_tokens
                # å¿µã®ãŸã‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                timings.marks.setdefault("gpt_first_token", timings.marks.get("gpt_req_start"))
                timings.marks.setdefault("gpt_done", dt.datetime.now())
                timings.perf.setdefault("gpt_first_token", timings.perf.get("gpt_req_start", time.perf_counter()))
                timings.perf.setdefault("gpt_done", time.perf_counter())

            # ğŸ”¹ ä¸€æ‹¬è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰
            else:
                st.subheader("ğŸ§  å›ç­”ï¼ˆä¸€æ‹¬è¡¨ç¤ºï¼‰")
                timings.mark("gpt_req_start")
                with st.spinner("å›ç­”ç”Ÿæˆä¸­â€¦"):
                    result: CompletionResult = responder.complete(
                        model=chat_model,
                        system_instruction=sys_inst,
                        user_content=prompt,
                        max_output_tokens=int(max_tokens)
                    )
                # ä¸€æ‹¬ã¯ã€Œæœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼å®Œäº†ã€ã¨ã¿ãªã™
                timings.mark("gpt_first_token")
                timings.mark("gpt_done")

                answer = result.text or ""
                chat_prompt_tokens = result.usage.input_tokens
                chat_completion_tokens = result.usage.output_tokens

                st.write(enrich_citations(answer, raw_hits))

            # ---- å‡ºå…¸å‡¦ç†ï¼ˆä¸¡ãƒ¢ãƒ¼ãƒ‰å…±é€šï¼‰----
            answer = enrich_citations(answer, raw_hits)
            import re as _re
            citations = _re.findall(r"\[S[^\]]+\]", answer)
            if citations:
                seen = []
                for c in citations:
                    if c not in seen:
                        seen.append(c)
                with st.expander("ğŸ“ å‡ºå…¸æ‹¡å¼µæ¸ˆã¿æœ€çµ‚ãƒ†ã‚­ã‚¹ãƒˆ", expanded=False):
                    st.caption("ä»¥ä¸‹ã¯å›ç­”å†…ã®å‡ºå…¸ã‚¿ã‚°ã‚’æ•´ç†ã—ãŸä¸€è¦§ã§ã™ã€‚")
                    st.markdown("### ğŸ“š å‡ºå…¸ï¼ˆå‡ºå…¸ã”ã¨ã«æ”¹è¡Œï¼‰")
                    st.text("\n".join(seen))
            else:
                st.caption("å‡ºå…¸ã‚¿ã‚°ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")

        else:
            st.subheader("ğŸ§© å–å¾—ã®ã¿ï¼ˆè¦ç´„ãªã—ï¼‰")
            st.info("Retrieve-only ãƒ¢ãƒ¼ãƒ‰ã§ã™ã€‚ä¸‹ã®å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ã”è¦§ãã ã•ã„ã€‚")
            answer = ""  # å¿µã®ãŸã‚

        
        # ===== ã“ã“ã‹ã‚‰ Word ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ =====
        try:
            # ç”Ÿæˆæ™‚åˆ»ï¼ˆJSTï¼‰
            JST = dt.timezone(dt.timedelta(hours=9), name="Asia/Tokyo")
            ts_jst = dt.datetime.now(JST).strftime("%Y-%m-%d %H:%M:%S %Z")

            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”¨ãƒ¡ã‚¿
            meta_doc = {
                "user": current_user or "(anonymous)",
                "chat_model": chat_model,
                "detail_label": detail_label,
                "detail": detail,
                "max_tokens": int(max_tokens),
                "top_k": int(top_k),
                "ts_jst": ts_jst,
            }

            # âœ… ãƒ•ã‚£ãƒ«ã‚¿æƒ…å ±ã‚’å®šç¾©
            # filters_doc = {
            #     "years": sorted(list(years_sel)) if years_sel else [],
            #     "pnos": sorted(list(pnos_sel_norm)) if pnos_sel_norm else [],
            #     "file_whitelist": sorted(list(effective_whitelist_norm)) if effective_whitelist_norm else [],
            #     "shards": target_shards or [],
            # }

            # æ˜ç¤ºæŒ‡å®šã®ã¿ï¼ˆUI + ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ï¼‰ã‚’è¨˜éŒ²ã—ãŸã„å ´åˆ
            explicit_files = sorted(list({norm_path(x) for x in (ui_file_whitelist | inline_files)}))
            filters_doc = {
                "years": sorted(list(years_sel)) if years_sel else [],
                "pnos": sorted(list(pnos_sel_norm)) if pnos_sel_norm else [],
                "file_whitelist": explicit_files,  # â† ã“ã¡ã‚‰ã«ç½®ãæ›ãˆ
                "shards": target_shards or [],
}

            prompt_text_for_doc = st.session_state.q.strip()
            answer_text_for_doc = answer or ""

            if Document is None:
                st.info("ğŸ“„ Word ä¿å­˜ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã«ã¯ `pip install python-docx` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            else:
                # âœ… filters_doc ã‚’æ¸¡ã™ï¼
                docx_bytes = _build_docx(prompt_text_for_doc, answer_text_for_doc, meta_doc, filters_doc)
                default_name = f"bot_answer_{dt.datetime.now(JST):%Y%m%d_%H%M%S}.docx"
                st.download_button(
                    "â¬‡ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‹å›ç­”ã‚’ Word ã§ä¿å­˜ (.docx)",
                    data=docx_bytes,
                    file_name=default_name,
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                    use_container_width=True,
                )
        except Exception as _docx_e:
            st.warning(f"Word ä¿å­˜ã§ã‚¨ãƒ©ãƒ¼: {_docx_e}")
        # ===== ã“ã“ã¾ã§ Word ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ =====




        # ---- å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ ----
        with st.expander("ğŸ” å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¸Šä½ãƒ’ãƒƒãƒˆï¼‰", expanded=False):
            for i, (_rid, score, meta) in enumerate(raw_hits, 1):
                txt = str(meta.get("text", "") or "")
                label = fmt_source(meta)
                year = meta.get("year", "")
                pno = meta.get("pno", "") or meta.get("project_no", "")
                snippet = (txt[:1000] + "â€¦") if len(txt) > 1000 else txt
                st.markdown(
                    f"**[{citation_tag(i, meta)}] score={float(score):.3f}**  "
                    f"`{label}` â€” **year:** {year} / **pno:** {pno}\n\n{snippet}"
                )

        # ---- ä½¿ç”¨é‡ã®æ¦‚ç®— ----
        render_usage_summary(
            embedding_model="text-embedding-3-large",
            embedding_tokens=question_tokens,
            chat_model=chat_model,
            chat_prompt_tokens=chat_prompt_tokens,
            chat_completion_tokens=chat_completion_tokens,
            use_backend_openai=(use_backend == "OpenAI"),
            title="ğŸ“Š ä½¿ç”¨é‡ã®æ¦‚ç®—",
        )
        # â± ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ‚äº†
        timings.mark("pipeline_end")
        render_metrics_ui(timings)

        # â± å›ç­”å¾Œã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
        with mem_post_box:
            st.caption("ï¼ˆå›ç­”å¾Œã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼‰")
            render_memory_kpi_row()

    except Exception as e:
        st.error(f"æ¤œç´¢/ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        # ä¾‹å¤–æ™‚ã‚‚å¯èƒ½ãªç¯„å›²ã§ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’è¡¨ç¤º
        timings.marks.setdefault("pipeline_end", dt.datetime.now())
        timings.perf.setdefault("pipeline_end", time.perf_counter())
        render_metrics_ui(timings)
else:
    st.info("è³ªå•ã‚’å…¥åŠ›ã—ã¦ã€é€ä¿¡ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®šã§ãã¾ã™ã€‚")





# æœ«å°¾ï¼šãƒ¡ãƒ¢ãƒªç›£è¦–ï¼ˆä»»æ„ã§äºŒé‡è¡¨ç¤ºã‚’é¿ã‘ã‚‹ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆä¾‹ï¼‰
# st.divider()
# st.markdown("### ğŸ§  ãƒ¡ãƒ¢ãƒªçŠ¶æ³ï¼ˆå‚è€ƒï¼‰")
# render_memory_kpi_row()
