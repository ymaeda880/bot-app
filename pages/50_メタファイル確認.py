# pages/50_ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª.py
# ------------------------------------------------------------
# ğŸ—‚ï¸ vectorstore/(openai|local)/<shard_id>/meta.jsonl ãƒ“ãƒ¥ãƒ¼ã‚¢
# - è¤‡æ•°ã‚·ãƒ£ãƒ¼ãƒ‰ã® meta.jsonl ã‚’æ¨ªæ–­ã—ã¦èª­ã¿è¾¼ã¿ãƒ»çµã‚Šè¾¼ã¿ãƒ»CSV/Excel å‡ºåŠ›
# - ğŸš« ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ©Ÿèƒ½ï¼ˆå‰Šé™¤ãƒ»ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç­‰ï¼‰ã¯å«ã‚ãªã„
# - â˜… ã‚¿ã‚¤ãƒˆãƒ«ç›´ä¸‹ã«ã€Œå¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ã® year / pno ä¸€è¦§ã€ï¼‹ Excel ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’è¡¨ç¤º
# ------------------------------------------------------------

from __future__ import annotations
from pathlib import Path
import io
import json
import pandas as pd
import streamlit as st

# å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆèª­ã¿è¾¼ã¿ç”¨ã®ã¿ä½¿ç”¨ï¼‰
from lib.vectorstore_utils import iter_jsonl

# ãƒ‘ã‚¹è¨­å®šã¯ PATHS ã«ä¸€æœ¬åŒ–
from config.path_config import PATHS
VS_ROOT: Path = PATHS.vs_root  # => <project>/data/vectorstore

# ============================================================
# ã‚¯ãƒªãƒƒãƒ—ãƒœãƒ¼ãƒ‰ã‚³ãƒ”ãƒ¼ï¼ˆJSåŸ‹ã‚è¾¼ã¿ï¼‰
# ============================================================
def copy_button(text: str, label: str, key: str):
    payload = json.dumps(text, ensure_ascii=False)
    html = f"""
    <button id="{key}" style="
        padding:6px 10px;border-radius:8px;border:1px solid #dadce0;
        background:#fff;cursor:pointer;font-size:0.9rem;">ğŸ“‹ {label}</button>
    <script>
      const btn = document.getElementById("{key}");
      if (btn) {{
        btn.addEventListener("click", async () => {{
          try {{
            await navigator.clipboard.writeText({payload});
            const old = btn.innerText;
            btn.innerText = "âœ… ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ";
            setTimeout(()=>{{ btn.innerText = old; }}, 1200);
          }} catch(e) {{
            console.error(e);
            alert("ã‚³ãƒ”ãƒ¼ã«å¤±æ•—ã—ã¾ã—ãŸ: " + e);
          }}
        }});
      }}
    </script>
    """
    st.components.v1.html(html, height=38)

# ============================================================
# UI
# ============================================================
st.set_page_config(page_title="50 ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª", page_icon="ğŸ—‚ï¸", layout="wide")
st.title("ğŸ—‚ï¸ ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªï¼ˆãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ï¼ã‚·ãƒ£ãƒ¼ãƒ‰ï¼‰")

# â˜… ã‚¿ã‚¤ãƒˆãƒ«ç›´ä¸‹ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ï¼ˆyear/pno ã‚µãƒãƒªãƒ¼ã‚’å¾Œã§ã“ã“ã¸æç”»ï¼‰
summary_box = st.container()

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
with st.sidebar:
    st.header("èª­ã¿è¾¼ã¿è¨­å®š")
    backend = st.radio("ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰", ["openai", "local"], index=0, horizontal=True)

    base_backend_dir = VS_ROOT / backend
    if not base_backend_dir.exists():
        st.error(f"{base_backend_dir} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    shard_dirs = sorted([p for p in base_backend_dir.iterdir() if p.is_dir()])
    shard_ids = [p.name for p in shard_dirs]
    sel_shards = st.multiselect("å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°å¯ï¼‰", shard_ids, default=shard_ids)
    max_rows_read = st.number_input("å„ã‚·ãƒ£ãƒ¼ãƒ‰ã®æœ€å¤§è¡Œæ•°ï¼ˆ0=å…¨ä»¶ï¼‰", min_value=0, value=0, step=1000)

if not sel_shards:
    st.warning("å°‘ãªãã¨ã‚‚1ã¤ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---
all_rows = []
for sid in sel_shards:
    meta_path = base_backend_dir / sid / "meta.jsonl"
    read_cnt = 0
    for obj in iter_jsonl(meta_path):
        obj = dict(obj)
        obj.setdefault("shard_id", sid)
        obj.setdefault("file", obj.get("doc_id"))
        all_rows.append(obj)
        read_cnt += 1
        if max_rows_read and read_cnt >= max_rows_read:
            break

if not all_rows:
    st.warning("è¡¨ç¤ºã§ãã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    st.stop()

df = pd.DataFrame(all_rows)

# è¶³ã‚Šãªã„åˆ—ã‚’è£œå®Œï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯ Noneï¼‰
for col in ["file","page","chunk_id","chunk_index","text","span_start","span_end","shard_id","year","pno"]:
    if col not in df.columns:
        df[col] = None

df["chunk_len"] = df["text"].astype(str).str.len()

# ============================================================
# â˜… ã‚¿ã‚¤ãƒˆãƒ«ç›´ä¸‹ã«ã€Œyear / pno ã®ä¸€è¦§ã¨ Excel ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€ã‚’è¡¨ç¤º
# ============================================================
with summary_box:
    st.subheader("ğŸ§­ ã‚µãƒãƒªãƒ¼ï¼ˆyear / pno ä¸€è¦§ï¼‰")

    # year ä¸€è¦§ï¼ˆå€¤ã®æ­£è¦åŒ–ï¼šæ•°å€¤åŒ–ã§ãã‚‹ã‚‚ã®ã¯ int åŒ–ï¼‰
    years_series = pd.to_numeric(df["year"], errors="coerce").dropna().astype(int)
    years_df = (
        years_series.value_counts()
        .rename("rows")
        .to_frame()
        .sort_index()
        .rename_axis("year")
        .reset_index()
    )

    # pno ä¸€è¦§ï¼ˆæ–‡å­—åˆ—åŒ–ã—ã¦é›†è¨ˆï¼‰
    pnos_series = df["pno"].dropna().astype(str)
    pnos_df = (
        pnos_series.value_counts()
        .rename("rows")
        .to_frame()
        .sort_index()
        .rename_axis("pno")
        .reset_index()
    )

    # year Ã— pno çµ„ã¿åˆã‚ã›
    yp_df = (
        df.assign(year=pd.to_numeric(df["year"], errors="coerce"))
          .dropna(subset=["year","pno"])
    )
    if not yp_df.empty:
        yp_df["year"] = yp_df["year"].astype(int)
        year_pno_df = (
            yp_df.groupby(["year","pno"])
                .size()
                .reset_index(name="rows")
                .sort_values(["year","pno"], kind="mergesort")
        )
    else:
        year_pno_df = pd.DataFrame(columns=["year","pno","rows"])

    # è¡¨ç¤ºï¼ˆå°ã•ã‚ã®å…ˆé ­ï¼‰
    c1, c2 = st.columns(2)
    with c1:
        st.caption("å­˜åœ¨ã™ã‚‹ yearï¼ˆå‡ºç¾ä»¶æ•°ï¼‰")
        st.dataframe(years_df.head(200), use_container_width=True, height=260)
    with c2:
        st.caption("å­˜åœ¨ã™ã‚‹ pnoï¼ˆå‡ºç¾ä»¶æ•°ï¼‰")
        st.dataframe(pnos_df.head(200), use_container_width=True, height=260)

    with st.expander("year Ã— pno ã®çµ„ã¿åˆã‚ã›ï¼ˆå‡ºç¾ä»¶æ•°ï¼‰", expanded=False):
        st.dataframe(year_pno_df.head(500), use_container_width=True, height=360)

    # Excelï¼ˆ.xlsxï¼‰ä½œæˆï¼ˆè¤‡æ•°ã‚·ãƒ¼ãƒˆï¼‰
    xlsx_bytes = io.BytesIO()
    with pd.ExcelWriter(xlsx_bytes, engine="xlsxwriter") as writer:
        # é¸æŠã‚·ãƒ£ãƒ¼ãƒ‰ä¸€è¦§
        pd.DataFrame({"shard_id": sel_shards}).to_excel(writer, index=False, sheet_name="shards")
        # å„ã‚µãƒãƒªãƒ¼
        years_df.to_excel(writer, index=False, sheet_name="years")
        pnos_df.to_excel(writer, index=False, sheet_name="pno")
        year_pno_df.to_excel(writer, index=False, sheet_name="year_pno")
        # å…ƒãƒ‡ãƒ¼ã‚¿ã®æœ€å°é™ï¼ˆå¿…è¦ãªã‚‰å…¨åˆ—ã«å¤‰æ›´ï¼‰
        cols_export = ["shard_id","file","page","year","pno","chunk_id","chunk_index","span_start","span_end","chunk_len"]
        export_df = df[[c for c in cols_export if c in df.columns]].copy()
        export_df.to_excel(writer, index=False, sheet_name="sample_rows")

    st.download_button(
        label="ğŸ“¥ ã‚µãƒãƒªãƒ¼ã‚’Excelï¼ˆ.xlsxï¼‰ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=xlsx_bytes.getvalue(),
        file_name="meta_summary_year_pno.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        use_container_width=True,
    )

st.divider()

# å…ˆé ­è¡¨ç¤ºï¼ˆæ—¢å­˜ï¼‰
st.dataframe(df.head(500), width="stretch", height=560)
st.divider()

# ============================================================
# ğŸ“‹ ãƒ•ã‚¡ã‚¤ãƒ«åã‚³ãƒ”ãƒ¼
# ============================================================
st.subheader("ğŸ“‹ ãƒ•ã‚¡ã‚¤ãƒ«åã‚³ãƒ”ãƒ¼ï¼ˆyear/file.pdfï¼‰")
file_list = sorted(df["file"].dropna().unique().tolist())
q = st.text_input("ãƒ•ã‚¡ã‚¤ãƒ«åãƒ•ã‚£ãƒ«ã‚¿", value="")
filtered = [f for f in file_list if q.lower() in str(f).lower()] if q else file_list
st.caption(f"ãƒ’ãƒƒãƒˆ: {len(filtered)} ä»¶")
cols = st.columns(3)
for i, f in enumerate(filtered[:100]):
    with cols[i % 3]:
        st.write(f"`{f}`")
        copy_button(text=f, label="ã‚³ãƒ”ãƒ¼", key=f"copy_{i}")
