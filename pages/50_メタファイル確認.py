# pages/50_ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª.py
# ------------------------------------------------------------
# ğŸ—‚ï¸ vectorstore/(openai|local)/<shard_id>/meta.jsonl ãƒ“ãƒ¥ãƒ¼ã‚¢
# - ã‚µãƒãƒªãƒ¼UIãªã—
# - ã‚·ãƒ£ãƒ¼ãƒ‰ï¼ˆ= year ãƒ•ã‚©ãƒ«ãƒ€ï¼‰ã‚’é¸ã‚“ã ã¨ãã ã‘é…å»¶èª­ã¿è¾¼ã¿
# - å…ˆã« yearÃ—pno é›†è¨ˆè¡¨ã‚’è¡¨ç¤º â†’ æ¬¡ã« pno å˜ä¸€é¸æŠã§çµã‚Šè¾¼ã¿
# - çµã‚Šè¾¼ã¿çµæœã‚’ Excel ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½
# ------------------------------------------------------------

from __future__ import annotations
from pathlib import Path
import io
import pandas as pd
import streamlit as st

# å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆèª­ã¿è¾¼ã¿ç”¨ã®ã¿ä½¿ç”¨ï¼‰
from lib.rag.vectorstore_utils import iter_jsonl

# ãƒ‘ã‚¹è¨­å®šã¯ PATHS ã«ä¸€æœ¬åŒ–
from config.path_config import PATHS
VS_ROOT: Path = PATHS.vs_root  # => <project>/data/vectorstore


# ============================================================
# UIï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã¯ backend ã®ã¿ï¼‰
# ============================================================
st.set_page_config(page_title="50 ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª", page_icon="ğŸ—‚ï¸", layout="wide")
st.title("ğŸ—‚ï¸ ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªï¼ˆã‚·ãƒ£ãƒ¼ãƒ‰é¸æŠå¾Œã«èª­ã¿è¾¼ã¿ï¼‰")

with st.sidebar:
    st.header("è¨­å®š")
    backend = st.radio("ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰", ["openai", "local"], index=0, horizontal=True)

# ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèª
base_backend_dir = VS_ROOT / backend
if not base_backend_dir.exists():
    st.error(f"{base_backend_dir} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# åˆ©ç”¨å¯èƒ½ãªã‚·ãƒ£ãƒ¼ãƒ‰ï¼ˆ= year ãƒ•ã‚©ãƒ«ãƒ€ï¼‰ã‚’åˆ—æŒ™
shard_dirs = sorted([p for p in base_backend_dir.iterdir() if p.is_dir()])
shard_ids = [p.name for p in shard_dirs]

st.subheader("ğŸ” ã‚·ãƒ£ãƒ¼ãƒ‰é¸æŠ â†’ èª­ã¿è¾¼ã¿")
sel_shard = st.selectbox(
    "ã‚·ãƒ£ãƒ¼ãƒ‰ï¼ˆyear)ã‚’é¸æŠï¼ˆé¸æŠã™ã‚‹ã¨èª­ã¿è¾¼ã¿é–‹å§‹ï¼‰",
    ["ï¼ˆé¸æŠã—ã¦ãã ã•ã„ï¼‰"] + shard_ids,
    index=0
)

if sel_shard != "ï¼ˆé¸æŠã—ã¦ãã ã•ã„ï¼‰":
    meta_path = base_backend_dir / sel_shard / "meta.jsonl"
    if not meta_path.exists():
        st.warning(f"{meta_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()

    rows = []
    pno_counts: dict[str, int] = {}

    with st.spinner(f"ã‚·ãƒ£ãƒ¼ãƒ‰ {sel_shard} ã® meta.jsonl ã‚’èª­ã¿è¾¼ã¿ä¸­â€¦"):
        for obj in iter_jsonl(meta_path):
            o = dict(obj)
            o.setdefault("shard_id", sel_shard)
            o.setdefault("file", o.get("doc_id"))
            rows.append(o)
            p = o.get("pno")
            if p is not None:
                pno_counts[str(p)] = pno_counts.get(str(p), 0) + 1

    if not rows:
        st.warning(f"ã‚·ãƒ£ãƒ¼ãƒ‰ {sel_shard} ã«è¡¨ç¤ºã§ãã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        st.stop()

    # DataFrame åŒ–ï¼ˆå¿…è¦æœ€å°åˆ—ã‚’è£œå®Œï¼‰
    df = pd.DataFrame(rows)
    for col in ["file", "page", "chunk_id", "chunk_index", "text",
                "span_start", "span_end", "shard_id", "year", "pno"]:
        if col not in df.columns:
            df[col] = None
    df["chunk_len"] = df["text"].astype(str).str.len()

    # ============================================================
    # ğŸ“… year Ã— pno çµ„ã¿åˆã‚ã›ï¼ˆå‡ºç¾ä»¶æ•°ï¼‰â€» pnoé¸æŠã®ã€Œå‰ã€ã«è¡¨ç¤º
    # ============================================================
    yp_df = (
        df.assign(year=pd.to_numeric(df["year"], errors="coerce"))
          .dropna(subset=["year", "pno"])
    )
    if not yp_df.empty:
        yp_df["year"] = yp_df["year"].astype(int)
        year_pno_df = (
            yp_df.groupby(["year", "pno"])
                 .size()
                 .reset_index(name="rows")
                 .sort_values(["year", "pno"], kind="mergesort")
        )
    else:
        year_pno_df = pd.DataFrame(columns=["year", "pno", "rows"])

    st.subheader("ğŸ“… year Ã— pno ã®çµ„ã¿åˆã‚ã›ï¼ˆå‡ºç¾ä»¶æ•°ï¼‰")
    if year_pno_df.empty:
        st.info("ã“ã®ã‚·ãƒ£ãƒ¼ãƒ‰å†…ã« year / pno ã®çµ„ã¿åˆã‚ã›ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        st.dataframe(year_pno_df, use_container_width=True, height=360)


    # ============================================================
    # ğŸ¯ pno å˜ä¸€é¸æŠ â†’ çµã‚Šè¾¼ã¿è¡¨ç¤ºï¼ˆå¹´Ã—pnoé›†è¨ˆã®å¾Œæ®µï¼‰
    # ============================================================
    pno_options = sorted(pno_counts.keys())

    sel_pno = st.selectbox(
        "pno ã‚’é¸æŠï¼ˆã“ã®ã‚·ãƒ£ãƒ¼ãƒ‰å†…ã®ã¿ï¼‰",
        ["ï¼ˆé¸æŠã—ã¦ãã ã•ã„ï¼‰"] + pno_options,  # â† å…ˆé ­ã«ç©ºæ¬„æ‰±ã„ã‚’è¿½åŠ 
        index=0,
        help="é¸ã‚“ã ã‚·ãƒ£ãƒ¼ãƒ‰å†…ã® pno ã ã‘ãŒå€™è£œã«ãªã‚Šã¾ã™ã€‚"
    )

    # ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
    if sel_pno == "ï¼ˆé¸æŠã—ã¦ãã ã•ã„ï¼‰":
        st.info("pno ã‚’é¸æŠã™ã‚‹ã¨è©²å½“ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
        st.stop()

    filtered_df = df[df["pno"].astype(str) == sel_pno]


    # ä»¶æ•°è¡¨ç¤º
    st.caption(
        f"ğŸ“Š è©²å½“ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(filtered_df):,} ä»¶"
        + ("" if sel_pno == "ï¼ˆã™ã¹ã¦ï¼‰" else f"ï¼ˆpno={sel_pno}ï¼‰")
        + f"ï½œã‚·ãƒ£ãƒ¼ãƒ‰: {sel_shard}"
    )

    # çµæœè¡¨ç¤ºï¼ˆå…¨ä»¶ï¼‰
    st.dataframe(filtered_df, use_container_width=True, height=600)

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆçµã‚Šè¾¼ã¿æ¸ˆã¿ã®ã¿ï¼‰
    xlsx_bytes = io.BytesIO()
    with pd.ExcelWriter(xlsx_bytes, engine="xlsxwriter") as writer:
        filtered_df.to_excel(writer, index=False, sheet_name="rows")

    st.download_button(
        label="ğŸ“¥ ã“ã®çµã‚Šè¾¼ã¿çµæœã‚’Excelï¼ˆ.xlsxï¼‰ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=xlsx_bytes.getvalue(),
        file_name=(
            f"meta_rows_shard_{sel_shard}"
            + ("" if sel_pno == "ï¼ˆã™ã¹ã¦ï¼‰" else f"_pno{sel_pno}")
            + ".xlsx"
        ),
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        use_container_width=True,
    )

else:
    st.info("ä¸Šã®ã‚»ãƒ¬ã‚¯ãƒˆãƒœãƒƒã‚¯ã‚¹ã‹ã‚‰ **ã‚·ãƒ£ãƒ¼ãƒ‰ï¼ˆyearï¼‰** ã‚’é¸æŠã™ã‚‹ã¨èª­ã¿è¾¼ã¿ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
