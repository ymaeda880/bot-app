# pages/10_ãƒœãƒƒãƒˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰.py
# ============================================
# gpt-5 / gpt-4.1ï¼ˆResponses API å°‚ç”¨ï¼‰
# - é€æ¬¡è¡¨ç¤ºï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰â‡” ä¸€æ‹¬è¡¨ç¤º åˆ‡æ›¿
# - ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸å¯æ™‚ã¯è‡ªå‹•ã§ä¸€æ‹¬è¡¨ç¤ºã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
# - max_output_tokens / usage.input_tokens, output_tokens å¯¾å¿œ
# - GPTå‘¼ã³å‡ºã—ã¯ lib/gpt_responder ã«é›†ç´„
# ============================================

"""
Internal Bot (RAG, Shards) â€” Streamlit ã‚¢ãƒ—ãƒª

æœ¬ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€ç¤¾å†…å‘ã‘ã®RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆUIã‚’æä¾›ã—ã¾ã™ã€‚
- ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ï¼ˆOpenAIåŸ‹ã‚è¾¼ã¿ï¼‰ã‹ã‚‰æ–‡æ›¸ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’æ¨ªæ–­æ¤œç´¢ï¼ˆTop-Kãƒãƒ¼ã‚¸ï¼‰
- OpenAI Responses API ã§å›ç­”ç”Ÿæˆï¼ˆgpt-5 / gpt-4.1 ç³»æƒ³å®šï¼‰
- é€æ¬¡è¡¨ç¤ºï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰ã¨ä¸€æ‹¬è¡¨ç¤ºã‚’UIã‹ã‚‰åˆ‡ã‚Šæ›¿ãˆå¯èƒ½
- ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãŒçµ„ç¹”æœªæ¤œè¨¼ç­‰ã§ç¦æ­¢ã•ã‚Œã‚‹å ´åˆã¯ã€è‡ªå‹•ã§ä¸€æ‹¬è¡¨ç¤ºã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
- ã‚³ã‚¹ãƒˆæ¦‚ç®—ï¼ˆåŸ‹ã‚è¾¼ã¿ãƒ»ç”Ÿæˆï¼‰ã‚’è¡¨ç¤º
- ä¸Šä½å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆS1..ï¼‰ã‚‚ç¢ºèªå¯èƒ½

NOTE:
- å®Ÿé‹ç”¨ã§ã¯ secrets.toml ã‹ç’°å¢ƒå¤‰æ•°ã§ OPENAI_API_KEY ã‚’æ¸¡ã—ã¦ãã ã•ã„ã€‚
- ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã¯ OpenAI å›ºå®šã€‚Numpy ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“VDBã«å¯¾ã—ã¦æ¤œç´¢ã—ã¾ã™ã€‚
"""

from __future__ import annotations
import os
from pathlib import Path
from typing import List, Tuple, Dict, Any
import random
import heapq
from itertools import count
import unicodedata
import re

import streamlit as st
import numpy as np

# â˜… GPTå‘¼ã³å‡ºã—ã¯ã“ã®ãƒ©ãƒƒãƒ‘ã«ä¸€å…ƒåŒ–
from lib.gpt_responder import GPTResponder, CompletionResult, Usage

from lib.rag_utils import EmbeddingStore, NumpyVectorDB
from config.path_config import PATHS
from lib.text_normalize import normalize_ja_text
from config.sample_questions import SAMPLES, ALL_SAMPLES

from lib.costs import (
    MODEL_PRICES_USD, EMBEDDING_PRICES_USD, DEFAULT_USDJPY,
    ChatUsage, estimate_chat_cost, estimate_embedding_cost, usd_to_jpy,
)

from lib.prompts.bot_prompt import build_prompt

# ========= /1M â†’ /1K å¤‰æ›ï¼ˆè¡¨ç¤ºç”¨ï¼‰ =========
MODEL_PRICES_PER_1K: Dict[str, Dict[str, float]] = {
    # ãƒ¢ãƒ‡ãƒ«å˜ä¾¡ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆUSD/1M tokensï¼‰ã‚’è¡¨ç¤º/è¨ˆç®—ç”¨ã« USD/1K tokens ã«å¤‰æ›
    m: {"in": float(p.get("in", 0.0)) / 1000.0, "out": float(p.get("out", 0.0)) / 1000.0}
    for m, p in MODEL_PRICES_USD.items()
}

# ========= ãƒ‘ã‚¹ =========
VS_ROOT: Path = PATHS.vs_root  # ä¾‹: <project>/data/vectorstoreï¼ˆOpenAIãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é…ä¸‹ã«ã‚·ãƒ£ãƒ¼ãƒ‰æ ¼ç´ï¼‰


# ========= ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
def _count_tokens(text: str, model_hint: str = "cl100k_base") -> int:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã®ãŠãŠã‚ˆãã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¦‹ç©ã‚‚ã‚‹ã€‚

    - tiktoken ãŒä½¿ãˆã‚‹å ´åˆã¯ãƒ¢ãƒ‡ãƒ«åãƒ’ãƒ³ãƒˆã‹ã‚‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å–å¾—ã—ã¦ç²¾åº¦é«˜ãã‚«ã‚¦ãƒ³ãƒˆ
    - ä½¿ãˆãªã„å ´åˆã¯ 1ãƒˆãƒ¼ã‚¯ãƒ³ â‰’ 4æ–‡å­— ã®è¿‘ä¼¼ã§æ¦‚ç®—

    Args:
        text (str): å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
        model_hint (str): tiktoken ã®ãƒ¢ãƒ‡ãƒ«åãƒ’ãƒ³ãƒˆï¼ˆæ—¢å®š: cl100k_baseï¼‰

    Returns:
        int: æ¦‚ç®—ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆæœ€ä½1ï¼‰
    """
    try:
        import tiktoken
        try:
            enc = tiktoken.encoding_for_model(model_hint)
        except Exception:
            enc = tiktoken.get_encoding("cl100k_base")
        return len(enc.encode(text))
    except Exception:
        return max(1, int(len(text) / 4))


def _fmt_source(meta: Dict[str, Any]) -> str:
    """
    æ¤œç´¢ãƒ’ãƒƒãƒˆã®ãƒ¡ã‚¿æƒ…å ±ã‹ã‚‰ã€äººé–“å¯èª­ãªã‚½ãƒ¼ã‚¹è¡¨è¨˜ã‚’ä½œã‚‹ã€‚

    ä¾‹: "foo.pdf p.12 (chunk_id)" ã¾ãŸã¯ "foo.pdf" / "(unknown)"

    Args:
        meta (Dict[str, Any]): ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¾æ›¸ï¼ˆfile/page/chunk_idãªã©ï¼‰

    Returns:
        str: æ•´å½¢æ¸ˆã¿ã®ãƒ©ãƒ™ãƒ«æ–‡å­—åˆ—
    """
    f = str(meta.get("file", "") or "")
    p = meta.get("page", None)
    cid = str(meta.get("chunk_id", "") or "")
    base = f"{f} p.{int(p)}" if (f and p is not None) else (f or "(unknown)")
    return f"{base} ({cid})" if cid else base


def _list_shard_dirs_openai() -> List[Path]:
    """
    OpenAI ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ç”¨ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ç›´ä¸‹ã«ã‚ã‚‹ã‚·ãƒ£ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åˆ—æŒ™ã€‚

    Returns:
        List[Path]: ã‚·ãƒ£ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒªã‚¹ãƒˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯ç©ºï¼‰
    """
    base = VS_ROOT / "openai"
    if not base.exists():
        return []
    return sorted([p for p in base.iterdir() if p.is_dir()])


def _norm_path(s: str) -> str:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«å/ãƒ‘ã‚¹ã®æ¯”è¼ƒç”¨ã«æ­£è¦åŒ–ï¼ˆNFKC, ãƒˆãƒªãƒ , ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿çµ±ä¸€, å°æ–‡å­—åŒ–ï¼‰ã€‚

    Args:
        s (str): å…¥åŠ›ãƒ‘ã‚¹æ–‡å­—åˆ—

    Returns:
        str: æ­£è¦åŒ–å¾Œã®ãƒ‘ã‚¹
    """
    s = unicodedata.normalize("NFKC", s or "")
    s = s.strip().replace("\\", "/")
    return s.lower()


def _get_openai_api_key() -> str | None:
    """
    OpenAI API ã‚­ãƒ¼ã‚’ secrets.tomlï¼ˆst.secrets.openai.api_keyï¼‰ â†’ ç’°å¢ƒå¤‰æ•° ã®é †ã§å–å¾—ã€‚

    Returns:
        Optional[str]: è¦‹ã¤ã‹ã‚Œã°ã‚­ãƒ¼æ–‡å­—åˆ—ã€ãªã‘ã‚Œã° None
    """
    try:
        ok = st.secrets.get("openai", {}).get("api_key")
        if ok:
            return str(ok)
    except Exception:
        pass
    return os.getenv("OPENAI_API_KEY")


# ---------- ãƒ¢ãƒ‡ãƒ«å€™è£œï¼ˆResponseså°‚ç”¨ï¼‰ ----------
RESPONSES_MODELS = [
    m for m in MODEL_PRICES_PER_1K.keys() if m.startswith("gpt-5") or m.startswith("gpt-4.1")
]


# =========================
# Streamlit UI
# =========================
st.set_page_config(page_title="Chat Bot (Sharded)", page_icon="ğŸ’¬", layout="wide")
st.title("ğŸ’¬ Internal Bot (RAG, Shards)")

# å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿æŒï¼ˆã‚µãƒ³ãƒ—ãƒ«æŒ¿å…¥ã¨é€£å‹•ï¼‰
if "q" not in st.session_state:
    st.session_state.q = ""


def _set_question(text: str):
    """ã‚µãƒ³ãƒ—ãƒ«é¸æŠãƒœã‚¿ãƒ³ã‹ã‚‰å…¥åŠ›æ¬„ã¸ãƒ†ã‚­ã‚¹ãƒˆã‚’æµã—è¾¼ã‚€ãŸã‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚"""
    st.session_state.q = text


with st.sidebar:
    st.header("è¨­å®š")

    # ---- ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆgpt-5 ç³»ã‚’å…ˆé ­ã«æ¥ã‚‹ã‚ˆã†ä¸¦ã³é †å·¥å¤«ï¼‰----
    st.markdown("### å›ç­”ãƒ¢ãƒ‡ãƒ«ï¼ˆResponses APIï¼‰")
    all_models_sorted = sorted(
        RESPONSES_MODELS, key=lambda x: (0 if x.startswith("gpt-5") else 1, x)
    )
    default_idx = all_models_sorted.index("gpt-5-mini") if "gpt-5-mini" in all_models_sorted else 0
    chat_model = st.selectbox("ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ", all_models_sorted, index=default_idx)

    # ---- ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã® Top-K ----
    top_k = st.slider("æ¤œç´¢ä»¶æ•°ï¼ˆTop-Kï¼‰", 1, 12, 6, 1)

    # ---- å›ç­”ã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå´ã«æ¸¡ã™ãƒ’ãƒ³ãƒˆå€¤ï¼‰----
    label_to_value = {"ç°¡æ½”": "concise", "æ¨™æº–": "standard", "è©³ç´°": "detailed", "è¶…è©³ç´°": "very_detailed"}
    detail_label = st.selectbox("è©³ã—ã•", list(label_to_value.keys()), index=2)
    detail = label_to_value[detail_label]

    # ---- å¼•ç”¨è¡¨è¨˜ï¼ˆ[S1] ãªã©ï¼‰ã‚’ä¿ƒã™ã‹ã©ã†ã‹ ----
    cite = st.checkbox("å‡ºå…¸ã‚’è§’æ‹¬å¼§ã§å¼•ç”¨ï¼ˆ[S1] ç­‰ï¼‰", value=True)

    # ---- å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™ï¼ˆResponses API ã® max_output_tokensï¼‰----
    max_tokens = st.slider("æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆç›®å®‰ï¼‰", 1000, 40000, 12000, 500)

    # ---- å›ç­”ç”Ÿæˆãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ï¼ˆOpenAI / å–å¾—ã®ã¿ï¼‰----
    answer_backend = st.radio("å›ç­”ç”Ÿæˆ", ["OpenAI", "Retrieve-only"], index=0)

    # ---- System Instructionï¼ˆå½¹å‰²è¦å®šï¼‰----
    sys_inst = st.text_area("System Instruction", "ã‚ãªãŸã¯å„ªç§€ãªç¤¾å†…ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™.", height=80)

    # ---- è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰ï¼ˆé€æ¬¡è¡¨ç¤º or ä¸€æ‹¬è¡¨ç¤ºï¼‰----
    display_mode = st.radio(
        "è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰",
        ["é€æ¬¡è¡¨ç¤ºï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰", "ä¸€æ‹¬è¡¨ç¤º"],
        index=0,
        help="é€æ¬¡è¡¨ç¤ºã ã¨ä½“æ„ŸãŒé€Ÿããªã‚Šã¾ã™ã€‚Responses API ã® stream ã‚’ä½¿ç”¨ã€‚"
    )

    # ---- æ¤œç´¢å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ã®çµã‚Šè¾¼ã¿ï¼ˆOpenAI åŸ‹ã‚è¾¼ã¿å°‚ç”¨ï¼‰----
    st.divider()
    st.subheader("æ¤œç´¢å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ï¼ˆOpenAIï¼‰")
    shard_dirs_all = _list_shard_dirs_openai()
    shard_ids_all = [p.name for p in shard_dirs_all]
    target_shards = st.multiselect("ï¼ˆæœªé¸æŠ=ã™ã¹ã¦ï¼‰", shard_ids_all, default=shard_ids_all)

    # ---- å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆï¼ˆå¹´/ãƒ•ã‚¡ã‚¤ãƒ«åï¼‰----
    st.caption("ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ã ã‘ã§æ¤œç´¢ã—ãŸã„å ´åˆ: å¹´/ãƒ•ã‚¡ã‚¤ãƒ«å ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼ˆä¾‹: 2025/foo.pdf, 2024/bar.pdfï¼‰")
    file_whitelist_str = st.text_input("å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä»»æ„ï¼‰", value="")
    file_whitelist = {s.strip() for s in file_whitelist_str.split(",") if s.strip()}

    # ---- APIã‚­ãƒ¼å­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼ˆUIå´ã«è­¦å‘Šè¡¨ç¤ºï¼‰----
    has_key = bool(_get_openai_api_key())
    if not has_key and answer_backend == "OpenAI":
        st.error("OpenAI APIã‚­ãƒ¼ãŒ secrets.toml / ç’°å¢ƒå¤‰æ•°ã«ã‚ã‚Šã¾ã›ã‚“ã€‚åŸ‹ã‚è¾¼ã¿ã¨å›ç­”ç”Ÿæˆã®åŒæ–¹ã«å¿…é ˆã§ã™ã€‚")

    # ---- è§£æ±ºæ¸ˆã¿ã®ä¸»è¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°/ã‚µãƒãƒ¼ãƒˆå‘ã‘ï¼‰----
    st.divider()
    st.markdown("### ğŸ“‚ è§£æ±ºãƒ‘ã‚¹ï¼ˆå‚ç…§ç”¨ï¼‰")
    st.text_input("VS_ROOT", str(PATHS.vs_root), key="p_vs", disabled=True)
    st.text_input("PDF_ROOT", str(PATHS.pdf_root), key="p_pdf", disabled=True)
    st.text_input("BACKUP_ROOT", str(PATHS.backup_root), key="p_bak", disabled=True)
    if hasattr(PATHS, "data_root"):
        st.text_input("DATA_ROOT", str(PATHS.data_root), key="p_data", disabled=True)

    # ---- ãƒ‡ãƒ¢ç”¨ã‚µãƒ³ãƒ—ãƒ«ï¼ˆã‚«ãƒ†ã‚´ãƒªâ†’è³ªå•é¸æŠã€ãƒ©ãƒ³ãƒ€ãƒ æŒ¿å…¥ã€å³é€ä¿¡ï¼‰----
    st.divider()
    st.subheader("ğŸ§ª ãƒ‡ãƒ¢ç”¨ã‚µãƒ³ãƒ—ãƒ«è³ªå•")
    cat = st.selectbox("ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ", ["ï¼ˆæœªé¸æŠï¼‰"] + list(SAMPLES.keys()))
    sample = ""
    if cat != "ï¼ˆæœªé¸æŠï¼‰":
        sample = st.selectbox("ã‚µãƒ³ãƒ—ãƒ«è³ªå•ã‚’é¸æŠ", ["ï¼ˆæœªé¸æŠï¼‰"] + SAMPLES[cat])
    else:
        st.caption("ã‚«ãƒ†ã‚´ãƒªã‚’é¸ã¶ã‹ã€ä¸‹ã®ãƒ©ãƒ³ãƒ€ãƒ æŒ¿å…¥ã‚’ä½¿ãˆã¾ã™ã€‚")

    cols_demo = st.columns(2)
    with cols_demo[0]:
        st.button(
            "â¬‡ï¸ ã“ã®è³ªå•ã‚’å…¥åŠ›æ¬„ã¸ã‚»ãƒƒãƒˆ",
            use_container_width=True,
            disabled=(sample in ("", "ï¼ˆæœªé¸æŠï¼‰")),
            on_click=lambda: _set_question(sample),
        )
    with cols_demo[1]:
        st.button(
            "ğŸ² ãƒ©ãƒ³ãƒ€ãƒ æŒ¿å…¥",
            use_container_width=True,
            on_click=lambda: _set_question(random.choice(ALL_SAMPLES)),
        )
    send_now = st.button(
        "ğŸš€ ã‚µãƒ³ãƒ—ãƒ«ã§å³é€ä¿¡",
        use_container_width=True,
        disabled=(st.session_state.q.strip() == ""),
    )

# ---- å…¥åŠ›æ¬„ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³é€£æºï¼‰----
q = st.text_area(
    "è³ªå•ã‚’å…¥åŠ›",
    value=st.session_state.q,
    placeholder="ã“ã®ç¤¾å†…ãƒœãƒƒãƒˆã«è³ªå•ã—ã¦ãã ã•ã„â€¦",
    height=100,
)
if q != st.session_state.q:
    st.session_state.q = q

# ---- é€ä¿¡åˆ¤å®šï¼ˆé€šå¸¸é€ä¿¡ or ã‚µãƒ³ãƒ—ãƒ«å³é€ä¿¡ï¼‰----
go_click = st.button("é€ä¿¡", type="primary")
go = go_click or bool(locals().get("send_now"))

# =========================
# å®Ÿè¡Œï¼ˆæ¤œç´¢ â†’ ç”Ÿæˆ â†’ ã‚³ã‚¹ãƒˆè¡¨ç¤º â†’ å‚ç…§è¡¨ç¤ºï¼‰
# =========================
if go and st.session_state.q.strip():
    api_key = _get_openai_api_key()

    try:
        # ---------- æ¤œç´¢ãƒ•ã‚§ãƒ¼ã‚º ----------
        # - ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ï¼ˆOpenAIé…ä¸‹ï¼‰ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’åˆ—æŒ™
        # - Top-K ã‚’å„ã‚·ãƒ£ãƒ¼ãƒ‰ã§å–ã‚Šã€ã‚¹ã‚³ã‚¢ã§ã‚°ãƒ­ãƒ¼ãƒãƒ«ä¸Šä½Kã«ãƒãƒ¼ã‚¸
        with st.spinner("æ¤œç´¢ä¸­â€¦"):
            vs_backend_dir = VS_ROOT / "openai"
            if not vs_backend_dir.exists():
                st.warning(f"ãƒ™ã‚¯ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆ{vs_backend_dir}ï¼‰ã€‚å…ˆã« **03 ãƒ™ã‚¯ãƒˆãƒ«åŒ–** ã‚’ OpenAI ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
                st.stop()

            # â–¼ å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ã®æ±ºå®šï¼ˆæœªæŒ‡å®šãªã‚‰å…¨ã‚·ãƒ£ãƒ¼ãƒ‰ï¼‰
            shard_dirs_all = _list_shard_dirs_openai()
            selected = [vs_backend_dir / s for s in target_shards] if target_shards \
                       else [vs_backend_dir / p.name for p in shard_dirs_all]
            shard_dirs = [p for p in selected if p.is_dir() and (p / "vectors.npy").exists()]
            if not shard_dirs:
                st.warning("æ¤œç´¢å¯èƒ½ãªã‚·ãƒ£ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
                st.stop()

            # â–¼ ã‚¯ã‚¨ãƒªå†…ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³æŒ‡å®š [[files: a.pdf, b.pdf]]
            inline = re.search(r"\[\[\s*files\s*:\s*([^\]]+)\]\]", st.session_state.q, flags=re.IGNORECASE)
            inline_files = set()
            if inline:
                inline_files = {s.strip() for s in inline.group(1).split(",") if s.strip()}

            # â–¼ å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆï¼ˆUIæŒ‡å®š + ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³æŒ‡å®šã®å’Œï¼‰
            effective_whitelist = {_norm_path(x) for x in (set(file_whitelist) | set(inline_files))}

            # â–¼ å®Ÿéš›ã«åŸ‹ã‚è¾¼ã‚€è³ªå•ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚¤ãƒ³ãƒ©ã‚¤ãƒ³æŒ‡å®šã¯å‰Šé™¤ã—ã¦æ­£è¦åŒ–ï¼‰
            clean_q = re.sub(r"\[\[\s*files\s*:[^\]]+\]\]", "", st.session_state.q, flags=re.IGNORECASE).strip()
            question = normalize_ja_text(clean_q)

            # â–¼ OpenAI åŸ‹ã‚è¾¼ã¿ï¼ˆå˜ä¸€ã‚¯ã‚¨ãƒªï¼‰
            estore = EmbeddingStore(backend="openai")
            emb_tokens = _count_tokens(question, model_hint="text-embedding-3-large")
            qv = estore.embed([question]).astype("float32")  # shape=(1, d)

            # â–¼ å„ã‚·ãƒ£ãƒ¼ãƒ‰ã®Top-Kçµæœã‚’æœ€å¤§ãƒ’ãƒ¼ãƒ—ã§ãƒãƒ¼ã‚¸ï¼ˆã‚¹ã‚³ã‚¢é™é †ï¼‰
            K = int(top_k)
            heap_: List[Tuple[float, int, int, Dict[str, Any]]] = []
            tiebreak = count()  # ã‚¹ã‚³ã‚¢ä¸€è‡´æ™‚ã®ã‚¿ã‚¤ãƒ–ãƒ¬ãƒ¼ã‚¯

            for shp in shard_dirs:
                try:
                    vdb = NumpyVectorDB(shp)
                    # return_="similarity" ã‚’æŒ‡å®šã— (row_idx, score, meta) ã‹ (score, meta) ã‚’å¾—ã‚‹
                    hits = vdb.search(qv, top_k=K, return_="similarity")
                    for h in hits:
                        if isinstance(h, tuple) and len(h) == 3:
                            row_idx, score, meta = h
                        elif isinstance(h, tuple) and len(h) == 2:
                            score, meta = h
                            row_idx = -1
                        else:
                            continue

                        md = dict(meta or {})
                        md["shard_id"] = shp.name

                        # â–¼ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆãŒã‚ã‚Œã°ãƒ•ã‚£ãƒ«ã‚¿
                        if effective_whitelist:
                            if _norm_path(str(md.get("file", ""))) not in effective_whitelist:
                                continue

                        sc = float(score)
                        tb = next(tiebreak)

                        # â–¼ ä¸Šä½Kã‚’ç¶­æŒã™ã‚‹æœ€å°ãƒ’ãƒ¼ãƒ—
                        if len(heap_) < K:
                            heapq.heappush(heap_, (sc, tb, row_idx, md))
                        else:
                            if sc > heap_[0][0]:
                                heapq.heapreplace(heap_, (sc, tb, row_idx, md))
                except Exception as e:
                    # ã‚·ãƒ£ãƒ¼ãƒ‰å˜ä½ã®æ¤œç´¢ã‚¨ãƒ©ãƒ¼ã¯å…¨ä½“åœæ­¢ã›ãšè­¦å‘Šã®ã¿ã«ç•™ã‚ã‚‹
                    st.warning(f"ã‚·ãƒ£ãƒ¼ãƒ‰ {shp.name} ã®æ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼: {e}")

            # â–¼ ã‚¹ã‚³ã‚¢é™é †ã«ä¸¦ã¹æ›¿ãˆã¦ä¸Šä½ãƒ’ãƒƒãƒˆã‚’å¾—ã‚‹
            raw_hits = [(rid, sc, md) for (sc, _tb, rid, md) in sorted(heap_, key=lambda x: x[0], reverse=True)]

        # ãƒ’ãƒƒãƒˆãªã—æ™‚ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é€šçŸ¥ã—ã¦åœæ­¢
        if not raw_hits:
            if effective_whitelist:
                st.warning("æŒ‡å®šã•ã‚ŒãŸå‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«å†…ã§è©²å½“ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å¹´/ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆä¾‹: 2025/foo.pdfï¼‰ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
            else:
                st.warning("è©²å½“ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚„ Top-K ã‚’èª¿æ•´ã—ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        # ---------- å›ç­”ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚º ----------
        # - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ï¼ˆSç•ªå·ä»˜ãã®æ–‡è„ˆã‚’ä»˜ä¸ï¼‰
        # - lib/gpt_responder çµŒç”±ã§ç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚° or ä¸€æ‹¬ï¼‰
        # - ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸å¯ã‚¨ãƒ©ãƒ¼æ™‚ã¯è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        chat_prompt_tokens = 0
        chat_completion_tokens = 0
        answer = None

        use_answer_backend = answer_backend
        if use_answer_backend == "OpenAI" and not api_key:
            use_answer_backend = "Retrieve-only"

        if use_answer_backend == "OpenAI":
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã®ãƒ©ãƒ™ãƒ«ä»˜ãæ–‡è„ˆã‚’ç”Ÿæˆ
            labeled_contexts = [
                f"[S{i}] {meta.get('text','')}\n[meta: {_fmt_source(meta)} / score={float(score):.3f}]"
                for i, (_rid, score, meta) in enumerate(raw_hits, 1)
            ]
            prompt = build_prompt(
                question,
                labeled_contexts,
                sys_inst=sys_inst,
                style_hint=detail,
                cite=cite,
                strict=False,
            )

            # â˜… å…±é€šãƒ©ãƒƒãƒ‘åˆæœŸåŒ–ï¼ˆAPIã‚­ãƒ¼ã¯ secrets / env ã‚’å„ªå…ˆï¼‰
            responder = GPTResponder(api_key=api_key)

            if display_mode == "é€æ¬¡è¡¨ç¤ºï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰":
                st.subheader("ğŸ§  å›ç­”ï¼ˆé€æ¬¡è¡¨ç¤ºï¼‰")

                # é€æ¬¡å‡ºåŠ›ã‚’ Streamlit ã«ãã®ã¾ã¾æµã™ï¼ˆå†…éƒ¨ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚å‡¦ç†ï¼‰
                st.write_stream(
                    responder.stream(
                        model=chat_model,
                        system_instruction=sys_inst,
                        user_content=prompt,
                        max_output_tokens=int(max_tokens),
                        on_error_text="Responses stream error.",
                    )
                )
                # ã‚¹ãƒˆãƒªãƒ¼ãƒ å®Œäº†å¾Œã®æœ€çµ‚å€¤ã‚’å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ã‚‚è¨­å®šæ¸ˆã¿ï¼‰
                answer = responder.final_text
                chat_prompt_tokens = responder.usage.input_tokens
                chat_completion_tokens = responder.usage.output_tokens

            else:
                # ä¸€æ‹¬ç”Ÿæˆï¼ˆéã‚¹ãƒˆãƒªãƒ¼ãƒ ï¼‰
                with st.spinner("å›ç­”ç”Ÿæˆä¸­â€¦"):
                    result: CompletionResult = responder.complete(
                        model=chat_model,
                        system_instruction=sys_inst,
                        user_content=prompt,
                        max_output_tokens=int(max_tokens),
                    )
                answer = result.text
                chat_prompt_tokens = result.usage.input_tokens
                chat_completion_tokens = result.usage.output_tokens

                st.subheader("ğŸ§  å›ç­”")
                st.write(answer)

        else:
            # å›ç­”ç”Ÿæˆã‚’è¡Œã‚ãªã„ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç¢ºèªç”¨ãªã©ï¼‰
            st.subheader("ğŸ§© å–å¾—ã®ã¿ï¼ˆè¦ç´„ãªã—ï¼‰")
            st.info("Retrieve-only ãƒ¢ãƒ¼ãƒ‰ã§ã™ã€‚ä¸‹ã®å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚")

        # ---------- ã‚³ã‚¹ãƒˆè¡¨ç¤ºãƒ•ã‚§ãƒ¼ã‚º ----------
        # - åŸ‹ã‚è¾¼ã¿ã®æ¦‚ç®—ã‚³ã‚¹ãƒˆ + å›ç­”ç”Ÿæˆã®æ¦‚ç®—ã‚³ã‚¹ãƒˆï¼ˆUSDâ†’JPYï¼‰ã‚’ç®—å‡º
        with st.container():
            emb_cost_usd = estimate_embedding_cost(
                "text-embedding-3-large",
                _count_tokens(st.session_state.q, "text-embedding-3-large"),
            )["usd"]
            chat_cost_usd = 0.0
            if use_answer_backend == "OpenAI":
                chat_cost_usd = estimate_chat_cost(
                    chat_model,
                    ChatUsage(input_tokens=chat_prompt_tokens, output_tokens=chat_completion_tokens),
                )["usd"]

            total_usd = emb_cost_usd + chat_cost_usd
            total_jpy = usd_to_jpy(total_usd, rate=DEFAULT_USDJPY)

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼šåˆè¨ˆJPY / å†…è¨³USD / å˜ä¾¡ï¼ˆUSD/1Kï¼‰
            st.markdown("### ğŸ’´ ä½¿ç”¨æ–™ã®æ¦‚ç®—ï¼ˆlib/costs ã«ã‚ˆã‚‹é›†è¨ˆï¼‰")
            cols = st.columns(3)
            with cols[0]:
                st.metric("åˆè¨ˆ (JPY)", f"{total_jpy:,.2f} å††")
                st.caption(f"ç‚ºæ›¿ {DEFAULT_USDJPY:.2f} JPY/USD")
            with cols[1]:
                st.write("**å†…è¨³ (USD)**")
                st.write(f"- Embedding: `${emb_cost_usd:.6f}`")
                if use_answer_backend == "OpenAI":
                    st.write(
                        f"- Chat åˆè¨ˆ: `${chat_cost_usd:.6f}` "
                        f"(in={chat_prompt_tokens} tok / out={chat_completion_tokens} tok)"
                    )
                st.write(f"- åˆè¨ˆ: `${total_usd:.6f}`")
            with cols[2]:
                emb_price_per_1k = float(EMBEDDING_PRICES_USD.get("text-embedding-3-large", 0.0)) / 1000.0
                st.write("**å˜ä¾¡ (USD / 1K tok)**")
                st.write(f"- Embedding: `${emb_price_per_1k:.5f}`ï¼ˆtext-embedding-3-largeï¼‰")
                st.write(f"- Chat å…¥åŠ›: `${MODEL_PRICES_PER_1K.get(chat_model,{}).get('in',0.0):.5f}`ï¼ˆ{chat_model}ï¼‰")
                st.write(f"- Chat å‡ºåŠ›: `${MODEL_PRICES_PER_1K.get(chat_model,{}).get('out',0.0):.5f}`ï¼ˆ{chat_model}ï¼‰")

        # ---------- å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¸Šä½ãƒ’ãƒƒãƒˆï¼‰ ----------
        with st.expander("ğŸ” å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¸Šä½ãƒ’ãƒƒãƒˆï¼‰", expanded=False):
            for i, (_rid, score, meta) in enumerate(raw_hits, 1):
                txt = str(meta.get("text", "") or "")
                src_label = _fmt_source(meta)
                snippet = (txt[:1000] + "â€¦") if len(txt) > 1000 else txt
                st.markdown(f"**[S{i}] score={float(score):.3f}**  `{src_label}`\n\n{snippet}")

    except Exception as e:
        # æ¤œç´¢/ç”Ÿæˆã‚’å«ã‚€å…¨ä½“ã®ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼ˆUIã«å¯è¦–åŒ–ï¼‰
        st.error(f"æ¤œç´¢/ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
else:
    # åˆæœŸçŠ¶æ…‹ã®ã‚¬ã‚¤ãƒ‰
    st.info("è³ªå•ã‚’å…¥åŠ›ã—ã¦ã€é€ä¿¡ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã‚·ãƒ£ãƒ¼ãƒ‰ã‚„å›ç­”è¨­å®šã€å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª¿æ•´ã§ãã¾ã™ã€‚")
