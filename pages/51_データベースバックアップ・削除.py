# pages/51_ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å‰Šé™¤.py
# ------------------------------------------------------------
# ğŸ—‘ï¸ ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ãƒšãƒ¼ã‚¸
# - å‰Šé™¤ / åˆæœŸåŒ– / ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— / å¾©å…ƒ / å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ•´ç†
# - ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿å­˜å…ˆï¼š<é¸æŠã—ãŸ BACKUP_ROOT> / <backend> / <shard_id> / <timestamp>
# - è¿½åŠ æ©Ÿèƒ½:
#   1) ã™ã¹ã¦ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’å³æ™‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
#   2) å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ã®ã¿å³æ™‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
#   3) ã€Œæœªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ—¥æ•°ã€ã—ãã„å€¤ã§ä¸€æ‹¬ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
#   4) ã‚·ãƒ£ãƒ¼ãƒ‰ã”ã¨å‰Šé™¤ï¼ˆãƒ•ã‚©ãƒ«ãƒ€å®Œå…¨å‰Šé™¤â†’ç©ºãƒ•ã‚©ãƒ«ãƒ€å†ä½œæˆï¼‰
#   5) å®Œå…¨åˆæœŸåŒ–ã«ã‚‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼†DELETEç¢ºèª
#   6) å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®å‰Šé™¤ï¼ˆæœ€æ–°Nä»¶ / ã—ãã„å€¤æ—¥æ•°ï¼‰
#   7) æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— vs ç¾è¡Œ å·®åˆ†é›†è¨ˆï¼ˆå…¨ã‚·ãƒ£ãƒ¼ãƒ‰ï¼‰
#   8) (pno,file) å·®åˆ†æŠ½å‡º / é¸æŠåŒæœŸï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—â†’ç¾è¡Œï¼‰
# ------------------------------------------------------------
from __future__ import annotations
from pathlib import Path
import json
import shutil
import numpy as np
import pandas as pd
import streamlit as st

from config.path_config import PATHS  # âœ… vs_root / backup_root ã‚’é›†ä¸­ç®¡ç†
from lib.rag.vectorstore_utils import iter_jsonl  # æ—¢å­˜ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£

# ğŸ” å¤–éƒ¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆlib ä¸‹ã¸åˆ‡ã‚Šå‡ºã—æ¸ˆã¿ï¼‰
from lib.backup_utils import (
    backup_all_local,
    list_backup_dirs_local,
    preview_backup_local,
    restore_from_backup_local,
    backup_age_days_local,
    cleanup_old_backups_keep_last,
    cleanup_old_backups_older_than_days,
)

from lib.processed_files_utils import remove_from_processed_files_selective

from lib.diff_utils import (
    diff_all_shards,
    meta_diff_for_shard,
    load_only_bak_pairs_for_shard,
    sync_pairs_from_backup_to_live,
    normalize_path,
)

# ============================================================
# åŸºæœ¬ãƒ‘ã‚¹
# ============================================================
VS_ROOT: Path = PATHS.vs_root

# ============================================================
# UI è¨­å®š
# ============================================================
st.set_page_config(page_title="51 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å‰Šé™¤", page_icon="ğŸ—‘ï¸", layout="wide")
st.title("ğŸ—‘ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å‰Šé™¤ï¼ˆã‚·ãƒ£ãƒ¼ãƒ‰å˜ä½ï¼‰")

st.markdown(
    f"""
**VectorStore:** `{VS_ROOT}`  
**æ¨™æº–ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—:** `{PATHS.backup_root}`  
**å¤–ä»˜ã‘SSDãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—:** `{PATHS.backup_root2}`  
**å¤–ä»˜ã‘SSDãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—2:** `{PATHS.backup_root3}`
    """,
    unsafe_allow_html=True,
)

st.info("ã“ã®ãƒšãƒ¼ã‚¸ã¯ **å‰Šé™¤ãƒ»åˆæœŸåŒ–ãƒ»ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—/å¾©å…ƒãƒ»å·®åˆ†åŒæœŸ** ã«ç‰¹åŒ–ã—ã¦ã„ã¾ã™ã€‚ä½œæ¥­å‰ã«å¿…ãšãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")

# ============================================================
# ã‚µã‚¤ãƒ‰ãƒãƒ¼: ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰/ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å…ˆ/ã‚·ãƒ£ãƒ¼ãƒ‰é¸æŠï¼ˆé¸æŠå¤‰æ›´ã§å³å†èª­è¾¼ï¼‰
# ============================================================
# ã‚µã‚¤ãƒ‰ãƒãƒ¼ãŒä¿å­˜ã—ãŸç¾åœ¨ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿å­˜å…ˆã‚’å–å¾—ï¼ˆç„¡ã‘ã‚Œã°æ¨™æº–ï¼‰
BACKUP_ROOT = Path(st.session_state.get("CURRENT_BACKUP_ROOT", str(PATHS.backup_root)))
try:
    BACKUP_ROOT.mkdir(parents=True, exist_ok=True)
except Exception:
    pass

with st.sidebar:
    st.header("å¯¾è±¡ã¨ä¿å­˜å…ˆ")

    # 1) ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
    backend = st.radio("ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰", ["openai", "local"], index=0, horizontal=True, key="sb_backend")

    # 2) ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿å­˜å…ˆï¼ˆã“ã“ã§é¸ã‚“ã ãƒ«ãƒ¼ãƒˆã§ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã‚€ï¼‰
    dest_label = st.radio(
        "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿å­˜å…ˆ",
        ["æ¨™æº–ï¼ˆbackup_rootï¼‰", "å¤–ä»˜ã‘SSDï¼ˆbackup_root2ï¼‰", "å¤–ä»˜ã‘SSD2ï¼ˆbackup_root3ï¼‰"],
        horizontal=True,
        key="bak_dest_sidebar",
    )

    def _resolve_backup_root(label: str) -> Path:
        if "SSD2" in label:
            return PATHS.backup_root3
        elif "SSD" in label:
            return PATHS.backup_root2
        else:
            return PATHS.backup_root

    # é¸æŠä¸­ã® BACKUP_ROOT ã‚’æ±ºå®šã—ã€ä»–ã®å‡¦ç†ã‹ã‚‰ã‚‚å‚ç…§ã§ãã‚‹ã‚ˆã†ã«ä¿å­˜
    BACKUP_ROOT = _resolve_backup_root(dest_label)
    try:
        BACKUP_ROOT.mkdir(parents=True, exist_ok=True)
    except Exception:
        pass
    st.session_state["CURRENT_BACKUP_ROOT"] = str(BACKUP_ROOT)

    # 3) ã‚·ãƒ£ãƒ¼ãƒ‰ä¸€è¦§ï¼ˆç¾è¡Œ + é¸æŠä¸­ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å…ˆã®å’Œé›†åˆï¼‰
    base_backend_dir = VS_ROOT / backend
    live_shards = []
    if base_backend_dir.exists():
        try:
            live_shards = sorted([p.name for p in base_backend_dir.iterdir() if p.is_dir()])
        except Exception:
            live_shards = []

    backup_backend_dir = BACKUP_ROOT / backend
    backup_shards = []
    if backup_backend_dir.exists():
        try:
            backup_shards = sorted([p.name for p in backup_backend_dir.iterdir() if p.is_dir()])
        except Exception:
            backup_shards = []

    shard_ids_all = sorted(set(live_shards) | set(backup_shards))

    def _label_for(sid: str) -> str:
        badges = []
        if sid in live_shards:
            badges.append("ç¾è¡Œ")
        if sid in backup_shards:
            if BACKUP_ROOT == PATHS.backup_root:
                badges.append("B1")
            elif BACKUP_ROOT == PATHS.backup_root2:
                badges.append("B2")
            else:
                badges.append("B3")
        suffix = f"ï¼ˆ{'/'.join(badges)}ï¼‰" if badges else ""
        return f"{sid}{suffix}"

    # â–¶ æ°¸ç¶šåŒ–ã•ã‚ŒãŸç¾åœ¨ã®é¸æŠã‚’å–å¾—ï¼ˆç„¡ã‘ã‚Œã° Noneï¼‰
    current_shard = st.session_state.get("sb_shard_value")

    # å€™è£œãŒç©ºã®å ´åˆã®ã‚¬ãƒ¼ãƒ‰
    if not shard_ids_all:
        st.warning("ã‚·ãƒ£ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆç¾è¡Œã¨é¸æŠä¸­ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å…ˆã®ã„ãšã‚Œã«ã‚‚å­˜åœ¨ã—ã¾ã›ã‚“ï¼‰ã€‚")
        # ç©ºã§ã‚‚ state ã¯è§¦ã£ã¦ãŠãï¼ˆå¾Œç¶šã®å‚ç…§ã‚¨ãƒ©ãƒ¼é˜²æ­¢ï¼‰
        st.session_state["sb_shard_value"] = None
        shard_id = "(ãªã—)"
    else:
        # ç¾åœ¨å€¤ãŒå€™è£œã«ç„¡ã‘ã‚Œã°ã€å…ˆé ­ã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if current_shard not in shard_ids_all:
            current_shard = shard_ids_all[0]

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—ã—ã¦ã‹ã‚‰ selectbox ã‚’æç”»
        default_idx = shard_ids_all.index(current_shard)
        sel = st.selectbox(
            "å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ï¼ˆã‚«ãƒƒã‚³å†…ã¯å­˜åœ¨å ´æ‰€: ç¾è¡Œ=VS_ROOT, B1/B2/B3=é¸æŠä¸­ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å…ˆï¼‰",
            options=shard_ids_all,
            index=default_idx,
            format_func=_label_for,
            key="sb_shard_selectbox"  # â† UI ã®å†…éƒ¨çŠ¶æ…‹ã‚­ãƒ¼ï¼ˆä¿æŒç”¨ï¼‰
        )

        # é¸æŠçµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜ï¼ˆæ¬¡å›ãƒªãƒ©ãƒ³æ™‚ã®ä¿æŒã«ä½¿ã†ï¼‰
        st.session_state["sb_shard_value"] = sel
        shard_id = sel
    

# ============================================================
# å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ã®ãƒ‘ã‚¹ï¼ˆç¾è¡Œï¼‰
# ============================================================
base_dir = VS_ROOT / backend / shard_id
meta_path = base_dir / "meta.jsonl"
vec_path  = base_dir / "vectors.npy"
pf_path   = base_dir / "processed_files.json"

# åˆæœŸå­˜åœ¨ä¿è¨¼ï¼ˆç¾è¡Œå´ã¯ç©ºã§ã‚‚ã‚ˆã„ãŒã€ãƒ•ã‚©ãƒ«ãƒ€ã¯ä½œã£ã¦ãŠãï¼‰
base_dir.mkdir(parents=True, exist_ok=True)

# ============================================================
# ğŸ›¡ï¸ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆæ‹¡å¼µï¼‰
# ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿å­˜å…ˆã¯ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§é¸æŠæ¸ˆã¿ï¼‰
# ============================================================
st.subheader("ğŸ›¡ï¸ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆæ‹¡å¼µï¼‰")
st.caption(f"ç¾åœ¨ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿å­˜å…ˆ: `{BACKUP_ROOT}`")

col_a, col_b, col_c = st.columns(3)
with col_a:
    if st.button("âš¡ å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’å³æ™‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—", use_container_width=True, key="bak_one"):
        copied, bdir = backup_all_local(base_dir, BACKUP_ROOT, backend, shard_id)
        if copied:
            st.success(f"[{backend}/{shard_id}] ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {bdir}")
        else:
            st.info(f"[{backend}/{shard_id}] ã®ã‚³ãƒ”ãƒ¼å¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆç©ºã‚·ãƒ£ãƒ¼ãƒ‰ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ï¼‰ã€‚ ä¿å­˜å…ˆ: {bdir}")

with col_b:
    if st.button("âš¡ ã™ã¹ã¦ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’å³æ™‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—", use_container_width=True, key="bak_all"):
        summary = []
        # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¡¨ç¤ºã—ã¦ã„ã‚‹å’Œé›†åˆã‚’åˆ©ç”¨
        for sid in (shard_ids_all if 'shard_ids_all' in locals() else []):
            sdir = VS_ROOT / backend / sid
            sdir.mkdir(parents=True, exist_ok=True)  # ç©ºã§ã‚‚OK
            copied, bdir = backup_all_local(sdir, BACKUP_ROOT, backend, sid)
            summary.append((sid, len(copied), bdir))
        ok = [f"- {sid}: {n}é …ç›® -> {bdir}" for sid, n, bdir in summary]
        st.success("å³æ™‚ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†:\n" + ("\n".join(ok) if ok else "å¯¾è±¡ãªã—"))

with col_c:
    threshold = st.selectbox("æœªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ—¥æ•° ä»¥ä¸Šãªã‚‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—", [1, 2, 3, 7, 14, 30], index=2, key="bak_thr")
    if st.button("ğŸ—“ æ¡ä»¶ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ", use_container_width=True, key="bak_cond"):
        targets = (shard_ids_all if 'shard_ids_all' in locals() else [])
        triggered, skipped = [], []
        for sid in targets:
            age = backup_age_days_local(BACKUP_ROOT, backend, sid)
            if age is None or age >= float(threshold):
                sdir = VS_ROOT / backend / sid
                sdir.mkdir(parents=True, exist_ok=True)
                copied, bdir = backup_all_local(sdir, BACKUP_ROOT, backend, sid)
                triggered.append((sid, age, len(copied), bdir))
            else:
                skipped.append((sid, age))
        msg = ""
        if triggered:
            msg += "ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè¡Œï¼ˆé–¾å€¤è¶…é or æœªå®Ÿæ–½ï¼‰:\n" + "\n".join(
                f"- {sid}: age={('None' if age is None else f'{age:.2f}d')} -> {n}é …ç›® @ {bdir}"
                for sid, age, n, bdir in triggered
            )
        if skipped:
            if msg:
                msg += "\n\n"
            msg += "ã‚¹ã‚­ãƒƒãƒ—ï¼ˆé–¾å€¤æœªæº€ï¼‰:\n" + "\n".join(f"- {sid}: age={age:.2f}d" for sid, age in skipped)
        st.info(msg or "å¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

st.divider()

# ============================================================
# ğŸ§¹ å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®æ•´ç†ï¼ˆæœ€æ–°Nä»¶ / ã—ãã„å€¤æ—¥æ•°ï¼‰
# ============================================================
st.subheader("ğŸ§¹ å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®æ•´ç†")

scope_label = st.radio(
    "å‰Šé™¤å¯¾è±¡ã‚¹ã‚³ãƒ¼ãƒ—",
    ["ç¾åœ¨ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã®ã¿", "å…¨ã‚·ãƒ£ãƒ¼ãƒ‰ï¼ˆbackendé…ä¸‹ã™ã¹ã¦ï¼‰"],
    horizontal=True,
    key="cleanup_scope"
)

c1, c2 = st.columns(2)
with c1:
    keep_last = st.number_input("ä¿æŒã™ã‚‹æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ•°", min_value=1, max_value=50, value=3, step=1, key="keep_last_bak")
    if st.button("ğŸ§¹ æœ€æ–°Nä»¶ã‚’æ®‹ã—ã¦å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤", use_container_width=True, key="btn_cleanup_keep_last"):
        targets = (shard_ids_all if "å…¨ã‚·ãƒ£ãƒ¼ãƒ‰" in scope_label else [shard_id])
        all_deleted = []
        for sid in targets:
            deleted = cleanup_old_backups_keep_last(BACKUP_ROOT, backend, sid, keep_last=int(keep_last))
            all_deleted.extend(deleted)
        if all_deleted:
            st.success(f"ä»¥ä¸‹ã®å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤ã—ã¾ã—ãŸ ({len(all_deleted)} ä»¶):\n" +
                       "\n".join(f"- {d}" for d in all_deleted))
        else:
            st.info("å‰Šé™¤å¯¾è±¡ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

with c2:
    older_days = st.number_input("ã“ã®æ—¥æ•°ã‚ˆã‚Šå¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤", min_value=1, max_value=3650, value=90, step=1, key="older_days_bak")
    if st.button("ğŸ§¹ ã—ãã„å€¤æ—¥æ•°ã‚ˆã‚Šå¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤", use_container_width=True, key="btn_cleanup_older_than"):
        targets = (shard_ids_all if "å…¨ã‚·ãƒ£ãƒ¼ãƒ‰" in scope_label else [shard_id])
        all_deleted = []
        for sid in targets:
            deleted = cleanup_old_backups_older_than_days(BACKUP_ROOT, backend, sid, older_than_days=int(older_days))
            all_deleted.extend(deleted)
        if all_deleted:
            st.success(f"ä»¥ä¸‹ã®å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤ã—ã¾ã—ãŸ ({len(all_deleted)} ä»¶):\n" +
                       "\n".join(f"- {d}" for d in all_deleted))
        else:
            st.info("å‰Šé™¤å¯¾è±¡ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

st.divider()

# ============================================================
# ğŸ“„ ç¾çŠ¶ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆç¾è¡Œï¼‰
# ============================================================
st.subheader("ğŸ“„ ç¾çŠ¶ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆç¾è¡Œï¼‰")
rows = [dict(obj) for obj in iter_jsonl(meta_path)] if meta_path.exists() else []
if not rows:
    st.caption("ã“ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã«ã¯ meta.jsonl ãŒå­˜åœ¨ã—ãªã„ã‹ã€ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
else:
    df = pd.DataFrame(rows)
    if "file" not in df.columns:
        df["file"] = None
    st.caption(f"ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df):,}")
    st.dataframe(df.head(500), use_container_width=True, height=420)

st.divider()

# ============================================================
# ğŸ“¦ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆå€‹åˆ¥ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰
# ============================================================
st.subheader("ğŸ“¦ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆå€‹åˆ¥ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰")
bdirs_prev = list_backup_dirs_local(BACKUP_ROOT, backend, shard_id)
if bdirs_prev:
    sel_bdir_prev = st.selectbox("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", bdirs_prev, format_func=lambda p: p.name, key="prev_bdir")
    if sel_bdir_prev:
        st.dataframe(preview_backup_local(sel_bdir_prev), use_container_width=True, height=180)
else:
    st.caption("ã¾ã ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

st.divider()

# ============================================================
# ğŸ§¹ é¸æŠãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼ˆå†…éƒ¨ãƒ¡ãƒ¢ãƒªå†…ï¼‰
# ============================================================
st.subheader("ğŸ§¹ é¸æŠãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼ˆå†…éƒ¨ãƒ¡ãƒ¢ãƒªå†…ï¼‰")
st.info(
    "- é¸æŠãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã™ã‚‹ meta.jsonl ã®è¡Œãƒ»vectors.npy ã®å¯¾å¿œãƒ™ã‚¯ãƒˆãƒ«ãƒ»processed_files.json ã®é …ç›®ã‚’å®‰å…¨ã«å‰Šé™¤ã—ã¾ã™ã€‚\n"
    "- å®Ÿè¡Œå‰ã« **ç¾è¡Œã®ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆ-Rollbackï¼‰** ã‚’è‡ªå‹•ä½œæˆã—ã¾ã™ã€‚å‰Šé™¤å¾Œã¯å¿…è¦ã«å¿œã˜ã¦ **-PostOp** ã‚’æ‰‹å‹•/è‡ªå‹•ã§å–å¾—ã—ã¦ãã ã•ã„ã€‚"
)
if rows:
    files = sorted(pd.Series([r.get("file") for r in rows if r.get("file")]).unique().tolist())
    c1, c2 = st.columns([2, 1])
    with c1:
        target_files = st.multiselect("å‰Šé™¤å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆyear/file.pdf ãªã©ï¼‰", files, key="sel_targets")
    with c2:
        st.caption("processed_files.json ã®å‡¦ç†: é¸æŠãƒ•ã‚¡ã‚¤ãƒ«ã‚’ processed_files.json ã‹ã‚‰å‰Šé™¤ï¼ˆæ—¢å®šå‹•ä½œï¼‰")
        confirm_del = st.checkbox("å‰Šé™¤ã«åŒæ„ã—ã¾ã™", key="confirm_selective")

    if st.button(
        "ğŸ§¹ å‰Šé™¤å®Ÿè¡Œ",
        type="primary",
        use_container_width=True,
        disabled=not (target_files and confirm_del),
        key="btn_selective_delete",
    ):
        try:
            # ç›´å‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆRollbackï¼‰
            copied, bdir = backup_all_local(base_dir, BACKUP_ROOT, backend, shard_id, label="Rollback")
            if copied:
                st.caption(f"ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {bdir}")

            # meta.jsonl å†æ§‹ç¯‰ + vectors.npy åŒæœŸ
            keep_lines, keep_vec_indices = [], []
            removed_meta, valid_idx = 0, 0
            target_set = set(target_files)

            if meta_path.exists():
                with meta_path.open("r", encoding="utf-8") as f:
                    for raw in f:
                        s = raw.strip()
                        if not s:
                            continue
                        try:
                            obj = json.loads(s)
                        except Exception:
                            keep_lines.append(raw)  # å£Šã‚Œè¡Œã¯ä¿å…¨
                            continue
                        fname = obj.get("file") if isinstance(obj, dict) else None
                        if fname in target_set:
                            removed_meta += 1
                            valid_idx += 1
                            continue
                        keep_lines.append(json.dumps(obj, ensure_ascii=False) + "\n")
                        keep_vec_indices.append(valid_idx)
                        valid_idx += 1
                with meta_path.open("w", encoding="utf-8") as f:
                    f.writelines(keep_lines)

            removed_vecs = 0
            if vec_path.exists():
                vecs = np.load(vec_path)
                if vecs.ndim == 2:
                    new_vecs = vecs[keep_vec_indices] if keep_vec_indices else np.empty((0, vecs.shape[1]))
                    removed_vecs = vecs.shape[0] - new_vecs.shape[0]
                    np.save(vec_path, new_vecs)

            # processed_files.json æ›´æ–°
            pf_msg = ""
            if pf_path.exists():
                before, after, removed_pf, removed_list = remove_from_processed_files_selective(
                    pf_path, target_files
                )
                if removed_pf > 0:
                    pf_msg = (
                        "processed_files.json ã‚’æ›´æ–°ã—ã¾ã—ãŸ:\n"
                        f"- é™¤å¤–æ•°: {removed_pf} ä»¶ (before={before}, after={after})\n"
                        f"- é™¤å¤–ã•ã‚ŒãŸé …ç›®ã®ä¾‹: {removed_list}"
                    )
            else:
                pf_msg = "- processed_files.json: è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆå‡¦ç†ã‚¹ã‚­ãƒƒãƒ—ï¼‰\n"

            st.success(
                "å‰Šé™¤å®Œäº† âœ…\n"
                f"- meta.jsonl: {removed_meta} è¡Œå‰Šé™¤\n"
                f"- vectors.npy: {removed_vecs} è¡Œå‰Šé™¤\n"
                f"{pf_msg}"
                f"\n- ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯: {bdir}"
            )

            # â˜… å‡¦ç†å¾Œã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼ˆPostOpï¼‰
            try:
                _, bdir_post = backup_all_local(base_dir, BACKUP_ROOT, backend, shard_id, label="PostOp")
                st.caption(f"å‡¦ç†å¾Œã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼ˆPostOpï¼‰: {bdir_post}")
            except Exception:
                st.warning("å‡¦ç†å¾Œãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆPostOpï¼‰ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            st.error(f"å‰Šé™¤ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

st.divider()

# ============================================================
# ğŸ—‚ï¸ ã‚·ãƒ£ãƒ¼ãƒ‰ã”ã¨å‰Šé™¤ï¼ˆãƒ•ã‚©ãƒ«ãƒ€å®Œå…¨å‰Šé™¤ï¼‰
# ============================================================
st.subheader("ğŸ—‚ï¸ ã‚·ãƒ£ãƒ¼ãƒ‰ã”ã¨å‰Šé™¤ï¼ˆãƒ•ã‚©ãƒ«ãƒ€å®Œå…¨å‰Šé™¤ï¼‰ï¼ˆå†…éƒ¨ãƒ¡ãƒ¢ãƒªå†…ï¼‰")
st.info(
    "- é¸æŠä¸­ã®ã‚·ãƒ£ãƒ¼ãƒ‰ï¼ˆVS_ROOT/<backend>/<shard_id>ï¼‰ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä¸¸ã”ã¨å‰Šé™¤ã—ã€ç©ºã§å†ä½œæˆã—ã¾ã™ã€‚\n"
    "- å®Ÿè¡Œå‰ã« **-Rollback** ã‚’è‡ªå‹•ä½œæˆã—ã¾ã™ã€‚å¿…è¦ãªã‚‰å®Ÿè¡Œå¾Œã« **-PostOp** ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
)

def _dir_stats(d: Path) -> tuple[int, int]:
    if not d.exists():
        return (0, 0)
    n, total = 0, 0
    for p in d.rglob("*"):
        if p.is_file():
            n += 1
            try:
                total += p.stat().st_size
            except Exception:
                pass
    return n, total

if base_dir.exists():
    cnt, total = _dir_stats(base_dir)
    st.caption(f"ã“ã®ã‚·ãƒ£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ«ãƒ€é…ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°: **{cnt:,}** / åˆè¨ˆã‚µã‚¤ã‚º: **{total:,} bytes**")

colx, coly = st.columns([2, 1])
with colx:
    do_backup_before_shard_delete = st.checkbox(
        "å‰Šé™¤å‰ã«æ¨™æº–ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆ-Rollbackï¼‰ã‚’ä½œæˆã™ã‚‹",
        value=True,
        key="sharddel_backup",
    )
    confirm_shard_del = st.checkbox("ã‚·ãƒ£ãƒ¼ãƒ‰ã”ã¨å‰Šé™¤ã«åŒæ„ã—ã¾ã™ï¼ˆå…ƒã«æˆ»ã›ã¾ã›ã‚“ï¼‰", key="sharddel_confirm")
with coly:
    typed = st.text_input("ã‚¿ã‚¤ãƒ—ç¢ºèªï¼šDELETE ã¨å…¥åŠ›", value="", key="sharddel_typed")

if st.button(
    "ğŸ—‚ï¸ ã‚·ãƒ£ãƒ¼ãƒ‰ã”ã¨å‰Šé™¤ã‚’å®Ÿè¡Œ",
    type="secondary",
    use_container_width=True,
    disabled=not (confirm_shard_del and typed.strip().upper() == "DELETE"),
    key="sharddel_exec",
):
    try:
        if do_backup_before_shard_delete and base_dir.exists():
            copied, bdir = backup_all_local(base_dir, BACKUP_ROOT, backend, shard_id, label="Rollback")
            st.caption(f"äº‹å‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆRollbackï¼‰: {bdir}")

        if base_dir.exists():
            shutil.rmtree(base_dir)
        base_dir.mkdir(parents=True, exist_ok=True)
        st.success(f"ã‚·ãƒ£ãƒ¼ãƒ‰ `{backend}/{shard_id}` ã‚’å‰Šé™¤ï¼ˆãƒ•ã‚©ãƒ«ãƒ€å†ä½œæˆæ¸ˆã¿ï¼‰")

        # â˜… PostOp ã‚’æ®‹ã—ãŸã„å ´åˆã¯ä»¥ä¸‹ã‚’æœ‰åŠ¹åŒ–
        try:
            _, bdir_post = backup_all_local(base_dir, BACKUP_ROOT, backend, shard_id, label="PostOp")
            st.caption(f"å‡¦ç†å¾Œã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼ˆPostOpï¼‰: {bdir_post}")
        except Exception:
            pass
    except Exception as e:
        st.error(f"ã‚·ãƒ£ãƒ¼ãƒ‰å‰Šé™¤ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

st.divider()

# ============================================================
# ğŸ¯ year/pno æŒ‡å®šå‰Šé™¤
# ============================================================
st.subheader("ğŸ¯ year / pno æŒ‡å®šå‰Šé™¤")
st.info(
    "- æŒ‡å®šã® (year, pno) ã«ä¸€è‡´ã™ã‚‹ meta.jsonl ã®è¡Œã¨ vectors.npy ã®å¯¾å¿œãƒ™ã‚¯ãƒˆãƒ«ã€processed_files.json ã‚’æ•´åˆã•ã›ã¾ã™ã€‚\n"
    "- å®Ÿè¡Œå‰ã« **-Rollback** ã‚’è‡ªå‹•ä½œæˆã€å®Œäº†å¾Œã« **-PostOp** ã‚’ä½œæˆã—ã¾ã™ã€‚"
)

if rows:
    years = sorted(set(str(r.get("year")) for r in rows if r.get("year")))
    pnos = sorted(set(str(r.get("pno")) for r in rows if r.get("pno")))

    c1, c2 = st.columns(2)
    with c1:
        sel_year = st.selectbox("å¯¾è±¡ year", ["(æœªé¸æŠ)"] + years, key="sel_year")
    with c2:
        sel_pno = st.selectbox("å¯¾è±¡ pno", ["(æœªé¸æŠ)"] + pnos, key="sel_pno")

    confirm_yp = st.checkbox("å‰Šé™¤ã«åŒæ„ã—ã¾ã™ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ¨å¥¨ï¼‰", key="confirm_yearpno")

    if st.button(
        "ğŸ§¹ year/pno æŒ‡å®šå‰Šé™¤ã‚’å®Ÿè¡Œ",
        type="primary",
        use_container_width=True,
        disabled=not confirm_yp,
        key="btn_del_yearpno"
    ):
        try:
            has_filter = (sel_year != "(æœªé¸æŠ)" or sel_pno != "(æœªé¸æŠ)")
            if not has_filter:
                st.warning("year ã¾ãŸã¯ pno ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆä¸¡æ–¹æœªé¸æŠã¯å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“ï¼‰ã€‚")
            else:
                # Rollback
                copied, bdir = backup_all_local(base_dir, BACKUP_ROOT, backend, shard_id, label="Rollback")
                if copied:
                    st.caption(f"ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {bdir}")

                keep_lines, keep_vec_indices = [], []
                removed_meta, valid_idx = 0, 0
                removed_files = set()

                if meta_path.exists():
                    with meta_path.open("r", encoding="utf-8") as f:
                        for raw in f:
                            s = raw.strip()
                            if not s:
                                continue
                            try:
                                obj = json.loads(s)
                            except Exception:
                                keep_lines.append(raw + ("\n" if not raw.endswith("\n") else ""))
                                continue

                            yr = str(obj.get("year", ""))
                            pno = str(obj.get("pno", ""))
                            match_year = (sel_year != "(æœªé¸æŠ)" and yr == sel_year)
                            match_pno  = (sel_pno  != "(æœªé¸æŠ)" and pno == sel_pno)

                            if (sel_year == "(æœªé¸æŠ)" or match_year) and (sel_pno == "(æœªé¸æŠ)" or match_pno):
                                removed_meta += 1
                                fpath = obj.get("file")
                                if isinstance(fpath, str) and fpath:
                                    removed_files.add(fpath)
                            else:
                                keep_lines.append(json.dumps(obj, ensure_ascii=False) + "\n")
                                keep_vec_indices.append(valid_idx)

                            valid_idx += 1

                    with meta_path.open("w", encoding="utf-8") as f:
                        f.writelines(keep_lines)

                removed_vecs = 0
                if vec_path.exists():
                    vecs = np.load(vec_path)
                    if vecs.ndim == 2:
                        new_vecs = vecs[keep_vec_indices] if keep_vec_indices else np.empty((0, vecs.shape[1]))
                        removed_vecs = vecs.shape[0] - new_vecs.shape[0]
                        np.save(vec_path, new_vecs)

                pf_msg = ""
                if removed_files:
                    if pf_path.exists():
                        before, after, removed_pf, removed_list = remove_from_processed_files_selective(
                            pf_path, sorted(removed_files)
                        )
                        if removed_pf > 0:
                            pf_msg = (
                                "processed_files.json ã‚’æ›´æ–°ã—ã¾ã—ãŸ:\n"
                                f"- é™¤å¤–æ•°: {removed_pf} ä»¶ (before={before}, after={after})\n"
                                f"- é™¤å¤–ã•ã‚ŒãŸé …ç›®ã®ä¾‹: {removed_list}\n"
                            )
                    else:
                        pf_msg = "processed_files.json ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚æ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚\n"

                st.success(
                    "å‰Šé™¤å®Œäº† âœ…\n"
                    f"- year={sel_year}, pno={sel_pno} ã«ä¸€è‡´ã™ã‚‹ãƒ¡ã‚¿ã‚’å‰Šé™¤\n"
                    f"- meta.jsonl: {removed_meta} è¡Œå‰Šé™¤\n"
                    f"- vectors.npy: {removed_vecs} è¡Œå‰Šé™¤\n"
                    f"{pf_msg}"
                    f"- ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯: {bdir}"
                )

                # PostOp
                try:
                    _, bdir_post = backup_all_local(base_dir, BACKUP_ROOT, backend, shard_id, label="PostOp")
                    st.caption(f"å‡¦ç†å¾Œã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼ˆPostOpï¼‰: {bdir_post}")
                except Exception:
                    st.warning("å‡¦ç†å¾Œãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆPostOpï¼‰ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            st.error(f"å‰Šé™¤ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

st.divider()

# ============================================================
# ğŸ—‘ï¸ å®Œå…¨åˆæœŸåŒ–ï¼ˆ3ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‰Šé™¤ï¼‰
# ============================================================
st.subheader("ğŸ—‘ï¸ å®Œå…¨åˆæœŸåŒ–")
targets = [
    ("meta.jsonl", meta_path),
    ("vectors.npy", vec_path),
    ("processed_files.json", pf_path),
]
present = [(name, p, (p.stat().st_size if p.exists() and p.is_file() else 0)) for name, p in targets if p.exists()]
total_bytes = sum(s for _, _, s in present)

if present:
    lines = [f"- {name}: {p} ({size:,} bytes)" for name, p, size in present]
    st.caption("å‰Šé™¤å¯¾è±¡ï¼ˆå­˜åœ¨ã—ã¦ã„ã‚‹ã‚‚ã®ã®ã¿ï¼‰:\n" + "\n".join(lines))
    st.caption(f"åˆè¨ˆã‚µã‚¤ã‚º: **{total_bytes:,} bytes**")
else:
    st.caption("å‰Šé™¤å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆmeta / vectors / processedï¼‰ã€‚")

col_init_l, col_init_r = st.columns([2, 1])
with col_init_l:
    do_backup_before_wipe = st.checkbox(
        "å‰Šé™¤å‰ã«æ¨™æº–ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆ-Rollbackï¼‰ã‚’ä½œæˆã™ã‚‹",
        value=True,
        key="wipe_backup",
    )
    confirm_wipe = st.checkbox("å®Œå…¨åˆæœŸåŒ–ã«åŒæ„ã—ã¾ã™ï¼ˆå…ƒã«æˆ»ã›ã¾ã›ã‚“ï¼‰", key="wipe_confirm")
with col_init_r:
    typed_init = st.text_input("ã‚¿ã‚¤ãƒ—ç¢ºèªï¼šDELETE ã¨å…¥åŠ›", value="", key="wipe_typed")

if st.button(
    "ğŸ—‘ï¸ åˆæœŸåŒ–å®Ÿè¡Œ",
    type="secondary",
    use_container_width=True,
    disabled=not (confirm_wipe and typed_init.strip().upper() == "DELETE"),
    key="wipe_execute",
):
    try:
        if do_backup_before_wipe:
            copied, bdir = backup_all_local(base_dir, BACKUP_ROOT, backend, shard_id, label="Rollback")
            st.caption(f"äº‹å‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆRollbackï¼‰: {bdir}")

        deleted = []
        for name, p in targets:
            if p.exists():
                p.unlink()
                deleted.append(f"{name}: {p}")

        if deleted:
            st.success("å®Œå…¨åˆæœŸåŒ–ã—ã¾ã—ãŸ:\n" + "\n".join(f"- {x}" for x in deleted))
        else:
            st.info("å‰Šé™¤å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

        # PostOpï¼ˆç©ºã®çŠ¶æ…‹ã‚’æ®‹ã—ãŸã„å ´åˆï¼‰
        try:
            _, bdir_post = backup_all_local(base_dir, BACKUP_ROOT, backend, shard_id, label="PostOp")
            st.caption(f"å‡¦ç†å¾Œã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼ˆPostOpï¼‰: {bdir_post}")
        except Exception:
            pass
    except Exception as e:
        st.error(f"å®Œå…¨åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

st.divider()

# ============================================================
# ğŸ” æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— vs ç¾è¡ŒDB å·®åˆ†ãƒã‚§ãƒƒã‚¯ï¼ˆå…¨ã‚·ãƒ£ãƒ¼ãƒ‰ï¼‰
# ============================================================
st.subheader("ğŸ” æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ç¾è¡ŒDBã®å·®åˆ†ãƒã‚§ãƒƒã‚¯ï¼ˆå…¨ã‚·ãƒ£ãƒ¼ãƒ‰ï¼‰")
st.info(
    "- é¸æŠä¸­ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿å­˜å…ˆã«ã‚ã‚‹**æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**ã¨ã€**ç¾è¡ŒDB**ã‚’å…¨ã‚·ãƒ£ãƒ¼ãƒ‰ã«ã¤ã„ã¦æ¯”è¼ƒã—ã¾ã™ã€‚\n"
    "- æ¯”è¼ƒå¯¾è±¡: meta.jsonl / vectors.npy / processed_files.jsonï¼ˆå­˜åœ¨ãƒ»ã‚µã‚¤ã‚ºãƒ»MD5ãƒ»ä»¶æ•°/shape ãªã©ï¼‰"
)

if st.button("ğŸ§® æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ã®å·®åˆ†ã‚’é›†è¨ˆï¼ˆå…¨ã‚·ãƒ£ãƒ¼ãƒ‰ï¼‰", type="secondary", use_container_width=True, key="btn_diff_all"):
    try:
        # å’Œé›†åˆã§æ¯”è¼ƒï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å´ã«ã—ã‹ãªã„ã‚·ãƒ£ãƒ¼ãƒ‰ã‚‚å«ã¾ã‚Œã‚‹ï¼‰
        targets = (shard_ids_all if 'shard_ids_all' in locals() else [])
        df, missing_backup = diff_all_shards(VS_ROOT, BACKUP_ROOT, backend, targets)
        if df.empty:
            st.info("å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            diff_items = int(df["different"].sum())
            st.write(f"**å·®åˆ†ã‚ã‚Š:** {diff_items} / {len(df)} é …ç›®")
            if missing_backup:
                st.warning("æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‚·ãƒ£ãƒ¼ãƒ‰: " + ", ".join(missing_backup))
            st.dataframe(df, use_container_width=True, height=400)
    except Exception as e:
        st.error(f"å·®åˆ†é›†è¨ˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

st.divider()

# ============================================================
# ğŸ” å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ã® meta.jsonl å·®åˆ†ï¼ˆpno, fileï¼‰ã‚’æŠ½å‡º
# ============================================================
st.subheader("ğŸ” å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ã®ãƒ¡ã‚¿å·®åˆ†ï¼ˆpno, fileï¼‰")
st.info(
    "- ç¾åœ¨é¸æŠã•ã‚Œã¦ã„ã‚‹ã‚·ãƒ£ãƒ¼ãƒ‰ã«ã¤ã„ã¦ã€**æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**ã¨**ç¾è¡ŒDB**ã® `(pno, file)` å·®åˆ†ã‚’æŠ½å‡ºã—ã¾ã™ã€‚\n"
    "- ç‰‡æ–¹ã«ã—ã‹ç„¡ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆè¿½åŠ /æ¬ è½ï¼‰ã‚’ä¸­å¿ƒã«è¡¨ç¤ºã—ã¾ã™ã€‚"
)

if st.button("ğŸ“‘ å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ã®å·®åˆ† (pno, file) ã‚’æŠ½å‡º", type="secondary", use_container_width=True, key="btn_meta_diff_one"):
    try:
        only_live, only_bak, latest_bdir = meta_diff_for_shard(VS_ROOT, BACKUP_ROOT, backend, shard_id)
        if latest_bdir is None:
            st.warning("ã“ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã« **æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“**ã€‚å…ˆã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")
        else:
            st.write(f"**ã‚·ãƒ£ãƒ¼ãƒ‰:** `{backend}/{shard_id}`")
            st.write(f"**æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—:** `{latest_bdir.name}`")
            st.write(f"- ç¾è¡Œã®ã¿: {len(only_live)} ä»¶ / ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ã¿: {len(only_bak)} ä»¶")

            colL, colR = st.columns(2)
            with colL:
                st.markdown("**ğŸŸ¢ ç¾è¡Œã«ã®ã¿å­˜åœ¨**")
                if only_live:
                    st.dataframe(pd.DataFrame(only_live, columns=["pno", "file"]), use_container_width=True, height=240)
                else:
                    st.caption("å·®åˆ†ãªã—")
            with colR:
                st.markdown("**ğŸŸ  ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã«ã®ã¿å­˜åœ¨**")
                if only_bak:
                    st.dataframe(pd.DataFrame(only_bak, columns=["pno", "file"]), use_container_width=True, height=240)
                else:
                    st.caption("å·®åˆ†ãªã—")

            st.success("å·®åˆ†æŠ½å‡º å®Œäº† âœ…")
    except Exception as e:
        st.error(f"å·®åˆ†æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

# ============================================================
# âœ… å·®åˆ†ã‚’é¸æŠã—ã¦åŒæœŸï¼ˆãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ä»˜ãï¼‰/ å…¨å·®åˆ†ä¸€æ‹¬åŒæœŸ
# ============================================================
st.subheader("âœ… å·®åˆ†ã‚’é¸æŠã—ã¦åŒæœŸï¼ˆå¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ï¼‰")
st.info(
    "- **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã«ã®ã¿å­˜åœ¨**ã™ã‚‹ `(pno, file)` ã‚’ä¸€è¦§è¡¨ç¤ºã—ã€é¸æŠåˆ†ã ã‘ç¾è¡Œã¸åŒæœŸã—ã¾ã™ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—â†’ç¾è¡Œï¼‰ã€‚\n"
    "- å®Ÿè¡Œæ™‚ã« **ç¾è¡Œã®ç›´å‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆ-Rollbackï¼‰** ã‚’è‡ªå‹•å–å¾—ã—ã€åŒæœŸå¾Œã¯ `sync_pairs_from_backup_to_live()` ãŒ **-PostOp** ã‚’è‡ªå‹•ä½œæˆã—ã¾ã™ã€‚\n"
    "- â€» ç¾è¡Œã«ã®ã¿å­˜åœ¨ï¼ˆonly_liveï¼‰ã¯è­¦å‘Šã®ã¿ã§åŒæœŸå¯¾è±¡å¤–ã§ã™ï¼ˆç‰‡æ–¹å‘åŒæœŸã§å®‰å…¨ã«ï¼‰ã€‚"
)

if st.button("ğŸ“¥ å·®åˆ†ã‚’èª­ã¿è¾¼ã‚€ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æœ€æ–° vs ç¾è¡Œï¼‰", type="secondary", use_container_width=True, key="btn_load_diffs_select"):
    try:
        only_bak_pairs, latest_bdir, only_live_cnt = load_only_bak_pairs_for_shard(VS_ROOT, BACKUP_ROOT, backend, shard_id)
        st.session_state["diff_pairs_only_bak"] = only_bak_pairs
        st.session_state["diff_latest_bdir"] = str(latest_bdir) if latest_bdir else ""
        if only_live_cnt > 0:
            st.warning(f"ç¾è¡Œã«ã®ã¿å­˜åœ¨ã™ã‚‹å·®åˆ† (only_live) ãŒ {only_live_cnt} ä»¶ã‚ã‚Šã¾ã™ã€‚ç‰‡æ–¹å‘åŒæœŸã®å¯¾è±¡å¤–ã§ã™ã€‚")
        st.success(f"å·®åˆ†ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã«ã®ã¿å­˜åœ¨ï¼‰ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(only_bak_pairs)} ä»¶")
    except Exception as e:
        st.error(f"å·®åˆ†èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

diff_pairs = st.session_state.get("diff_pairs_only_bak")
latest_bdir_str = st.session_state.get("diff_latest_bdir")
if diff_pairs is not None and latest_bdir_str is not None:
    if latest_bdir_str:
        st.caption(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æœ€æ–°: `{Path(latest_bdir_str).name}` / å·®åˆ†ä»¶æ•°: {len(diff_pairs)}")
    df_view = pd.DataFrame(diff_pairs, columns=["pno", "file"])
    if not df_view.empty:
        df_view.insert(0, "sync", True)
        edited = st.data_editor(
            df_view,
            use_container_width=True,
            height=360,
            column_config={
                "sync": st.column_config.CheckboxColumn("åŒæœŸ", default=True),
                "pno": st.column_config.TextColumn("pno"),
                "file": st.column_config.TextColumn("file"),
            },
            disabled=["pno", "file"],
            key="diff_editor_only_bak",
        )

        colL, colR = st.columns(2)
        with colL:
            if st.button("ğŸŸ¢ é¸æŠåˆ†ã®ã¿åŒæœŸ", type="primary", use_container_width=True, key="btn_sync_selected"):
                try:
                    sel_pairs = [
                        (str(row["pno"]) if row["pno"] is not None else None, normalize_path(row["file"]))
                        for _, row in edited.iterrows() if row.get("sync")
                    ]
                    res = sync_pairs_from_backup_to_live(VS_ROOT, BACKUP_ROOT, backend, shard_id, sel_pairs)
                    st.success(
                        f"åŒæœŸå®Œäº† âœ… è¿½åŠ  {res.get('added', 0)} ä»¶\n"
                        f"ãƒ»ç›´å‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—(Rollback): {res.get('live_backup_dir')}\n"
                        f"ãƒ»å‡¦ç†å¾Œã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ(PostOp): {res.get('postop_backup_dir')}"
                    )
                except Exception as e:
                    st.error(f"åŒæœŸä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

        with colR:
            if st.button("ğŸŸ£ å…¨ã¦ã®å·®åˆ†ã‚’åŒæœŸ", type="secondary", use_container_width=True, key="btn_sync_all"):
                try:
                    res = sync_pairs_from_backup_to_live(VS_ROOT, BACKUP_ROOT, backend, shard_id, diff_pairs)
                    st.success(
                        f"åŒæœŸå®Œäº† âœ… è¿½åŠ  {res.get('added', 0)} ä»¶\n"
                        f"ãƒ»ç›´å‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—(Rollback): {res.get('live_backup_dir')}\n"
                        f"ãƒ»å‡¦ç†å¾Œã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ(PostOp): {res.get('postop_backup_dir')}"
                    )
                except Exception as e:
                    st.error(f"åŒæœŸä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    else:
        st.caption("å·®åˆ†ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
else:
    st.caption("â†‘ ã¾ãšã€å·®åˆ†ã‚’èª­ã¿è¾¼ã‚€ã€ã‚’æŠ¼ã—ã¦ã€åŒæœŸå€™è£œã‚’è¡¨ç¤ºã—ã¦ãã ã•ã„ã€‚")

st.divider()

# ============================================================
# â™»ï¸ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾©å…ƒï¼ˆé¸æŠã—ãŸãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— â†’ ç¾è¡Œï¼‰
# ============================================================
st.subheader("â™»ï¸ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾©å…ƒï¼ˆå†…éƒ¨ãƒ¡ãƒ¢ãƒªå†…ã«å¾©å…ƒï¼‰")
st.info(
    "- `<backup_root>/<backend>/<shard_id>/<timestamp-Location[-Rollback|-PostOp]>` ã‹ã‚‰ã€"
    " meta.jsonl / vectors.npy / processed_files.json ã‚’ç¾è¡Œã¸å¾©å…ƒã—ã¾ã™ï¼ˆä¸Šæ›¸ãï¼‰ã€‚"
)

bdirs_restore = list_backup_dirs_local(BACKUP_ROOT, backend, shard_id)
if not bdirs_restore:
    st.caption("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ã€ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
else:
    sel_bdir_restore = st.selectbox("å¾©å…ƒã™ã‚‹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’é¸æŠ", bdirs_restore, format_func=lambda p: p.name, key="restore_bdir")
    if sel_bdir_restore:
        st.dataframe(preview_backup_local(sel_bdir_restore), use_container_width=True, height=160)
        ok_restore = st.checkbox("å¾©å…ƒã«åŒæ„ã—ã¾ã™ï¼ˆç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¸Šæ›¸ãã•ã‚Œã¾ã™ï¼‰", key="restore_ok")
        if st.button("â™»ï¸ å¾©å…ƒå®Ÿè¡Œ", type="primary", use_container_width=True, disabled=not ok_restore, key="restore_exec"):
            try:
                restored, missing = restore_from_backup_local(base_dir, sel_bdir_restore)
                msg = "å¾©å…ƒå®Œäº† âœ…\n" + "\n".join(f"- {x}" for x in restored)
                if missing:
                    msg += "\n\nãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å†…ã«å­˜åœ¨ã—ãªã‹ã£ãŸé …ç›®:\n" + "\n".join(f"- {x}" for x in missing)
                st.success(msg)
            except Exception as e:
                st.error(f"å¾©å…ƒä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
