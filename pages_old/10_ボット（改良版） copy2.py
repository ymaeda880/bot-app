# pages/10_ãƒœãƒƒãƒˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰.py
# =============================================================================
# ğŸ’¬ Internal Bot (RAG, Shards) â€” year/pno ãƒ•ã‚£ãƒ«ã‚¿ä»˜ãï¼ˆå€™è£œãƒ—ãƒªãƒ•ã‚£ãƒ«ã‚¿ï¼‹TopKãƒ–ãƒ¼ã‚¹ãƒˆï¼‰
# =============================================================================

from __future__ import annotations
from pathlib import Path
from typing import List, Tuple, Dict, Any, Iterable, Set
import heapq
from itertools import count
import json

import streamlit as st
import numpy as np

from config.path_config import PATHS
from config.sample_questions import SAMPLES, ALL_SAMPLES
from lib.text_normalize import normalize_ja_text
from lib.prompts.bot_prompt import build_prompt
from lib.gpt_responder import GPTResponder, CompletionResult
from lib.rag.rag_utils import EmbeddingStore, NumpyVectorDB
from lib.costs import (
    MODEL_PRICES_USD, EMBEDDING_PRICES_USD, DEFAULT_USDJPY,
    ChatUsage, estimate_chat_cost, estimate_embedding_cost, usd_to_jpy,
)
from lib.openai_utils import count_tokens
from lib.bot_utils import (
    list_shard_dirs_openai, norm_path, fmt_source,
    enrich_citations, citation_tag, get_openai_api_key,
    parse_inline_files, strip_inline_files,
    # æ–°è¦ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    to_halfwidth_digits, clean_pno_token, parse_years, parse_pnos, norm_pno_forms,
    year_ok, pno_ok, file_ok, scan_candidate_files, filters_caption,
)

#from lib.ui import warn_before_close
#warn_before_close()  # â†ã“ã‚Œã§OKï¼

# ===== å˜ä¾¡ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆUSD/1K tok ã«æ•´å½¢ï¼‰ ======================================
MODEL_PRICES_PER_1K = {
    m: {"in": float(p.get("in", 0.0))/1000.0, "out": float(p.get("out", 0.0))/1000.0}
    for m, p in MODEL_PRICES_USD.items()
}

VS_ROOT: Path = PATHS.vs_root

# ===== Streamlit åŸºæœ¬è¨­å®šï¼ã‚¿ã‚¤ãƒˆãƒ« ==========================================
st.set_page_config(page_title="Chat Bot (Sharded)", page_icon="ğŸ’¬", layout="wide")
st.title("ğŸ’¬ Internal Bot (RAG, Shards)")

if "q" not in st.session_state:
    st.session_state.q = ""

def _set_q(x: str) -> None:
    st.session_state.q = x or ""

####################
# ãƒ¡ãƒ¢ãƒªç›£è¦–å‡¦ç†
# pages/10_ãƒœãƒƒãƒˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰.py ã®æœ«å°¾ãªã©ã§
#####################

from lib.monitors.ui_memory_monitor import render_memory_kpi_row

st.divider()
st.markdown("### ğŸ§  ãƒ¡ãƒ¢ãƒªçŠ¶æ³ï¼ˆå‚è€ƒï¼‰")
render_memory_kpi_row()

# ===== ã‚µã‚¤ãƒ‰ãƒãƒ¼ ==============================================================
with st.sidebar:
    st.header("è¨­å®š")

    model_candidates = [m for m in MODEL_PRICES_PER_1K if m.startswith(("gpt-5", "gpt-4.1"))]
    all_models_sorted = sorted(model_candidates, key=lambda x: (0 if x.startswith("gpt-5") else 1, x))
    if not all_models_sorted:
        st.error("åˆ©ç”¨å¯èƒ½ãª Responses ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚lib/costs.MODEL_PRICES_USD ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    default_idx = all_models_sorted.index("gpt-5-mini") if "gpt-5-mini" in all_models_sorted else 0
    chat_model = st.selectbox("ãƒ¢ãƒ‡ãƒ«ï¼ˆResponsesï¼‰", all_models_sorted, index=default_idx)

    top_k = st.slider("æ¤œç´¢ä»¶æ•°ï¼ˆTop-Kï¼‰", 1, 12, 6, 1)

    detail = {"ç°¡æ½”": "concise", "æ¨™æº–": "standard", "è©³ç´°": "detailed", "è¶…è©³ç´°": "very_detailed"}[
        st.selectbox("è©³ã—ã•", ["ç°¡æ½”", "æ¨™æº–", "è©³ç´°", "è¶…è©³ç´°"], index=2)
    ]

    cite = st.checkbox("å‡ºå…¸ã‚’ [S1] ã§ä¿ƒã™", True)
    max_tokens = st.slider("æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³", 1000, 40000, 12000, 500)
    answer_backend = st.radio("å›ç­”ç”Ÿæˆ", ["OpenAI", "Retrieve-only"], index=0)
    sys_inst = st.text_area("System Instruction", "ã‚ãªãŸã¯å„ªç§€ãªç¤¾å†…ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™.", height=80)
    display_mode = st.radio("è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰", ["é€æ¬¡è¡¨ç¤ºï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ ï¼‰", "ä¸€æ‹¬è¡¨ç¤º"], index=0)

    st.divider(); st.subheader("æ¤œç´¢å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ï¼ˆOpenAIï¼‰")
    shard_dirs_all = list_shard_dirs_openai(VS_ROOT)
    shard_ids_all = [p.name for p in shard_dirs_all]
    target_shards = st.multiselect("ï¼ˆæœªé¸æŠ=ã™ã¹ã¦ï¼‰", shard_ids_all, default=shard_ids_all)

    # âœ… year/pno å…¥åŠ›ï¼ˆãƒ‘ãƒ¼ã‚¹ã¯ bot_utils ã«å§”è­²ï¼‰
    st.divider(); st.subheader("year / pno ãƒ•ã‚£ãƒ«ã‚¿")
    years_input = st.text_input("yearï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šãƒ»ä»»æ„ï¼‰", value="", help="ä¾‹: 2019,2023")
    pnos_input  = st.text_input("pnoï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç•ªå·, ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šãƒ»ä»»æ„ï¼‰", value="", help="ä¾‹: 010,045,120")

    years_sel: Set[int] = parse_years(years_input)
    pnos_raw: Set[str]  = parse_pnos(pnos_input)

    # pno ã®è¡¨è¨˜ã‚†ã‚Œï¼ˆå…ƒ/ã‚¼ãƒ­é™¤å»/3æ¡ã‚¼ãƒ­åŸ‹ã‚ï¼‰ã‚’å¸å
    pnos_sel_norm: Set[str] = set()
    for p in pnos_raw:
        pnos_sel_norm |= norm_pno_forms(p)

    st.caption(
        f"year ãƒ•ã‚£ãƒ«ã‚¿: {sorted(list(years_sel)) or 'ï¼ˆæœªæŒ‡å®šï¼‰'} / "
        f"pno ãƒ•ã‚£ãƒ«ã‚¿: {sorted(list(pnos_sel_norm)) or 'ï¼ˆæœªæŒ‡å®šï¼‰'}"
    )
    st.caption("â€» æœªå…¥åŠ›ãªã‚‰å…¨ä»¶å¯¾è±¡ã€‚ã©ã¡ã‚‰ã‹/ä¸¡æ–¹ã‚’å…¥åŠ›ã—ãŸå ´åˆã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ã—ã¾ã™ã€‚")

    st.caption("å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ï¼ˆä¾‹: 2025/foo.pdf, 2024/bar.pdfï¼‰")
    file_whitelist_str = st.text_input("å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä»»æ„ï¼‰", value="")
    ui_file_whitelist = {s.strip() for s in file_whitelist_str.split(",") if s.strip()}

    api_key = get_openai_api_key()
    if not api_key and answer_backend == "OpenAI":
        st.error("OpenAI APIã‚­ãƒ¼ãŒ secrets.toml / ç’°å¢ƒå¤‰æ•°ã«ã‚ã‚Šã¾ã›ã‚“ã€‚")

    st.divider(); st.markdown("### ğŸ“‚ è§£æ±ºãƒ‘ã‚¹ï¼ˆå‚ç…§ç”¨ï¼‰")
    st.text_input("VS_ROOT", str(PATHS.vs_root), disabled=True)
    st.text_input("PDF_ROOT", str(PATHS.pdf_root), disabled=True)
    st.text_input("BACKUP_ROOT", str(PATHS.backup_root), disabled=True)
    if hasattr(PATHS, "data_root"):
        st.text_input("DATA_ROOT", str(PATHS.data_root), disabled=True)

    st.divider(); st.subheader("ğŸ§ª ãƒ‡ãƒ¢ç”¨ã‚µãƒ³ãƒ—ãƒ«è³ªå•")
    cat = st.selectbox("ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ", ["ï¼ˆæœªé¸æŠï¼‰"] + list(SAMPLES.keys()))
    sample_options = ["ï¼ˆæœªé¸æŠï¼‰"] if cat == "ï¼ˆæœªé¸æŠï¼‰" else ["ï¼ˆæœªé¸æŠï¼‰"] + SAMPLES.get(cat, [])
    sample = st.selectbox("ã‚µãƒ³ãƒ—ãƒ«è³ªå•ã‚’é¸æŠ", sample_options, index=0)
    cols_demo = st.columns(2)
    with cols_demo[0]:
        st.button("â¬‡ï¸ ã“ã®è³ªå•ã‚’å…¥åŠ›æ¬„ã¸ã‚»ãƒƒãƒˆ", width='stretch',
                  disabled=(sample in ("", "ï¼ˆæœªé¸æŠï¼‰")), on_click=lambda: _set_q(sample))
    with cols_demo[1]:
        st.button("ğŸ² ãƒ©ãƒ³ãƒ€ãƒ æŒ¿å…¥", width='stretch',
                  on_click=lambda: _set_q(str(np.random.choice(ALL_SAMPLES)) if ALL_SAMPLES else ""))
    send_now = st.button("ğŸš€ ã‚µãƒ³ãƒ—ãƒ«ã§å³é€ä¿¡", width='stretch',
                         disabled=(st.session_state.q.strip() == ""))

# ===== æœ¬æ–‡ =========================================================
q = st.text_area("è³ªå•ã‚’å…¥åŠ›", value=st.session_state.q, height=100,
                 placeholder="ã“ã®ç¤¾å†…ãƒœãƒƒãƒˆã«è³ªå•ã—ã¦ãã ã•ã„â€¦")
if q != st.session_state.q:
    st.session_state.q = q

go = st.button("é€ä¿¡", type="primary")
go = go or bool(locals().get("send_now"))

# ===== å®Ÿè¡Œï¼šæ¤œç´¢ â†’ ç”Ÿæˆ â†’ ã‚³ã‚¹ãƒˆ â†’ å‚ç…§ ===========================
if go and st.session_state.q.strip():
    try:
        vs_backend_dir = PATHS.vs_root / "openai"
        if not vs_backend_dir.exists():
            st.warning(f"ãƒ™ã‚¯ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆ{vs_backend_dir}ï¼‰ã€‚å…ˆã«ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        selected_ids = target_shards or [p.name for p in list_shard_dirs_openai(PATHS.vs_root)]
        shard_dirs = [vs_backend_dir / s for s in selected_ids]
        shard_dirs = [p for p in shard_dirs if p.is_dir() and (p / "vectors.npy").exists()]
        if not shard_dirs:
            st.warning("æ¤œç´¢å¯èƒ½ãªã‚·ãƒ£ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            st.stop()

        # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ [[files: ...]] ã¨ UI ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆã‚’åˆç®—ï¼ˆæ­£è¦åŒ–ã¯ file_ok å†…ã§å†åˆ©ç”¨ï¼‰
        inline_files = parse_inline_files(st.session_state.q)
        effective_whitelist_norm: Set[str] = {norm_path(x) for x in (ui_file_whitelist | inline_files)}

        # ---- meta.jsonl èµ°æŸ»ï¼ˆå€™è£œæŠ½å‡ºï¼‰ ----
        cand_by_shard, cand_total = scan_candidate_files(shard_dirs, years_sel, pnos_sel_norm)

        if (years_sel or pnos_sel_norm) and cand_total == 0:
            st.warning("æŒ‡å®šã® year/pno ã«ä¸€è‡´ã™ã‚‹**å€™è£œãƒ•ã‚¡ã‚¤ãƒ«ãŒ 0 ä»¶**ã®ãŸã‚ã€æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã›ã‚“ã€‚æ¡ä»¶ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        # year/pno ç”±æ¥ã®å€™è£œã‚’ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆã«åˆæˆ
        for _sd, files in cand_by_shard.items():
            effective_whitelist_norm |= files

        # ãƒ‡ãƒãƒƒã‚°è¡¨è¨˜
        st.caption(filters_caption(years_sel, pnos_sel_norm, cand_total if (years_sel or pnos_sel_norm) else None))

        # ---- ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ ----
        question = normalize_ja_text(strip_inline_files(st.session_state.q))
        estore = EmbeddingStore(backend="openai")
        question_tokens = count_tokens(st.session_state.q, "text-embedding-3-large")
        qv = estore.embed([question]).astype("float32")

        # ---- å„ã‚·ãƒ£ãƒ¼ãƒ‰ TopK ã‚’ãƒãƒ¼ã‚¸ ----
        K = int(top_k)
        heap_: List[Tuple[float, int, int, Dict[str, Any]]] = []  # (score, tiebreak, row_idx, meta)
        tie = count()
        for shp in shard_dirs:
            try:
                vdb = NumpyVectorDB(shp)
                local_k = K
                if years_sel or pnos_sel_norm or effective_whitelist_norm:
                    local_k = max(K * 10, 50)
                hits = vdb.search(qv, top_k=local_k, return_="similarity")
                for h in hits:
                    if isinstance(h, tuple) and len(h) == 3:
                        row_idx, score, meta = h
                    else:
                        score, meta = h
                        row_idx = -1

                    md = dict(meta or {}); md["shard_id"] = shp.name

                    # çµ±ä¸€åˆ¤å®šï¼ˆyear/pno/fileï¼‰
                    if not year_ok(md, years_sel):                 # å¹´
                        continue
                    if not pno_ok(md, pnos_sel_norm):              # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç•ªå·
                        continue
                    if not file_ok(md, effective_whitelist_norm):  # ãƒ•ã‚¡ã‚¤ãƒ«é™å®š
                        continue

                    sc = float(score)
                    if len(heap_) < K:
                        heapq.heappush(heap_, (sc, next(tie), row_idx, md))
                    elif sc > heap_[0][0]:
                        heapq.heapreplace(heap_, (sc, next(tie), row_idx, md))
            except Exception as e:
                st.warning(f"ã‚·ãƒ£ãƒ¼ãƒ‰ {shp.name} ã®æ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼: {e}")

        raw_hits = [(rid, sc, md) for (sc, _tb, rid, md) in sorted(heap_, key=lambda x: x[0], reverse=True)]
        if not raw_hits:
            if years_sel or pnos_sel_norm:
                st.warning("æ¡ä»¶ã«ã¯åˆã†å€™è£œãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã—ãŸãŒã€ä¸Šä½ã‚¹ã‚³ã‚¢ã«å…¥ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n"
                           "ğŸ‘‰ Top-K ã‚’å¢—ã‚„ã™ï¼ã‚¯ã‚¨ãƒªã‚’å…·ä½“åŒ–ã™ã‚‹ï¼ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’çµã‚‹ã€ã®ã„ãšã‚Œã‹ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚")
            else:
                st.warning("è©²å½“ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            st.stop()


        # ---- å›ç­”ç”Ÿæˆ ----
        chat_prompt_tokens = chat_completion_tokens = 0
        use_backend = "OpenAI" if (answer_backend == "OpenAI" and api_key) else "Retrieve-only"

        if use_backend == "OpenAI":
            labeled = [
                f"[S{i}] {m.get('text', '')}\n[meta: {fmt_source(m)} / score={float(s):.3f}]"
                for i, (_rid, s, m) in enumerate(raw_hits, 1)
            ]
            prompt = build_prompt(
                question, labeled, sys_inst=sys_inst, style_hint=detail, cite=cite, strict=False
            )
            responder = GPTResponder(api_key=api_key)
            use_stream = (display_mode == "é€æ¬¡è¡¨ç¤ºï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ ï¼‰")  # â† ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§åˆ‡æ›¿

            # ğŸ”¹ é€æ¬¡å‡ºåŠ›ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰

            # ğŸ”¹ é€æ¬¡å‡ºåŠ›ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
            if use_stream:
                st.subheader("ğŸ§  å›ç­”ï¼ˆé€æ¬¡è¡¨ç¤ºï¼‰")
                with st.chat_message("assistant"):
                    st.write_stream(
                        responder.stream(
                            model=chat_model,
                            system_instruction=sys_inst,
                            user_content=prompt,
                            max_output_tokens=int(max_tokens),
                            on_error_text="Responses stream error."
                        )
                    )
                answer = responder.final_text or ""
                chat_prompt_tokens = responder.usage.input_tokens
                chat_completion_tokens = responder.usage.output_tokens

            # ğŸ”¹ ä¸€æ‹¬è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰
            else:
                with st.spinner("å›ç­”ç”Ÿæˆä¸­â€¦"):
                    result: CompletionResult = responder.complete(
                        model=chat_model,
                        system_instruction=sys_inst,
                        user_content=prompt,
                        max_output_tokens=int(max_tokens)
                    )
                answer = result.text or ""
                chat_prompt_tokens = result.usage.input_tokens
                chat_completion_tokens = result.usage.output_tokens

                st.subheader("ğŸ§  å›ç­”ï¼ˆä¸€æ‹¬è¡¨ç¤ºï¼‰")
                st.write(enrich_citations(answer, raw_hits))

            # ---- å‡ºå…¸å‡¦ç†ï¼ˆä¸¡ãƒ¢ãƒ¼ãƒ‰å…±é€šï¼‰----
            answer = enrich_citations(answer, raw_hits)
            import re as _re
            citations = _re.findall(r"\[S[^\]]+\]", answer)
            if citations:
                seen = []
                for c in citations:
                    if c not in seen:
                        seen.append(c)
                with st.expander("ğŸ“ å‡ºå…¸æ‹¡å¼µæ¸ˆã¿æœ€çµ‚ãƒ†ã‚­ã‚¹ãƒˆ", expanded=False):
                    st.caption("ä»¥ä¸‹ã¯å›ç­”å†…ã®å‡ºå…¸ã‚¿ã‚°ã‚’æ•´ç†ã—ãŸä¸€è¦§ã§ã™ã€‚")
                    st.markdown("### ğŸ“š å‡ºå…¸ï¼ˆå‡ºå…¸ã”ã¨ã«æ”¹è¡Œï¼‰")
                    st.text("\n".join(seen))
            else:
                st.caption("å‡ºå…¸ã‚¿ã‚°ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")

        else:
            st.subheader("ğŸ§© å–å¾—ã®ã¿ï¼ˆè¦ç´„ãªã—ï¼‰")
            st.info("Retrieve-only ãƒ¢ãƒ¼ãƒ‰ã§ã™ã€‚ä¸‹ã®å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ã”è¦§ãã ã•ã„ã€‚")


        #     if use_stream:
        #         st.subheader("ğŸ§  å›ç­”ï¼ˆé€æ¬¡è¡¨ç¤ºï¼‰")
        #         with st.chat_message("assistant"):
        #             answer = st.write_stream(
        #                 responder.stream(
        #                     model=chat_model,
        #                     system_instruction=sys_inst,
        #                     user_content=prompt,
        #                     max_output_tokens=int(max_tokens),
        #                     on_error_text="Responses stream error."
        #                 )
        #             )
        #         chat_prompt_tokens = responder.usage.input_tokens
        #         chat_completion_tokens = responder.usage.output_tokens

        #     # ğŸ”¹ ä¸€æ‹¬è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰
        #     else:
        #         with st.spinner("å›ç­”ç”Ÿæˆä¸­â€¦"):
        #             result: CompletionResult = responder.complete(
        #                 model=chat_model,
        #                 system_instruction=sys_inst,
        #                 user_content=prompt,
        #                 max_output_tokens=int(max_tokens)
        #             )
        #         answer = enrich_citations(result.text or "", raw_hits)
        #         chat_prompt_tokens = result.usage.input_tokens
        #         chat_completion_tokens = result.usage.output_tokens

        #         st.subheader("ğŸ§  å›ç­”ï¼ˆä¸€æ‹¬è¡¨ç¤ºï¼‰")
        #         st.write(answer)

        # else:
        #     st.subheader("ğŸ§© å–å¾—ã®ã¿ï¼ˆè¦ç´„ãªã—ï¼‰")
        #     st.info("Retrieve-only ãƒ¢ãƒ¼ãƒ‰ã§ã™ã€‚ä¸‹ã®å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ã”è¦§ãã ã•ã„ã€‚")






        # ---- ã‚³ã‚¹ãƒˆè¡¨ç¤º ----
        emb_cost_usd = estimate_embedding_cost("text-embedding-3-large", question_tokens)["usd"]
        chat_cost_usd = 0.0
        if use_backend == "OpenAI":
            chat_cost_usd = estimate_chat_cost(
                chat_model, ChatUsage(input_tokens=chat_prompt_tokens, output_tokens=chat_completion_tokens)
            )["usd"]
        total_usd = emb_cost_usd + chat_cost_usd
        total_jpy = usd_to_jpy(total_usd, rate=DEFAULT_USDJPY)

        st.markdown("### ğŸ’´ ä½¿ç”¨æ–™ã®æ¦‚ç®—")
        c1, c2, c3 = st.columns(3)
        with c1:
            st.metric("åˆè¨ˆ (JPY)", f"{total_jpy:,.2f} å††")
            st.caption(f"ç‚ºæ›¿ {DEFAULT_USDJPY:.2f} JPY/USD")
        with c2:
            st.write("**å†…è¨³ (USD)**")
            st.write(f"- Embedding: `${emb_cost_usd:.6f}`")
            if use_backend == "OpenAI":
                st.write(f"- Chat: `${chat_cost_usd:.6f}` (in={chat_prompt_tokens}, out={chat_completion_tokens})")
            st.write(f"- åˆè¨ˆ: `${total_usd:.6f}`")
        with c3:
            emb_per_1k = float(EMBEDDING_PRICES_USD.get("text-embedding-3-large", 0.0))/1000.0
            st.write("**å˜ä¾¡ (USD / 1K tok)**")
            st.write(f"- Embedding: `${emb_per_1k:.5f}`ï¼ˆtext-embedding-3-largeï¼‰")
            st.write(f"- Chat å…¥åŠ›: `${MODEL_PRICES_PER_1K.get(chat_model,{}).get('in',0.0):.5f}`ï¼ˆ{chat_model}ï¼‰")
            st.write(f"- Chat å‡ºåŠ›: `${MODEL_PRICES_PER_1K.get(chat_model,{}).get('out',0.0):.5f}`ï¼ˆ{chat_model}ï¼‰")

        # ---- å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ ----
        with st.expander("ğŸ” å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¸Šä½ãƒ’ãƒƒãƒˆï¼‰", expanded=False):
            for i, (_rid, score, meta) in enumerate(raw_hits, 1):
                txt = str(meta.get("text", "") or "")
                label = fmt_source(meta)
                year = meta.get("year", "")
                pno = meta.get("pno", "") or meta.get("project_no", "")
                snippet = (txt[:1000] + "â€¦") if len(txt) > 1000 else txt
                st.markdown(
                    f"**[{citation_tag(i, meta)}] score={float(score):.3f}**  "
                    f"`{label}` â€” **year:** {year} / **pno:** {pno}\n\n{snippet}"
                )
    except Exception as e:
        st.error(f"æ¤œç´¢/ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
else:
    st.info("è³ªå•ã‚’å…¥åŠ›ã—ã¦ã€é€ä¿¡ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®šã§ãã¾ã™ã€‚")

# ãƒ¡ãƒ¢ãƒªç›£è¦–å‡¦ç†
# pages/10_ãƒœãƒƒãƒˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰.py ã®æœ«å°¾ãªã©ã§
# from lib.monitors.ui_memory_monitor import render_memory_kpi_row

# st.divider()
# st.markdown("### ğŸ§  ãƒ¡ãƒ¢ãƒªçŠ¶æ³ï¼ˆå‚è€ƒï¼‰")
# render_memory_kpi_row()
