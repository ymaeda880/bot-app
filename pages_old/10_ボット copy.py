# pages/10_ãƒœãƒƒãƒˆ.py
# =============================================================================
# ğŸ’¬ Internal Bot (RAG, Shards) â€” year/pno ãƒ•ã‚£ãƒ«ã‚¿ä»˜ãï¼ˆå€™è£œãƒ—ãƒªãƒ•ã‚£ãƒ«ã‚¿ï¼‹TopKãƒ–ãƒ¼ã‚¹ãƒˆï¼‰
# + â± å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°è¨ˆæ¸¬ï¼ˆVectorDB èµ°æŸ» / åŸ‹ã‚è¾¼ã¿ / æ¤œç´¢ / GPT API / ã‚¹ãƒˆãƒªãƒ¼ãƒ ï¼‰
# =============================================================================

from __future__ import annotations
from pathlib import Path
from typing import List, Tuple, Dict, Any, Iterable, Set
import heapq
from itertools import count
import json

import streamlit as st
import numpy as np

from config.path_config import PATHS
from config.sample_questions import SAMPLES, ALL_SAMPLES
from lib.text_normalize import normalize_ja_text
from lib.prompts.bot_prompt import build_prompt
from lib.gpt_responder import GPTResponder, CompletionResult
from lib.rag.rag_utils import EmbeddingStore, NumpyVectorDB
from lib.costs import (
    MODEL_PRICES_USD, EMBEDDING_PRICES_USD, DEFAULT_USDJPY,
    ChatUsage, estimate_chat_cost, estimate_embedding_cost, usd_to_jpy,
    render_usage_summary, _model_prices_per_1k
)
from lib.openai_utils import count_tokens
from lib.bot_utils import (
    list_shard_dirs_openai, norm_path, fmt_source,
    enrich_citations, citation_tag, get_openai_api_key,
    parse_inline_files, strip_inline_files,
    # æ–°è¦ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    to_halfwidth_digits, clean_pno_token, parse_years, parse_pnos, norm_pno_forms,
    year_ok, pno_ok, file_ok, scan_candidate_files, filters_caption,
)

from lib.metrics.timing_utils import Timings, stream_with_timing, render_metrics_ui
import datetime as dt
import time


# from lib.ui import warn_before_close
# warn_before_close()  # â†å¿…è¦ãªã‚‰æœ‰åŠ¹åŒ–

# ===== ä¾¡æ ¼ãƒ†ãƒ¼ãƒ–ãƒ«æ•´å½¢ï¼ˆUSD/1K tokï¼‰ =======================================
MODEL_PRICES_PER_1K = _model_prices_per_1k()

VS_ROOT: Path = PATHS.vs_root

# ===== Streamlit åŸºæœ¬è¨­å®šï¼ã‚¿ã‚¤ãƒˆãƒ« ==========================================
st.set_page_config(page_title="Chat Bot (Sharded)", page_icon="ğŸ’¬", layout="wide")
st.title("ğŸ’¬ Internal Bot (RAG, Shards)")

if "q" not in st.session_state:
    st.session_state.q = ""

def _set_q(x: str) -> None:
    st.session_state.q = x or ""

####################
# ãƒ¡ãƒ¢ãƒªç›£è¦–ï¼ˆä»»æ„ï¼‰
#####################
from lib.monitors.ui_memory_monitor import render_memory_kpi_row

st.divider()
st.markdown("### ğŸ§  ãƒ¡ãƒ¢ãƒªçŠ¶æ³ï¼ˆå‚è€ƒï¼‰")
render_memory_kpi_row()

# ===== ã‚µã‚¤ãƒ‰ãƒãƒ¼ ==============================================================
with st.sidebar:
    st.header("è¨­å®š")

    model_candidates = [m for m in MODEL_PRICES_PER_1K if m.startswith(("gpt-5", "gpt-4.1"))]
    all_models_sorted = sorted(model_candidates, key=lambda x: (0 if x.startswith("gpt-5") else 1, x))
    if not all_models_sorted:
        st.error("åˆ©ç”¨å¯èƒ½ãª Responses ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚lib/costs.MODEL_PRICES_USD ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    default_idx = all_models_sorted.index("gpt-5-mini") if "gpt-5-mini" in all_models_sorted else 0
    chat_model = st.selectbox("ãƒ¢ãƒ‡ãƒ«ï¼ˆResponsesï¼‰", all_models_sorted, index=default_idx)

    top_k = st.slider("æ¤œç´¢ä»¶æ•°ï¼ˆTop-Kï¼‰", 1, 12, 6, 1)

    detail = {
        "ç°¡æ½”": "concise", "æ¨™æº–": "standard", "è©³ç´°": "detailed", "è¶…è©³ç´°": "very_detailed"
    }[st.selectbox("è©³ã—ã•", ["ç°¡æ½”", "æ¨™æº–", "è©³ç´°", "è¶…è©³ç´°"], index=2)]

    cite = st.checkbox("å‡ºå…¸ã‚’ [S1] ã§ä¿ƒã™", True)
    max_tokens = st.slider("æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³", 1000, 40000, 12000, 500)
    answer_backend = st.radio("å›ç­”ç”Ÿæˆ", ["OpenAI", "Retrieve-only"], index=0)
    sys_inst = st.text_area("System Instruction", "ã‚ãªãŸã¯å„ªç§€ãªç¤¾å†…ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™.", height=80)
    display_mode = st.radio("è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰", ["é€æ¬¡è¡¨ç¤ºï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ ï¼‰", "ä¸€æ‹¬è¡¨ç¤º"], index=0)

    st.divider(); st.subheader("æ¤œç´¢å¯¾è±¡ã‚·ãƒ£ãƒ¼ãƒ‰ï¼ˆOpenAIï¼‰")
    shard_dirs_all = list_shard_dirs_openai(VS_ROOT)
    shard_ids_all = [p.name for p in shard_dirs_all]
    target_shards = st.multiselect("ï¼ˆæœªé¸æŠ=ã™ã¹ã¦ï¼‰", shard_ids_all, default=shard_ids_all)

    # âœ… year/pno å…¥åŠ›ï¼ˆãƒ‘ãƒ¼ã‚¹ã¯ bot_utils ã«å§”è­²ï¼‰
    st.divider(); st.subheader("year / pno ãƒ•ã‚£ãƒ«ã‚¿")
    years_input = st.text_input("yearï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šãƒ»ä»»æ„ï¼‰", value="", help="ä¾‹: 2019,2023")
    pnos_input  = st.text_input("pnoï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç•ªå·, ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šãƒ»ä»»æ„ï¼‰", value="", help="ä¾‹: 010,045,120")

    years_sel: Set[int] = parse_years(years_input)
    pnos_raw: Set[str]  = parse_pnos(pnos_input)

    # pno ã®è¡¨è¨˜ã‚†ã‚Œï¼ˆå…ƒ/ã‚¼ãƒ­é™¤å»/3æ¡ã‚¼ãƒ­åŸ‹ã‚ï¼‰ã‚’å¸å
    pnos_sel_norm: Set[str] = set()
    for p in pnos_raw:
        pnos_sel_norm |= norm_pno_forms(p)

    st.caption(
        f"year ãƒ•ã‚£ãƒ«ã‚¿: {sorted(list(years_sel)) or 'ï¼ˆæœªæŒ‡å®šï¼‰'} / "
        f"pno ãƒ•ã‚£ãƒ«ã‚¿: {sorted(list(pnos_sel_norm)) or 'ï¼ˆæœªæŒ‡å®šï¼‰'}"
    )
    st.caption("â€» æœªå…¥åŠ›ãªã‚‰å…¨ä»¶å¯¾è±¡ã€‚ã©ã¡ã‚‰ã‹/ä¸¡æ–¹ã‚’å…¥åŠ›ã—ãŸå ´åˆã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ã—ã¾ã™ã€‚")

    st.caption("å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ï¼ˆä¾‹: 2025/foo.pdf, 2024/bar.pdfï¼‰")
    file_whitelist_str = st.text_input("å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä»»æ„ï¼‰", value="")
    ui_file_whitelist = {s.strip() for s in file_whitelist_str.split(",") if s.strip()}

    api_key = get_openai_api_key()
    if not api_key and answer_backend == "OpenAI":
        st.error("OpenAI APIã‚­ãƒ¼ãŒ secrets.toml / ç’°å¢ƒå¤‰æ•°ã«ã‚ã‚Šã¾ã›ã‚“ã€‚")

    st.divider(); st.markdown("### ğŸ“‚ è§£æ±ºãƒ‘ã‚¹ï¼ˆå‚ç…§ç”¨ï¼‰")
    st.text_input("VS_ROOT", str(PATHS.vs_root), disabled=True)
    st.text_input("PDF_ROOT", str(PATHS.pdf_root), disabled=True)
    st.text_input("BACKUP_ROOT", str(PATHS.backup_root), disabled=True)
    if hasattr(PATHS, "data_root"):
        st.text_input("DATA_ROOT", str(PATHS.data_root), disabled=True)

    st.divider(); st.subheader("ğŸ§ª ãƒ‡ãƒ¢ç”¨ã‚µãƒ³ãƒ—ãƒ«è³ªå•")
    cat = st.selectbox("ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ", ["ï¼ˆæœªé¸æŠï¼‰"] + list(SAMPLES.keys()))
    sample_options = ["ï¼ˆæœªé¸æŠï¼‰"] if cat == "ï¼ˆæœªé¸æŠï¼‰" else ["ï¼ˆæœªé¸æŠï¼‰"] + SAMPLES.get(cat, [])
    sample = st.selectbox("ã‚µãƒ³ãƒ—ãƒ«è³ªå•ã‚’é¸æŠ", sample_options, index=0)
    cols_demo = st.columns(2)
    with cols_demo[0]:
        st.button("â¬‡ï¸ ã“ã®è³ªå•ã‚’å…¥åŠ›æ¬„ã¸ã‚»ãƒƒãƒˆ", width='stretch',
                  disabled=(sample in ("", "ï¼ˆæœªé¸æŠï¼‰")), on_click=lambda: _set_q(sample))
    with cols_demo[1]:
        st.button("ğŸ² ãƒ©ãƒ³ãƒ€ãƒ æŒ¿å…¥", width='stretch',
                  on_click=lambda: _set_q(str(np.random.choice(ALL_SAMPLES)) if ALL_SAMPLES else ""))
    send_now = st.button("ğŸš€ ã‚µãƒ³ãƒ—ãƒ«ã§å³é€ä¿¡", width='stretch',
                         disabled=(st.session_state.q.strip() == ""))
    
    #### ãƒ¡ãƒ¢ãƒªçŠ¶æ³ï¼ˆå›ç­”å‰ / å›ç­”å¾Œï¼‰
    st.divider()
    st.subheader("ğŸ§  ãƒ¡ãƒ¢ãƒªçŠ¶æ³ï¼ˆå›ç­”å‰ / å›ç­”å¾Œï¼‰")
    # å›ç­”å‰/å¾Œã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæç”»å…ˆï¼ˆã“ã®é †åºã§ä¸‹ã«ä¸¦ã¶ï¼‰
    mem_pre_box = st.container()
    mem_post_box = st.container()

    
    


# ===== æœ¬æ–‡ =========================================================
q = st.text_area("è³ªå•ã‚’å…¥åŠ›", value=st.session_state.q, height=100,
                 placeholder="ã“ã®ç¤¾å†…ãƒœãƒƒãƒˆã«è³ªå•ã—ã¦ãã ã•ã„â€¦")
if q != st.session_state.q:
    st.session_state.q = q

go = st.button("é€ä¿¡", type="primary")
go = go or bool(locals().get("send_now"))

# ===== å®Ÿè¡Œï¼šæ¤œç´¢ â†’ ç”Ÿæˆ â†’ ã‚³ã‚¹ãƒˆ â†’ å‚ç…§ ===========================
if go and st.session_state.q.strip():
    timings = Timings()
    timings.mark("pipeline_start")

    # â† ã“ã“ãŒâ€œå›ç­”å‰â€ã€‚ã“ã®æ™‚ç‚¹ã®ãƒ¡ãƒ¢ãƒªã‚’ä¸Šæ®µã«ãƒ¬ãƒ³ãƒ€
    with mem_pre_box:
        st.caption("ï¼ˆå›ç­”å‰ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼‰")
        render_memory_kpi_row()

    try:
        vs_backend_dir = PATHS.vs_root / "openai"
        if not vs_backend_dir.exists():
            st.warning(f"ãƒ™ã‚¯ãƒˆãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆ{vs_backend_dir}ï¼‰ã€‚å…ˆã«ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        selected_ids = target_shards or [p.name for p in list_shard_dirs_openai(PATHS.vs_root)]
        shard_dirs = [vs_backend_dir / s for s in selected_ids]
        shard_dirs = [p for p in shard_dirs if p.is_dir() and (p / "vectors.npy").exists()]
        if not shard_dirs:
            st.warning("æ¤œç´¢å¯èƒ½ãªã‚·ãƒ£ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            st.stop()

        # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ [[files: ...]] ã¨ UI ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆã‚’åˆç®—ï¼ˆæ­£è¦åŒ–ã¯ file_ok å†…ã§å†åˆ©ç”¨ï¼‰
        inline_files = parse_inline_files(st.session_state.q)
        effective_whitelist_norm: Set[str] = {norm_path(x) for x in (ui_file_whitelist | inline_files)}

        # ---- meta.jsonl èµ°æŸ»ï¼ˆå€™è£œæŠ½å‡ºï¼‰ ----
        timings.mark("scan_start")
        cand_by_shard, cand_total = scan_candidate_files(shard_dirs, years_sel, pnos_sel_norm)
        timings.mark("scan_end")

        if (years_sel or pnos_sel_norm) and cand_total == 0:
            st.warning("æŒ‡å®šã® year/pno ã«ä¸€è‡´ã™ã‚‹**å€™è£œãƒ•ã‚¡ã‚¤ãƒ«ãŒ 0 ä»¶**ã®ãŸã‚ã€æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã›ã‚“ã€‚æ¡ä»¶ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        # year/pno ç”±æ¥ã®å€™è£œã‚’ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆã«åˆæˆ
        for _sd, files in cand_by_shard.items():
            effective_whitelist_norm |= files

        # ãƒ‡ãƒãƒƒã‚°è¡¨è¨˜
        st.caption(filters_caption(years_sel, pnos_sel_norm, cand_total if (years_sel or pnos_sel_norm) else None))

        # ---- ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ ----
        question = normalize_ja_text(strip_inline_files(st.session_state.q))
        estore = EmbeddingStore(backend="openai")
        question_tokens = count_tokens(st.session_state.q, "text-embedding-3-large")

        timings.mark("embed_start")
        qv = estore.embed([question]).astype("float32")
        timings.mark("embed_end")

        # ---- å„ã‚·ãƒ£ãƒ¼ãƒ‰ TopK ã‚’ãƒãƒ¼ã‚¸ ----
        timings.mark("search_start")
        K = int(top_k)
        heap_: List[Tuple[float, int, int, Dict[str, Any]]] = []  # (score, tiebreak, row_idx, meta)
        tie = count()
        for shp in shard_dirs:
            try:
                vdb = NumpyVectorDB(shp)
                local_k = K
                if years_sel or pnos_sel_norm or effective_whitelist_norm:
                    local_k = max(K * 10, 50)
                hits = vdb.search(qv, top_k=local_k, return_="similarity")
                for h in hits:
                    if isinstance(h, tuple) and len(h) == 3:
                        row_idx, score, meta = h
                    else:
                        score, meta = h
                        row_idx = -1

                    md = dict(meta or {}); md["shard_id"] = shp.name

                    # çµ±ä¸€åˆ¤å®šï¼ˆyear/pno/fileï¼‰
                    if not year_ok(md, years_sel):                 # å¹´
                        continue
                    if not pno_ok(md, pnos_sel_norm):              # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç•ªå·
                        continue
                    if not file_ok(md, effective_whitelist_norm):  # ãƒ•ã‚¡ã‚¤ãƒ«é™å®š
                        continue

                    sc = float(score)
                    if len(heap_) < K:
                        heapq.heappush(heap_, (sc, next(tie), row_idx, md))
                    elif sc > heap_[0][0]:
                        heapq.heapreplace(heap_, (sc, next(tie), row_idx, md))
            except Exception as e:
                st.warning(f"ã‚·ãƒ£ãƒ¼ãƒ‰ {shp.name} ã®æ¤œç´¢ã§ã‚¨ãƒ©ãƒ¼: {e}")
        timings.mark("search_end")

        raw_hits = [(rid, sc, md) for (sc, _tb, rid, md) in sorted(heap_, key=lambda x: x[0], reverse=True)]
        if not raw_hits:
            if years_sel or pnos_sel_norm:
                st.warning("æ¡ä»¶ã«ã¯åˆã†å€™è£œãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã—ãŸãŒã€ä¸Šä½ã‚¹ã‚³ã‚¢ã«å…¥ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n"
                           "ğŸ‘‰ Top-K ã‚’å¢—ã‚„ã™ï¼ã‚¯ã‚¨ãƒªã‚’å…·ä½“åŒ–ã™ã‚‹ï¼ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’çµã‚‹ã€ã®ã„ãšã‚Œã‹ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚")
            else:
                st.warning("è©²å½“ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            st.stop()

        # ---- å›ç­”ç”Ÿæˆ ----
        chat_prompt_tokens = chat_completion_tokens = 0
        use_backend = "OpenAI" if (answer_backend == "OpenAI" and api_key) else "Retrieve-only"

        if use_backend == "OpenAI":
            labeled = [
                f"[S{i}] {m.get('text', '')}\n[meta: {fmt_source(m)} / score={float(s):.3f}]"
                for i, (_rid, s, m) in enumerate(raw_hits, 1)
            ]
            prompt = build_prompt(
                question, labeled, sys_inst=sys_inst, style_hint=detail, cite=cite, strict=False
            )
            responder = GPTResponder(api_key=api_key)
            use_stream = (display_mode == "é€æ¬¡è¡¨ç¤ºï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ ï¼‰")

            # ğŸ”¹ é€æ¬¡å‡ºåŠ›ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ ï¼‰
            if use_stream:
                st.subheader("ğŸ§  å›ç­”ï¼ˆé€æ¬¡è¡¨ç¤ºï¼‰")
                timings.mark("gpt_req_start")
                with st.chat_message("assistant"):
                    st.write_stream(
                        stream_with_timing(
                            responder.stream(
                                model=chat_model,
                                system_instruction=sys_inst,
                                user_content=prompt,
                                max_output_tokens=int(max_tokens),
                                on_error_text="Responses stream error."
                            ),
                            timings,
                            first_key="gpt_first_token",
                            done_key="gpt_done",
                        )
                    )
                answer = responder.final_text or ""
                chat_prompt_tokens = responder.usage.input_tokens
                chat_completion_tokens = responder.usage.output_tokens
                # å¿µã®ãŸã‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                timings.marks.setdefault("gpt_first_token", timings.marks.get("gpt_req_start"))
                timings.marks.setdefault("gpt_done", dt.datetime.now())
                timings.perf.setdefault("gpt_first_token", timings.perf.get("gpt_req_start", time.perf_counter()))
                timings.perf.setdefault("gpt_done", time.perf_counter())

            # ğŸ”¹ ä¸€æ‹¬è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰
            else:
                st.subheader("ğŸ§  å›ç­”ï¼ˆä¸€æ‹¬è¡¨ç¤ºï¼‰")
                timings.mark("gpt_req_start")
                with st.spinner("å›ç­”ç”Ÿæˆä¸­â€¦"):
                    result: CompletionResult = responder.complete(
                        model=chat_model,
                        system_instruction=sys_inst,
                        user_content=prompt,
                        max_output_tokens=int(max_tokens)
                    )
                # ä¸€æ‹¬ã¯ã€Œæœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼å®Œäº†ã€ã¨ã¿ãªã™
                timings.mark("gpt_first_token")
                timings.mark("gpt_done")

                answer = result.text or ""
                chat_prompt_tokens = result.usage.input_tokens
                chat_completion_tokens = result.usage.output_tokens

                st.write(enrich_citations(answer, raw_hits))

            # ---- å‡ºå…¸å‡¦ç†ï¼ˆä¸¡ãƒ¢ãƒ¼ãƒ‰å…±é€šï¼‰----
            answer = enrich_citations(answer, raw_hits)
            import re as _re
            citations = _re.findall(r"\[S[^\]]+\]", answer)
            if citations:
                seen = []
                for c in citations:
                    if c not in seen:
                        seen.append(c)
                with st.expander("ğŸ“ å‡ºå…¸æ‹¡å¼µæ¸ˆã¿æœ€çµ‚ãƒ†ã‚­ã‚¹ãƒˆ", expanded=False):
                    st.caption("ä»¥ä¸‹ã¯å›ç­”å†…ã®å‡ºå…¸ã‚¿ã‚°ã‚’æ•´ç†ã—ãŸä¸€è¦§ã§ã™ã€‚")
                    st.markdown("### ğŸ“š å‡ºå…¸ï¼ˆå‡ºå…¸ã”ã¨ã«æ”¹è¡Œï¼‰")
                    st.text("\n".join(seen))
            else:
                st.caption("å‡ºå…¸ã‚¿ã‚°ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")

        else:
            st.subheader("ğŸ§© å–å¾—ã®ã¿ï¼ˆè¦ç´„ãªã—ï¼‰")
            st.info("Retrieve-only ãƒ¢ãƒ¼ãƒ‰ã§ã™ã€‚ä¸‹ã®å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ã”è¦§ãã ã•ã„ã€‚")
            answer = ""  # å¿µã®ãŸã‚

        # ---- å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ ----
        with st.expander("ğŸ” å‚ç…§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¸Šä½ãƒ’ãƒƒãƒˆï¼‰", expanded=False):
            for i, (_rid, score, meta) in enumerate(raw_hits, 1):
                txt = str(meta.get("text", "") or "")
                label = fmt_source(meta)
                year = meta.get("year", "")
                pno = meta.get("pno", "") or meta.get("project_no", "")
                snippet = (txt[:1000] + "â€¦") if len(txt) > 1000 else txt
                st.markdown(
                    f"**[{citation_tag(i, meta)}] score={float(score):.3f}**  "
                    f"`{label}` â€” **year:** {year} / **pno:** {pno}\n\n{snippet}"
                )

        # ---- ä½¿ç”¨é‡ã®æ¦‚ç®— ----
        render_usage_summary(
            embedding_model="text-embedding-3-large",
            embedding_tokens=question_tokens,
            chat_model=chat_model,
            chat_prompt_tokens=chat_prompt_tokens,
            chat_completion_tokens=chat_completion_tokens,
            use_backend_openai=(use_backend == "OpenAI"),
            title="ğŸ“Š ä½¿ç”¨é‡ã®æ¦‚ç®—",
        )
        # â± ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ‚äº†
        timings.mark("pipeline_end")
        render_metrics_ui(timings)

        # â± ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ‚äº†ç›´å‰/ç›´å¾Œã«â€œå›ç­”å¾Œâ€ã®ãƒ¡ãƒ¢ãƒªã‚’ä¸‹æ®µã«ãƒ¬ãƒ³ãƒ€
        with mem_post_box:
            st.caption("ï¼ˆå›ç­”å¾Œã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼‰")
            render_memory_kpi_row()

    except Exception as e:
        st.error(f"æ¤œç´¢/ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        # ä¾‹å¤–æ™‚ã‚‚å¯èƒ½ãªç¯„å›²ã§ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’è¡¨ç¤º
        timings.marks.setdefault("pipeline_end", dt.datetime.now())
        timings.perf.setdefault("pipeline_end", time.perf_counter())
        render_metrics_ui(timings)
else:
    st.info("è³ªå•ã‚’å…¥åŠ›ã—ã¦ã€é€ä¿¡ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®šã§ãã¾ã™ã€‚")

# æœ«å°¾ï¼šãƒ¡ãƒ¢ãƒªç›£è¦–ï¼ˆä»»æ„ã§äºŒé‡è¡¨ç¤ºã‚’é¿ã‘ã‚‹ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆä¾‹ï¼‰
# st.divider()
# st.markdown("### ğŸ§  ãƒ¡ãƒ¢ãƒªçŠ¶æ³ï¼ˆå‚è€ƒï¼‰")
# render_memory_kpi_row()
