# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# File: lib/pdf_ingest.py
# ç›®çš„: pages/05_pdfãƒ™ã‚¯ãƒˆãƒ«åŒ–.py ã‹ã‚‰ãƒ­ã‚¸ãƒƒã‚¯ã‚’åˆ†é›¢ã—ã€ãƒ†ã‚¹ãƒˆå®¹æ˜“æ€§ã¨å†åˆ©ç”¨æ€§ã‚’å‘ä¸Š
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from __future__ import annotations

from pathlib import Path
from datetime import datetime
from typing import List, Tuple, Dict, Set
import json

import pdfplumber
import numpy as np

# tiktoken ã¯ç’°å¢ƒã«ã‚ˆã£ã¦ encoding_for_model ãŒå¤±æ•—ã™ã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãã§æº–å‚™
try:
    import tiktoken
    _enc = tiktoken.encoding_for_model("text-embedding-3-large")
except Exception:
    try:
        import tiktoken
        _enc = tiktoken.get_encoding("cl100k_base")
    except Exception:
        _enc = None  # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆNone ã®ã¨ãã¯å˜ç´” lenï¼‰

# å¤–éƒ¨ä¾å­˜ï¼ˆæœ¬ãƒªãƒå†…ï¼‰
from lib.rag_utils import split_text, EmbeddingStore, NumpyVectorDB, ProcessedFilesSimple  # noqa: F401 (ProcessedFilesSimpleã¯ä»–ãƒ•ã‚¡ã‚¤ãƒ«ã§ä½¿ç”¨)
from lib.vectorstore_utils import load_processed_files, save_processed_files
from lib.text_normalize import normalize_ja_text

OPENAI_EMBED_MODEL = "text-embedding-3-large"

# ============================================================
# Token helper
# ============================================================

def count_tokens(text: str) -> int:
    """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¿”ã™ï¼ˆtiktoken ãŒç„¡ã„/å£Šã‚Œã¦ã„ã‚‹å ´åˆã¯æ–‡å­—æ•°ã§ä»£ç”¨ï¼‰ã€‚"""
    if _enc is None:
        return len(text or "")
    try:
        return len(_enc.encode(text or ""))
    except Exception:
        return len(text or "")

# ============================================================
# ãƒ‘ã‚¹åˆ—æŒ™ / VS ç”Ÿæˆ
# ============================================================

def list_shards(pdf_root: Path) -> List[str]:
    """PDF_ROOT ç›´ä¸‹ã®ã‚·ãƒ£ãƒ¼ãƒ‰ï¼ˆå¹´åº¦ï¼‰ä¸€è¦§ã€‚"""
    if not pdf_root.exists():
        return []
    return sorted([p.name for p in pdf_root.iterdir() if p.is_dir()])


def list_pnos(pdf_root: Path, shard_id: str) -> List[str]:
    """æŒ‡å®š shard é…ä¸‹ã® pno ä¸€è¦§ã€‚"""
    d = pdf_root / shard_id
    if not d.exists():
        return []
    return sorted([p.name for p in d.iterdir() if p.is_dir()])


def list_pdfs_in_pno(pdf_root: Path, shard_id: str, pno: str) -> List[Path]:
    """`<pdf_root>/<shard>/<pno>` ç›´ä¸‹ã® *.pdf / *.PDF ã‚’åˆ—æŒ™ï¼ˆå†å¸°ãªã—ï¼‰ã€‚"""
    d = pdf_root / shard_id / pno
    if not d.exists():
        return []
    files: List[Path] = []
    files.extend(sorted(d.glob("*.pdf")))
    files.extend(sorted(d.glob("*.PDF")))
    return files


def ensure_vs_dir(vs_root: Path, backend: str, shard_id: str) -> Path:
    """`<vs_root>/<backend>/<shard>` ã‚’ä½œæˆã—ã¦è¿”ã™ã€‚"""
    d = vs_root / backend / shard_id
    d.mkdir(parents=True, exist_ok=True)
    return d

# ============================================================
# processed_files.json é–¢é€£ï¼ˆäº’æ›è€ƒæ…®ï¼‰
# ============================================================

def _load_processed_keyset(pf_json_path: Path) -> Set[str]:
    """processed_files ã®ä»£è¡¨å½¢å¼ã‚’ set[str] ã§è¿”ã™ï¼ˆè¾žæ›¸/é…åˆ—/æ··åœ¨ã‚’å¸åŽï¼‰ã€‚"""
    done: Set[str] = set()
    try:
        with pf_json_path.open("r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception:
        data = None
    if not data:
        return done

    if isinstance(data, dict):
        vals = data.get("done") or data.get("files") or data.get("processed") or []
        if isinstance(vals, list):
            for v in vals:
                if isinstance(v, str):
                    done.add(v)
                elif isinstance(v, dict):
                    v2 = v.get("file") or v.get("path") or v.get("name")
                    if isinstance(v2, str):
                        done.add(v2)
        elif isinstance(vals, dict):
            for v in vals.values():
                if isinstance(v, str):
                    done.add(v)
        return done

    if isinstance(data, list):
        for e in data:
            if isinstance(e, str) and e:
                done.add(e)
            elif isinstance(e, dict):
                v = e.get("file") or e.get("path") or e.get("name")
                if isinstance(v, str) and v:
                    done.add(v)
    return done


def is_processed_any_form(done_set: Set[str], shard_id: str, pno: str, filename: str) -> bool:
    """ç¾è¡Œ/æ—§äº’æ›ã® 3 å½¢å¼ã®ã„ãšã‚Œã‹ã«ä¸€è‡´ã™ã‚Œã°å‡¦ç†æ¸ˆã¿ã¨åˆ¤æ–­ã€‚"""
    key_full   = f"{shard_id}/{pno}/{filename}"
    key_shard  = f"{shard_id}/{filename}"
    key_legacy = filename
    return (key_full in done_set) or (key_shard in done_set) or (key_legacy in done_set)


def migrate_processed_files_to_canonical(pf_json: Path, shard_id: str, pno: str) -> None:
    """processed_files ã‚’ `shard/pno/filename` ã«æ­£è¦åŒ–ã€‚å¤‰æ›´ãŒã‚ã‚Œã°ä¿å­˜ã€‚"""
    pf_list = load_processed_files(pf_json)
    if not pf_list:
        return
    changed = False
    canonical: List[str] = []

    for entry in pf_list:
        if isinstance(entry, str):
            val = entry
        elif isinstance(entry, dict):
            val = entry.get("file") or entry.get("path") or entry.get("name")
        else:
            continue
        if not val:
            continue

        parts = val.split("/")
        if len(parts) == 1:
            val = f"{shard_id}/{pno}/{val}"
            changed = True
        elif len(parts) == 2:
            if parts[0] == shard_id:
                val = f"{shard_id}/{pno}/{parts[1]}"
                changed = True
        canonical.append(val)

    seen, dedup = set(), []
    for v in canonical:
        if v in seen:
            continue
        seen.add(v)
        dedup.append(v)

    if changed:
        save_processed_files(pf_json, dedup)

# ============================================================
# é™¤å¤–ãƒ˜ãƒ«ãƒ‘ï¼ˆ*_skip / _side.json: ocr âˆˆ {skipped, failed, locked}ï¼‰
# ============================================================

_EXCLUDED_SIDE_OCR = {"skipped", "failed", "locked"}

def _is_skip_file(p: Path) -> bool:
    """ãƒ™ãƒ¼ã‚¹åã« '_skip' ã‚’å«ã‚€ PDF ã‚’é™¤å¤–å¯¾è±¡ã«ã™ã‚‹ï¼ˆå¤§æ–‡å­—ãƒ»å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„ï¼‰ã€‚"""
    return "_skip" in p.stem.lower()

def _is_side_excluded(p: Path) -> Tuple[bool, str]:
    """<basename>_side.json ã® ocr ãŒ {skipped, failed, locked} ãªã‚‰é™¤å¤–ã€‚"""
    side_path = p.with_name(p.stem + "_side.json")
    if not side_path.exists():
        return False, ""
    try:
        meta = json.loads(side_path.read_text(encoding="utf-8"))
    except Exception:
        return False, ""
    ocr_val = str(meta.get("ocr", "")).lower()
    if ocr_val in _EXCLUDED_SIDE_OCR:
        return True, f"side.json ã® ocr:'{ocr_val}'"
    return False, ""

def _filter_skip(paths: List[Path]) -> Tuple[List[Path], List[str]]:
    """*_skip ã¨ side.json(ocrâˆˆ{skipped,failed,locked}) ã‚’é™¤å¤–ã—ã¦ (kept, logs) ã‚’è¿”ã™ã€‚"""
    kept: List[Path] = []
    logs: List[str] = []
    for p in paths:
        if _is_skip_file(p):
            logs.append(f"ä¸æŽ¡ç”¨: {p.name} â€” `_skip` ã®ãŸã‚é™¤å¤–")
            continue
        is_ex, reason = _is_side_excluded(p)
        if is_ex:
            logs.append(f"ä¸æŽ¡ç”¨: {p.name} â€” {reason}")
            continue
        kept.append(p)
    return kept, logs

# ============================================================
# OCR å„ªå…ˆã®å€™è£œæ±ºå®š
# ============================================================

def decide_ocr_candidates(pdf_paths: List[Path]) -> Tuple[List[Tuple[Path, str]], List[str]]:
    """åŒä¸€ãƒ™ãƒ¼ã‚¹åã« A.pdf / A_ocr.pdf ãŒã‚ã‚‹å ´åˆã¯ _ocr ã‚’æŽ¡ç”¨ã—ã¦ãƒ™ãƒ¼ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã€‚"""
    by_base: Dict[str, Dict[str, Path]] = {}
    for p in pdf_paths:
        stem = p.stem
        if stem.endswith("_ocr"):
            base = stem[:-4]
            by_base.setdefault(base, {})["ocr"] = p
        else:
            base = stem
            by_base.setdefault(base, {})["base"] = p

    candidates: List[Tuple[Path, str]] = []
    logs: List[str] = []

    for base, d in by_base.items():
        ocr = d.get("ocr")
        basep = d.get("base")
        if ocr is not None:
            candidates.append((ocr, "done"))
            if basep is not None:
                logs.append(f"ðŸŸ¢ æŽ¡ç”¨: {ocr.name}ï¼ˆ_ocrå„ªå…ˆï¼‰ / â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {basep.name}")
            else:
                logs.append(f"ðŸŸ¢ æŽ¡ç”¨: {ocr.name}ï¼ˆ_ocr å˜ç‹¬ï¼‰")
        elif basep is not None:
            candidates.append((basep, "no"))
            logs.append(f"ðŸŸ¢ æŽ¡ç”¨: {basep.name}ï¼ˆ_ocr ãªã—ï¼‰")

    candidates.sort(key=lambda t: t[0].name)
    return candidates, logs

# ============================================================
# æœªå‡¦ç†ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ / ãƒ™ã‚¯ãƒˆãƒ«æ•°
# ============================================================

def compute_unprocessed_map(pdf_root: Path, vs_root: Path, shard_id: str, backend: str) -> Dict[str, Dict[str, List[str] | str | int]]:
    """pno ã”ã¨ã®æœªå‡¦ç†çŠ¶æ³ï¼ˆcomplete/none/partial ã¨æ®‹ä»¶ï¼‰ã‚’è¿”ã™ã€‚"""
    result: Dict[str, Dict[str, List[str] | str | int]] = {}

    vs_dir = vs_root / backend / shard_id
    pf_json = vs_dir / "processed_files.json"
    done_set = _load_processed_keyset(pf_json) if pf_json.exists() else set()

    pnos_all = list_pnos(pdf_root, shard_id)

    for pno in pnos_all:
        raw_pdfs = list_pdfs_in_pno(pdf_root, shard_id, pno)

        # é™¤å¤–ï¼ˆ*_skip / side.json: ocrâˆˆ{skipped,failed,locked}ï¼‰
        filtered_pdfs, _ = _filter_skip(raw_pdfs)

        # OCRå„ªå…ˆã®æŽ¡å¦åˆ¤å®šã¯ filtered å¾Œã§å®Ÿæ–½
        candidates, _logs = decide_ocr_candidates(filtered_pdfs)
        cand_names = [p.name for (p, _ocr) in candidates]

        if not cand_names:
            result[pno] = {"status": "complete", "total_candidates": 0, "unprocessed_files": []}
            continue

        processed_flags = [is_processed_any_form(done_set, shard_id, pno, fn) for fn in cand_names]
        processed_count = sum(1 for x in processed_flags if x)

        if processed_count == 0:
            status = "none"; unproc: List[str] = cand_names[:]  # å…¨æœªå‡¦ç†
        elif processed_count == len(cand_names):
            status = "complete"; unproc = []
        else:
            status = "partial"; unproc = [fn for fn, ok in zip(cand_names, processed_flags) if not ok]

        result[pno] = {"status": status, "total_candidates": len(cand_names), "unprocessed_files": unproc}

    return result


def get_vector_count(base_dir: Path) -> int:
    """`vectors.npy` ã®è¡Œæ•°ï¼ˆç·ãƒ™ã‚¯ãƒˆãƒ«æ•°ï¼‰ã‚’è¿”ã™ï¼ˆå¤±æ•—æ™‚ã¯ 0ï¼‰ã€‚"""
    p = base_dir / "vectors.npy"
    if not p.exists():
        return 0
    try:
        arr = np.load(p, mmap_mode="r")
        return int(arr.shape[0]) if arr.ndim == 2 else 0
    except Exception:
        return 0

# ============================================================
# 1 ãƒ•ã‚¡ã‚¤ãƒ«å–ã‚Šè¾¼ã¿ï¼ˆæŠ½å‡ºâ†’åˆ†å‰²â†’åŸ‹ã‚è¾¼ã¿â†’DB è¿½è¨˜ï¼‰
# ============================================================

def ingest_pdf_file(
    vdb: NumpyVectorDB,
    estore: EmbeddingStore,
    shard_id: str,
    pno: str,
    pdf_path: Path,
    year_val: int | None,
    batch_size: int,
    chunk_size: int,
    overlap: int,
    ocr_flag: str,
    embed_model_label: str,
) -> Tuple[int, int]:
    """1 ã¤ã® PDF ã‚’ ingest ã—ã¦ã€(è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«æ•°, è¿½åŠ ãƒãƒ£ãƒ³ã‚¯æ•°) ã‚’è¿”ã™ã€‚

    è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã¯é€šå¸¸ 1 ã‹ 0ï¼ˆä¾‹å¤–ã‚¹ã‚­ãƒƒãƒ—æ™‚ï¼‰ã€‚
    """
    name = pdf_path.name
    new_files = 0
    new_chunks = 0

    with pdfplumber.open(str(pdf_path)) as pdf:
        total_pages = max(len(pdf.pages), 1)

        for page_no, page in enumerate(pdf.pages, start=1):
            raw = page.extract_text(x_tolerance=1.5, y_tolerance=1.5) or ""
            raw = raw.replace("\t", " ").replace("\xa0", " ")
            text = " ".join(raw.split())
            if not text:
                continue

            text = normalize_ja_text(text)
            spans: List[Tuple[str, int, int]] = split_text(text, chunk_size=int(chunk_size), overlap=int(overlap))
            if not spans:
                continue

            chunks = [s[0] for s in spans]
            vectors: List[np.ndarray] = []
            metas: List[dict] = []

            for i in range(0, len(chunks), int(batch_size)):
                batch = chunks[i:i + int(batch_size)]
                vecs = estore.embed(batch, batch_size=int(batch_size)).astype("float32")
                vectors.append(vecs)

                for j, (ch, s, e) in enumerate(spans[i:i + int(batch_size)]):
                    metas.append({
                        "file": f"{shard_id}/{pno}/{name}",
                        "year": year_val,
                        "pno": pno,
                        "page": page_no,
                        "chunk_id": f"{name}#p{page_no}-{i+j}",
                        "chunk_index": i + j,
                        "text": ch,
                        "span_start": s,
                        "span_end": e,
                        "chunk_len_tokens": count_tokens(ch),
                        "ingested_at": datetime.utcnow().isoformat(timespec="seconds") + "Z",
                        "shard_id": shard_id,
                        "embed_model": embed_model_label,
                        "ocr": "done" if ocr_flag == "done" else "no",
                    })

            vec_mat = np.vstack(vectors) if len(vectors) > 1 else vectors[0]
            vdb.add(vec_mat, metas)
            new_chunks += len(metas)

    new_files += 1
    return new_files, new_chunks
