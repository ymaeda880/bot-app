# lib/diff_utils.py
from __future__ import annotations
from pathlib import Path
from typing import Optional, Tuple, List, Set, Dict
import json
import urllib.parse
import unicodedata
import hashlib
import numpy as np
import pandas as pd

# æ—¢å­˜ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
from lib.backup_utils import backup_all_local, list_backup_dirs_local

# ============================================================
# åŸºæœ¬ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ============================================================

def normalize_path(s: Optional[str]) -> Optional[str]:
    """URLãƒ‡ã‚³ãƒ¼ãƒ‰â†’NFKCæ­£è¦åŒ–â†’åŒºåˆ‡ã‚Šçµ±ä¸€ï¼ˆ\\â†’/ï¼‰ã€‚éæ–‡å­—åˆ—ã¯ None ã‚’è¿”ã™ã€‚"""
    if not isinstance(s, str):
        return None
    try:
        dec = urllib.parse.unquote(s)
        norm = unicodedata.normalize("NFKC", dec)
        return norm.replace("\\", "/")
    except Exception:
        return s

def md5_file(path: Path, chunk_size: int = 1024 * 1024) -> Optional[str]:
    try:
        h = hashlib.md5()
        with path.open("rb") as f:
            for b in iter(lambda: f.read(chunk_size), b""):
                h.update(b)
        return h.hexdigest()
    except Exception:
        return None

def vector_dim(vec_path: Path) -> Optional[Tuple[int, int]]:
    """vectors.npy ã® shape ã‚’ (n, d) ã§è¿”ã™ã€‚ç„¡ã„/å£Šã‚Œã¯ Noneã€‚"""
    if not vec_path.exists():
        return None
    try:
        arr = np.load(vec_path)
        return tuple(arr.shape)  # (n, d)
    except Exception:
        return None

# ============================================================
# meta / vectors / processed ã®çµ±è¨ˆï¼ˆå·®åˆ†ã‚µãƒãƒªç”¨ï¼‰
# ============================================================

def meta_stats(p: Path) -> dict:
    if not p.exists():
        return {"exists": False, "size": 0, "md5": None, "rows": None}
    rows = 0
    try:
        with p.open("r", encoding="utf-8") as f:
            for _ in f:
                rows += 1
    except Exception:
        rows = None
    return {"exists": True, "size": p.stat().st_size, "md5": md5_file(p), "rows": rows}

def vectors_stats(p: Path) -> dict:
    if not p.exists():
        return {"exists": False, "size": 0, "md5": None, "shape": None}
    shape = None
    try:
        arr = np.load(p)
        shape = tuple(arr.shape)
    except Exception:
        shape = None
    return {"exists": True, "size": p.stat().st_size, "md5": md5_file(p), "shape": shape}

def processed_stats(p: Path) -> dict:
    if not p.exists():
        return {"exists": False, "size": 0, "md5": None, "count": None}
    count = None
    try:
        with p.open("r", encoding="utf-8") as f:
            obj = json.load(f)
        if isinstance(obj, list):
            count = len(obj)
        elif isinstance(obj, dict):
            count = len(obj.keys())
    except Exception:
        count = None
    return {"exists": True, "size": p.stat().st_size, "md5": md5_file(p), "count": count}

def compare_item(backend: str, shard: str, item: str, live_path: Path, backup_path: Path) -> dict:
    if item == "meta.jsonl":
        live = meta_stats(live_path)
        bak  = meta_stats(backup_path)
        keys = ("exists", "size", "md5", "rows")
    elif item == "vectors.npy":
        live = vectors_stats(live_path)
        bak  = vectors_stats(backup_path)
        keys = ("exists", "size", "md5", "shape")
    else:  # processed_files.json
        live = processed_stats(live_path)
        bak  = processed_stats(backup_path)
        keys = ("exists", "size", "md5", "count")

    different = any(live.get(k) != bak.get(k) for k in keys)
    return {
        "backend": backend,
        "shard_id": shard,
        "item": item,
        "different": different,
        "live_exists": live.get("exists"),
        "bak_exists": bak.get("exists"),
        "live_size": live.get("size"),
        "bak_size": bak.get("size"),
        "live_md5": live.get("md5"),
        "bak_md5": bak.get("md5"),
        "live_rows": live.get("rows") if "rows" in live else None,
        "bak_rows": bak.get("rows") if "rows" in bak else None,
        "live_shape": live.get("shape") if "shape" in live else None,
        "bak_shape": bak.get("shape") if "shape" in bak else None,
        "live_count": live.get("count") if "count" in live else None,
        "bak_count": bak.get("count") if "count" in bak else None,
    }

# ============================================================
# meta.jsonl ã®èª­ã¿å–ã‚Š
# ============================================================

def read_meta_pairs(meta_file: Path, drop_empty: bool = False) -> Set[Tuple[Optional[str], Optional[str]]]:
    """
    meta.jsonl â†’ {(pno, file)} é›†åˆã€‚
    - file/pno ã¯ normalizeï¼ˆURLãƒ‡ã‚³ãƒ¼ãƒ‰, NFKC, åŒºåˆ‡ã‚Šçµ±ä¸€ï¼‰ã€‚pno ã¯ str åŒ–ã€‚
    - drop_empty=Falseï¼ˆæ—¢å®šï¼‰: ç©º(None,"")ã§ã‚‚é›†åˆã«å«ã‚ã‚‹ï¼ˆå·®åˆ†ãŒæ¶ˆãˆã¦ã—ã¾ã†ã®ã‚’é˜²ãï¼‰
    """
    pairs: Set[Tuple[Optional[str], Optional[str]]] = set()
    if not meta_file.exists():
        return pairs
    with meta_file.open("r", encoding="utf-8") as f:
        for line in f:
            s = line.strip()
            if not s:
                continue
            try:
                obj = json.loads(s)
            except Exception:
                continue
            pno = obj.get("pno")
            pno = str(pno) if pno is not None else None
            file_ = normalize_path(obj.get("file"))
            if drop_empty and not file_:
                continue
            pairs.add((pno, file_))
    return pairs

def read_meta_dict(meta_file: Path) -> Dict[Tuple[Optional[str], Optional[str]], str]:
    """
    key=(pno, file_norm), val=1è¡ŒJSONæ–‡å­—åˆ—(æ”¹è¡Œä»˜ã)
    """
    d: Dict[Tuple[Optional[str], Optional[str]], str] = {}
    if not meta_file.exists():
        return d
    with meta_file.open("r", encoding="utf-8") as f:
        for raw in f:
            s = raw.strip()
            if not s:
                continue
            try:
                obj = json.loads(s)
            except Exception:
                continue
            pno = obj.get("pno")
            pno = str(pno) if pno is not None else None
            file_ = normalize_path(obj.get("file"))
            d[(pno, file_)] = json.dumps(obj, ensure_ascii=False) + "\n"
    return d

# ============================================================
# å·®åˆ†é›†è¨ˆ APIï¼ˆãƒšãƒ¼ã‚¸ã‹ã‚‰å‘¼ã³å‡ºã—ï¼‰
# ============================================================

def diff_all_shards(
    vs_root: Path,
    backup_root: Path,
    backend: str,
    shard_ids: List[str],
) -> Tuple[pd.DataFrame, List[str]]:
    """
    å„ shard ã«ã¤ã„ã¦æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆtimestamp é…ä¸‹ï¼‰ã¨ç¾è¡Œã®
    meta.jsonl / vectors.npy / processed_files.json ã‚’æ¯”è¼ƒã—ã€è¡Œã‚µãƒãƒªã‚’è¿”ã™ã€‚
    """
    records = []
    missing_backup: List[str] = []

    for sid in shard_ids:
        live_dir = vs_root / backend / sid
        live_meta = live_dir / "meta.jsonl"
        live_vec  = live_dir / "vectors.npy"
        live_pf   = live_dir / "processed_files.json"

        bdirs_sid = list_backup_dirs_local(backup_root, backend, sid)
        if not bdirs_sid:
            missing_backup.append(sid)
            # backup ä¸åœ¨ã®è¡Œã‚‚è¨˜éŒ²ï¼ˆbackup_path ã¯ãƒ€ãƒŸãƒ¼ï¼‰
            null_path = Path("/dev/null")
            records.append(compare_item(backend, sid, "meta.jsonl", live_meta, null_path))
            records.append(compare_item(backend, sid, "vectors.npy", live_vec, null_path))
            records.append(compare_item(backend, sid, "processed_files.json", live_pf, null_path))
            continue

        latest_bdir = bdirs_sid[0]
        bak_meta = latest_bdir / "meta.jsonl"
        bak_vec  = latest_bdir / "vectors.npy"
        bak_pf   = latest_bdir / "processed_files.json"

        records.append(compare_item(backend, sid, "meta.jsonl", live_meta, bak_meta))
        records.append(compare_item(backend, sid, "vectors.npy", live_vec, bak_vec))
        records.append(compare_item(backend, sid, "processed_files.json", live_pf, bak_pf))

    if not records:
        return pd.DataFrame(), missing_backup

    cols_view = [
        "backend", "shard_id", "item", "different",
        "live_exists", "bak_exists",
        "live_size", "bak_size",
        "live_rows", "bak_rows",
        "live_shape", "bak_shape",
        "live_count", "bak_count",
        "live_md5", "bak_md5",
    ]
    df = pd.DataFrame.from_records(records)
    df = df[cols_view]
    return df, missing_backup

def meta_diff_for_shard(
    vs_root: Path,
    backup_root: Path,
    backend: str,
    shard_id: str,
) -> Tuple[List[Tuple[Optional[str], Optional[str]]],
           List[Tuple[Optional[str], Optional[str]]],
           Optional[Path]]:
    """
    ç¾è¡Œ vs æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— ã® (pno, file) å·®åˆ†ã€‚
    æˆ»ã‚Šå€¤: (only_live_sorted, only_bak_sorted, latest_backup_dir or None)
    """
    bdirs_sid = list_backup_dirs_local(backup_root, backend, shard_id)
    if not bdirs_sid:
        return [], [], None

    latest_bdir = bdirs_sid[0]
    live_dir = vs_root / backend / shard_id
    live_meta = live_dir / "meta.jsonl"
    bak_meta  = latest_bdir / "meta.jsonl"

    live_pairs = read_meta_pairs(live_meta, drop_empty=False)
    bak_pairs  = read_meta_pairs(bak_meta,  drop_empty=False)

    only_live = sorted(list(live_pairs - bak_pairs))
    only_bak  = sorted(list(bak_pairs - live_pairs))
    return only_live, only_bak, latest_bdir

def load_only_bak_pairs_for_shard(
    vs_root: Path,
    backup_root: Path,
    backend: str,
    shard_id: str,
) -> Tuple[List[Tuple[Optional[str], Optional[str]]], Optional[Path], int]:
    """
    ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æœ€æ–°ã«ã®ã¿å­˜åœ¨ã™ã‚‹å·®åˆ†ï¼ˆonly_bakï¼‰ã‚’è¿”ã™ã€‚
    ã¤ã„ã§ã« only_live ä»¶æ•°ã‚‚è¿”ã™ï¼ˆUI ã®æ³¨æ„å–šèµ·ç”¨ï¼‰ã€‚
    """
    only_live, only_bak, latest_bdir = meta_diff_for_shard(
        vs_root, backup_root, backend, shard_id
    )
    return only_bak, latest_bdir, len(only_live)

# ============================================================
# ğŸ”„ å·®åˆ†åŒæœŸï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— â†’ ç¾è¡Œï¼‰
#   - æŒ‡å®šã•ã‚ŒãŸ (pno, file) ã®ã¿ã‚’ meta.jsonl / vectors.npy / processed_files.json ã«åæ˜ 
#   - åŒæœŸå‰ã«ç¾è¡Œå´ã®ç›´å‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆï¼ˆãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
#   - ç¾è¡Œã‚·ãƒ£ãƒ¼ãƒ‰ãŒç„¡ã„å ´åˆã¯ãƒ•ã‚©ãƒ«ãƒ€ã¨ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ã‹ã‚‰åŒæœŸ
# ============================================================

def sync_pairs_from_backup_to_live(
    VS_ROOT: Path,
    BACKUP_ROOT: Path,
    backend: str,
    shard_id: str,
    pairs_to_add: List[Tuple[Optional[str], Optional[str]]],
) -> dict:
    """
    æŒ‡å®š (pno,file) ã ã‘ã‚’ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— â†’ ç¾è¡Œ ã«åŒæœŸã€‚
    returns: {"added": è¿½åŠ ä»¶æ•°, "live_backup_dir": "<ç›´å‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—dir or None>"}
    """
    result = {"added": 0, "live_backup_dir": None}

    # --- æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å³å¯†ã«å–å¾—ï¼ˆtimestamp ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ï¼‰ ---
    bdirs = list_backup_dirs_local(BACKUP_ROOT, backend, shard_id)
    if not bdirs:
        raise FileNotFoundError("ã“ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    latest_bdir = bdirs[0]
    bak_meta = latest_bdir / "meta.jsonl"
    bak_vec  = latest_bdir / "vectors.npy"

    # --- ç¾è¡Œå´ãƒ‘ã‚¹ ---
    live_dir = VS_ROOT / backend / shard_id
    live_meta = live_dir / "meta.jsonl"
    live_vec  = live_dir / "vectors.npy"
    live_pf   = live_dir / "processed_files.json"

    # --- ç¾è¡ŒåˆæœŸåŒ–ï¼ˆãƒ•ã‚©ãƒ«ãƒ€&ç©ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼‰ ---
    live_dir.mkdir(parents=True, exist_ok=True)
    if not live_meta.exists():
        live_meta.touch()
    if not live_pf.exists():
        with live_pf.open("w", encoding="utf-8") as f:
            json.dump([], f, ensure_ascii=False, indent=2)

    # --- ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å´ã®å­˜åœ¨/å½¢çŠ¶ãƒã‚§ãƒƒã‚¯ ---
    if not bak_meta.exists() or not bak_vec.exists():
        raise FileNotFoundError(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒä¸å®Œå…¨ã§ã™: {latest_bdir}")

    bak_arr = np.load(bak_vec)
    if bak_arr.ndim != 2:
        raise ValueError("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å´ vectors.npy ã®å½¢çŠ¶ãŒä¸æ­£ã§ã™ã€‚")
    d = bak_arr.shape[1]

    # ç¾è¡Œ vectors ãŒç„¡ã„å ´åˆã¯ç©ºã‹ã‚‰
    if live_vec.exists():
        live_arr = np.load(live_vec)
        if live_arr.ndim != 2 or live_arr.shape[1] != d:
            raise ValueError("ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒãŒä¸€è‡´ã—ã¾ã›ã‚“ã€‚")
    else:
        live_arr = np.empty((0, d))

    # --- ç›´å‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰ ---
    try:
        _copied, bdir_live_backup = backup_all_local(live_dir, BACKUP_ROOT, backend, shard_id, label="Rollback")
        result["live_backup_dir"] = str(bdir_live_backup)
    except Exception:
        bdir_live_backup = None  # å¤±æ•—ã—ã¦ã‚‚åŒæœŸã¯ç¶™ç¶š

    # --- backup å´ã® (pno,file) â†’ è¡Œãƒ†ã‚­ã‚¹ãƒˆ / è¡Œindex ã‚’æ§‹ç¯‰ ---
    meta_dict = read_meta_dict(bak_meta)
    idx_map: Dict[Tuple[Optional[str], Optional[str]], int] = {}
    with bak_meta.open("r", encoding="utf-8") as f:
        i = 0
        for line in f:
            s = line.strip()
            if not s:
                continue
            try:
                obj = json.loads(s)
            except Exception:
                continue
            pno  = obj.get("pno")
            pno  = str(pno) if pno is not None else None
            file_ = normalize_path(obj.get("file"))
            idx_map[(pno, file_)] = i
            i += 1

    # --- è¿½åŠ å¯¾è±¡ã‚’æ­£è¦åŒ–ã—ã¦çªãåˆã‚ã› ---
    add_lines: List[str] = []
    add_indices: List[int] = []
    added_pairs: List[Tuple[Optional[str], Optional[str]]] = []

    for pno_in, file_in in pairs_to_add:
        pno_norm  = str(pno_in) if pno_in is not None else None
        file_norm = normalize_path(file_in)
        key = (pno_norm, file_norm)
        line = meta_dict.get(key)
        if line is None:
            # UIã¨ã®æ­£è¦åŒ–å·®ç•°ã§ãƒ’ãƒƒãƒˆã—ãªã„å¯èƒ½æ€§ã‚’æœ€å°åŒ–ã™ã‚‹ãŸã‚ã€ã•ã‚‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¢ã—
            # ï¼ˆfile ãŒç©º/None ã®ã‚±ãƒ¼ã‚¹ã‚‚ã‚ã‚‹ã®ã§ã€pno ã ã‘ä¸€è‡´ã§æ‹¾ã†ç­‰ã¯å±é™ºã€‚ã“ã“ã§ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
            continue
        add_lines.append(line)
        if key in idx_map:
            add_indices.append(idx_map[key])
        added_pairs.append(key)

    if not add_lines:
        # åŒæœŸå¯¾è±¡ãªã—
        return result

    # --- meta.jsonl ã¸è¿½è¨˜ ---
    with live_meta.open("a", encoding="utf-8") as f:
        f.writelines(add_lines)

    # --- vectors ã‚’çµåˆä¿å­˜ ---
    if add_indices:
        add_vecs = bak_arr[add_indices]
        new_vecs = np.vstack([live_arr, add_vecs])
        np.save(live_vec, new_vecs)
    else:
        # å¿µã®ãŸã‚ live ã‚’ãã®ã¾ã¾ä¿å­˜ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯ç©ºã‚’ä¿å­˜ï¼‰
        np.save(live_vec, live_arr)

    # --- processed_files.json ã‚’ãƒ¦ãƒ‹ãƒ¼ã‚¯è¿½è¨˜ ---
    try:
        with live_pf.open("r", encoding="utf-8") as f:
            pf = json.load(f)
        if not isinstance(pf, list):
            pf = []
    except Exception:
        pf = []

    existing = set(normalize_path(x) for x in pf if isinstance(x, str))
    for _pno, fpath in added_pairs:
        norm = normalize_path(fpath) if isinstance(fpath, str) else None
        if norm and norm not in existing:
            pf.append(norm)
            existing.add(norm)

    with live_pf.open("w", encoding="utf-8") as f:
        json.dump(pf, f, ensure_ascii=False, indent=2)

    # å¤‰æ›´å¾Œï¼ˆæŠœç²‹ï¼‰
    result["added"] = len(added_pairs)

    # â˜… åŒæœŸãŒæ­£å¸¸çµ‚äº†ã—ãŸã‚‰ã€Œç¾åœ¨ã®çŠ¶æ…‹ã€ã‚’ PostOp ã§æ’®ã‚‹
    try:
        _copied2, bdir_postop = backup_all_local(
            live_dir, BACKUP_ROOT, backend, shard_id, label="PostOp"
        )
        result["postop_backup_dir"] = str(bdir_postop)
    except Exception:
        result["postop_backup_dir"] = None

    return result
